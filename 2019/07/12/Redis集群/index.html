

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&quot;auto&quot;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://qiniu.xiaoming.net.cn/%E5%8D%9A%E5%AE%A2icon.jpeg">
  <link rel="icon" href="https://qiniu.xiaoming.net.cn/%E5%8D%9A%E5%AE%A2icon.jpeg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="数据分布分布式数据库首先要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整体数据的一个子集。常见的分区规则有顺序分区和哈希分区：

哈希分区：离散度好，数据分布业务无关，无法顺序访问
顺序分区：离散度易倾斜，数据分布业务相关，可以顺序访问
">
  <meta name="author" content="Silverming">
  <meta name="keywords" content="">
  
  <title>Redis集群 - Silverming</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">

<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">

<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":false,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"54ebb03ad7ad5b762ac8ff7958df6d3f","google":"G-M2RT7SDT3L","gtag":"G-M2RT7SDT3L","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"tFHjJkaAYKqH8BIXKnJVurUc-MdYXbMMI","app_key":"1qR5F7XyydYd5YJtIpMJBFmP","server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Silverming</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://qiniu.xiaoming.net.cn/%E5%8D%9A%E5%AE%A2%E8%83%8C%E6%99%AF%E5%9B%BE.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Redis集群">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2019-07-12 16:55" pubdate>
        2019年7月12日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      11.2k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      128
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Redis集群</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2020年12月1日 晚上
                
              </p>
            
            <div class="markdown-body">
              <h1 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h1><p>分布式数据库首先要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整体数据的一个子集。<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA.png?raw=true" alt=""><br>常见的分区规则有顺序分区和哈希分区：</p>
<ul>
<li>哈希分区：离散度好，数据分布业务无关，无法顺序访问</li>
<li>顺序分区：离散度易倾斜，数据分布业务相关，可以顺序访问</li>
</ul>
<a id="more"></a>

<h2 id="常见哈希分区规则"><a href="#常见哈希分区规则" class="headerlink" title="常见哈希分区规则"></a>常见哈希分区规则</h2><h3 id="节点取余分区"><a href="#节点取余分区" class="headerlink" title="节点取余分区"></a>节点取余分区</h3><p>使用特定的数据，如Redis的键或用户ID，再根据节点数量N使用公式： hash（key）%N计算出哈希值，用来决定数据映射到哪一个节点上。这种方 案存在一个问题：当节点数量变化时，如扩容或收缩节点，数据节点映射关系需要重新计算，会导致数据的重新迁移。<br>这种方式的突出优点是简单性，常用于数据库的分库分表规则，一般采用预分区的方式，提前根据数据量规划好分区数，比如划分为512或1024张表，保证可支撑未来一段时间的数据量，再根据负载情况将表迁移到其他数据库中。扩容时通常采用翻倍扩容，避免数据映射全部被打乱导致全量迁移的情况。</p>
<h3 id="一致性哈希分区"><a href="#一致性哈希分区" class="headerlink" title="一致性哈希分区"></a>一致性哈希分区</h3><p>一致性哈希分区（Distributed Hash Table）实现思路是为系统中每个节点分配一个token，范围一般在0~2^32，这些token构成一个哈希环。数据读写执行节点查找操作时，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值的token节点<br>优点：加入和删除节点只影响哈希环中相邻的节点，对其他节点无影响。<br>缺点：</p>
<ul>
<li>加减节点会造成哈希环中部分数据无法命中，需要手动处理或者忽略这部分数据，因此一致性哈希常用于缓存场景。</li>
<li>当使用少量节点时，节点变化将大范围影响哈希环中数据映射，因此这种方式不适合少量数据节点的分布式方案。</li>
<li>普通的一致性哈希分区在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡。</li>
</ul>
<h3 id="虚拟槽分区"><a href="#虚拟槽分区" class="headerlink" title="虚拟槽分区"></a>虚拟槽分区</h3><p>虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中，整数定义为槽（slot）。这个范围一般远远大于节点数。槽是集群内数据管理和迁移的基本单位。采用大范围槽的主要目的是为了方便数据拆分和集群扩展，每个节点会负责一定数量的槽<br>如图：当前集群有5个节点，每个节点平均大约负责3276个槽。由于采用高质 量的哈希算法，每个槽所映射的数据通常比较均匀，将数据平均划分到5个节点进行数据分区。<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/Redis%E8%99%9A%E6%8B%9F%E6%A7%BD%E5%88%86%E5%8C%BA.png?raw=true" alt=""></p>
<h2 id="Redis数据分区"><a href="#Redis数据分区" class="headerlink" title="Redis数据分区"></a>Redis数据分区</h2><p>Redis Cluster采用虚拟槽分区，所有的键根据哈希函数映射到0~16383整数槽内，计算公式：slot=CRC16（key）&amp;16383。每一个节点负责维护一部分槽以及槽所映射的键值数据。<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA.png?raw=true" alt=""><br>Redis虚拟槽分区的特点： </p>
<ul>
<li>解耦数据和节点之间的关系，简化了节点扩容和收缩难度。 </li>
<li>节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据。 </li>
<li>支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景。</li>
</ul>
<h2 id="集群功能限制"><a href="#集群功能限制" class="headerlink" title="集群功能限制"></a>集群功能限制</h2><ol>
<li>key批量操作支持有限。如mset、mget，目前只支持具有相同slot值的key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可能存在于多个节点上因此不被支持。</li>
<li>key事务操作支持有限。同理只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能。</li>
<li>key作为数据分区的最小粒度，因此不能将一个大的键值对象如hash、list等映射到不同的节点。</li>
<li>不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模式下只能使用一个数据库空间，即db0。</li>
<li>复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。</li>
</ol>
<h1 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h1><h2 id="手动搭建"><a href="#手动搭建" class="headerlink" title="手动搭建"></a>手动搭建</h2><h3 id="准备节点"><a href="#准备节点" class="headerlink" title="准备节点"></a>准备节点</h3><p>Redis集群一般由多个节点组成，至少需要6个才能保证高可用的集群。集群相关配置如下：</p>
<figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros"><span class="hljs-comment">#端口</span><span class="hljs-built_in"><br>port </span>6379<br><span class="hljs-comment">#开启集群模式</span><br>cluster-enabled <span class="hljs-literal">yes</span><br><span class="hljs-comment">#节点超时时间</span><br>cluster-node-timeout 15000<br><span class="hljs-comment">#集群内部配置文件</span><br>cluster-config-file <span class="hljs-string">"nodes-6379.conf"</span><br></code></pre></div></td></tr></table></figure>
<p>准备好多个节点配置文件后，启动所有节点</p>
<figure class="highlight stata"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs stata">redis-server <span class="hljs-keyword">conf</span>/redis-6379.<span class="hljs-keyword">conf</span><br>redis-server <span class="hljs-keyword">conf</span>/redis-6380.<span class="hljs-keyword">conf</span><br>redis-server <span class="hljs-keyword">conf</span>/redis-6381.<span class="hljs-keyword">conf</span> <br>redis-server <span class="hljs-keyword">conf</span>/redis-6382.<span class="hljs-keyword">conf</span><br>redis-server <span class="hljs-keyword">conf</span>/redis-6383.<span class="hljs-keyword">conf</span><br>redis-server <span class="hljs-keyword">conf</span>/redis-6384.<span class="hljs-keyword">conf</span><br></code></pre></div></td></tr></table></figure>
<blockquote>
<p>第一次启动时，集群没有哪部配置文件，会自动创建一份。Redis自动维护集群配置文件，当集群内节点信息发生变化，如添加节点、节点下线、故障转移等。节点会自动保存集群状态到配置文件中。  </p>
</blockquote>
<p>节点启动后，可以通过<code>cluster nodes</code>获取集群节点状态，因为此时每个节点还没有关联，只能看到自己的信息</p>
<h3 id="节点握手"><a href="#节点握手" class="headerlink" title="节点握手"></a>节点握手</h3><p>节点握手是指让一批运行在集群模式下的节点通过Gossp协议彼此通信，关联起来，由客户端发起命令：表示该客户端与另外的一个节点关联</p>
<blockquote>
<p>cluster meet {ip} {port}  </p>
</blockquote>
<p><strong>大致流程（6379关联6380）</strong></p>
<ol>
<li>节点6379本地创建6380节点信息对象，并发送meet消息。</li>
<li>节点6380接受到meet消息后，保存6379节点信息并回复pong消息。</li>
<li>之后节点6379和6380彼此定期通过ping/pong消息进行正常的节点通信。</li>
</ol>
<p>只需要在一个节点对其他节点进行节点握手，握手状态就会通过消息在集群内部传播，其他节点会自动发现新节点并发起握手流程<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E8%8A%82%E7%82%B9%E6%8F%A1%E6%89%8B.png?raw=true" alt=""><br>节点建立握手后集群还不能工作，此时集群还处于下线状态，所有数据的读写都被禁止,还需要进行分配槽的操作</p>
<figure class="highlight accesslog"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs accesslog"><span class="hljs-number">127.0.0.1:6379</span>&gt; set hello redis<br>(error) CLUSTERDOWN Hash slot not served<br></code></pre></div></td></tr></table></figure>

<h3 id="分配槽"><a href="#分配槽" class="headerlink" title="分配槽"></a>分配槽</h3><p>Redis集群把所有的数据映射到16384个槽中。每个key会映射为一个固定的槽，只有当节点分配了槽，才能响应和这些槽关联的键命令。通过<code>cluster addslots</code>命令为节点分配槽。</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">redis-cli -h <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> -p <span class="hljs-number">6379</span> cluster addslots &#123;<span class="hljs-number">0.</span>.<span class="hljs-number">.5461</span>&#125;<br>redis-cli -h <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> -p <span class="hljs-number">6380</span> cluster addslots &#123;<span class="hljs-number">5462.</span>.<span class="hljs-number">.10922</span>&#125;<br>redis-cli -h <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> -p <span class="hljs-number">6381</span> cluster addslots &#123;<span class="hljs-number">10923.</span>.<span class="hljs-number">.16383</span>&#125;<br></code></pre></div></td></tr></table></figure>
<p>此时集群进入在线状态，可以通过执行cluster nodes命令查看节点和槽的分配关系：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript"><span class="hljs-number">5004e273167</span>ac7ec3cd252e084b396e409f88b51 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6383@</span><span class="hljs-number">16383</span> master - <span class="hljs-number">0</span> <span class="hljs-number">1563070800554</span> <span class="hljs-number">3</span> connected<br>ed024ca813815308a7704400de84bb44d8867718 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6382@</span><span class="hljs-number">16382</span> master - <span class="hljs-number">0</span> <span class="hljs-number">1563070800000</span> <span class="hljs-number">0</span> connected<br>a2e6c40de0a37dba1cc299a01f3552c116d9657f <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6380@</span><span class="hljs-number">16380</span> master - <span class="hljs-number">0</span> <span class="hljs-number">1563070801562</span> <span class="hljs-number">5</span> connected <span class="hljs-number">5462</span><span class="hljs-number">-10922</span><br><span class="hljs-number">7984f</span>b7d565c89133d94a2454d124949dcf4d9d4 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6384@</span><span class="hljs-number">16384</span> master - <span class="hljs-number">0</span> <span class="hljs-number">1563070798000</span> <span class="hljs-number">4</span> connected<br><span class="hljs-number">418f</span>6a083e488a0678dc8d8b7f6b9fee4f5355c6 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6379@</span><span class="hljs-number">16379</span> myself,master - <span class="hljs-number">0</span> <span class="hljs-number">1563070801000</span> <span class="hljs-number">1</span> connected <span class="hljs-number">0</span><span class="hljs-number">-5461</span><br><span class="hljs-number">52f</span>bde319a11302d9c225af476e745bfde9e2670 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-symbol">6381@</span><span class="hljs-number">16381</span> master - <span class="hljs-number">0</span> <span class="hljs-number">1563070800000</span> <span class="hljs-number">2</span> connected <span class="hljs-number">10923</span><span class="hljs-number">-16383</span><br></code></pre></div></td></tr></table></figure>
<p>作为一个完整的集群，每个负责处理槽的节点应该具有从节点，保证当它出现故障时可以自动进行故障转移。集群模式下，Reids节点角色分为主节点和从节点。首次启动的节点和被分配槽的 节点都是主节点，从节点负责复制主节点槽信息和相关的数据。使用<code>cluster replicate {nodeId}</code>命令让一个节点成为从节点。其中命令执行必须在对应的从节点上执行，nodeId是要复制主节点的节点ID:</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript"><span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6382</span>&gt; cluster replicate <span class="hljs-number">418f</span>6a083e488a0678dc8d8b7f6b9fee4f5355c6<br>redis-cli -p <span class="hljs-number">6383</span> cluster replicate a2e6c40de0a37dba1cc299a01f3552c116d9657f<br>redis-cli -p <span class="hljs-number">6384</span> cluster replicate <span class="hljs-number">52f</span>bde319a11302d9c225af476e745bfde9e2670<br></code></pre></div></td></tr></table></figure>


<h2 id="使用redis-trib-rb搭建集群-Redis5之后不推荐）"><a href="#使用redis-trib-rb搭建集群-Redis5之后不推荐）" class="headerlink" title="使用redis-trib.rb搭建集群(Redis5之后不推荐）"></a>使用redis-trib.rb搭建集群(Redis5之后不推荐）</h2><p><code>redis-trib.rb</code>是采用Ruby实现的Redis集群管理工具。内部通过Cluster相 关命令帮我们简化集群创建、检查、槽迁移和均衡等常见运维操作，使用之前需要安装Ruby依赖环境</p>
<h3 id="Ruby环境准备"><a href="#Ruby环境准备" class="headerlink" title="Ruby环境准备"></a>Ruby环境准备</h3><ol>
<li><p>Mac系统自带Ruby环境，如果没有先安装Ruby</p>
</li>
<li><p>使用<code>gem</code>安装redis依赖</p>
<blockquote>
<p>gem install redis  </p>
</blockquote>
</li>
<li><p>将redis安装目录中src目录下的<code>redis-trib.rb</code>文件放到<code>/usr/local/bin</code>目录下</p>
</li>
<li><p>客户端执行redis-trib.rb确保环境搭建正确</p>
</li>
</ol>
<h3 id="准备节点-1"><a href="#准备节点-1" class="headerlink" title="准备节点"></a>准备节点</h3><p>跟之前一样准备好节点配置并启动</p>
<h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><p>使用<code>redis-trib.rb create</code>命令完成节点握手和槽分配过程</p>
<blockquote>
<p>redis-trib.rb create —replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384  </p>
</blockquote>
<h2 id="使用redis-cli-—cluster"><a href="#使用redis-cli-—cluster" class="headerlink" title="使用redis-cli —cluster"></a>使用redis-cli —cluster</h2><p>在Rdis5之后使用上面的方法会给出警告，Redis5之后给出了新的方式，推荐使用<code>redis-cli --cluster</code> 代替redis-trib.rb<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis-trib%E6%8A%A5%E9%94%99.png?raw=true" alt=""></p>
<p>使用命令如下：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">redis-cli --cluster create <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6379</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6380</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6381</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6382</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6383</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6384</span> --cluster-replicas <span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure>
<blockquote>
<p>—replicas参数指定集群中每个主节点配备几个从节点  </p>
</blockquote>
<p>执行后输出如下：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">&gt;&gt;&gt; Performing hash slots allocation on <span class="hljs-number">6</span> nodes...<br>Master[<span class="hljs-number">0</span>] -&gt; Slots <span class="hljs-number">0</span> - <span class="hljs-number">5460</span><br>Master[<span class="hljs-number">1</span>] -&gt; Slots <span class="hljs-number">5461</span> - <span class="hljs-number">10922</span><br>Master[<span class="hljs-number">2</span>] -&gt; Slots <span class="hljs-number">10923</span> - <span class="hljs-number">16383</span><br>Adding replica <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6383</span> to <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6379</span><br>Adding replica <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6384</span> to <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6380</span><br>Adding replica <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6382</span> to <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6381</span><br>&gt;&gt;&gt; Trying to optimize slaves allocation <span class="hljs-keyword">for</span> anti-affinity<br>[WARNING] Some slaves are <span class="hljs-keyword">in</span> the same host as their master<br>M: <span class="hljs-number">4</span>aed7d5835ce70cd788e8dca25ecc17a3b483d62 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6379</span><br>   slots:[<span class="hljs-number">0</span><span class="hljs-number">-5460</span>] (<span class="hljs-number">5461</span> slots) master<br>M: a8942f231fc69c02640bb2e61cb0e6f659b4d218 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6380</span><br>   slots:[<span class="hljs-number">5461</span><span class="hljs-number">-10922</span>] (<span class="hljs-number">5462</span> slots) master<br>M: cf2e667527b1a8e1eda1cd81403a97cfb9da6a93 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6381</span><br>   slots:[<span class="hljs-number">10923</span><span class="hljs-number">-16383</span>] (<span class="hljs-number">5461</span> slots) master<br>S: d237450657c855a57236c52c6e6ec6e098384dec <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6382</span><br>   replicates a8942f231fc69c02640bb2e61cb0e6f659b4d218<br>S: e323baf44b75e2aeb88b691f6593753a3b9224ca <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6383</span><br>   replicates cf2e667527b1a8e1eda1cd81403a97cfb9da6a93<br>S: d60caa155a815118ef4de33e9f71667d89cb1fc0 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6384</span><br>   replicates <span class="hljs-number">4</span>aed7d5835ce70cd788e8dca25ecc17a3b483d62<br>Can I <span class="hljs-keyword">set</span> the above configuration? (type <span class="hljs-string">'yes'</span> to accept): yes<br>&gt;&gt;&gt; Nodes configuration updated<br>&gt;&gt;&gt; Assign a different config epoch to each node<br>&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster<br>Waiting <span class="hljs-keyword">for</span> the cluster to join<br>.....<br>&gt;&gt;&gt; Performing Cluster Check (using node <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6379</span>)<br>M: <span class="hljs-number">4</span>aed7d5835ce70cd788e8dca25ecc17a3b483d62 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6379</span><br>   slots:[<span class="hljs-number">0</span><span class="hljs-number">-5460</span>] (<span class="hljs-number">5461</span> slots) master<br>   <span class="hljs-number">1</span> additional replica(s)<br>S: d60caa155a815118ef4de33e9f71667d89cb1fc0 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6384</span><br>   slots: (<span class="hljs-number">0</span> slots) slave<br>   replicates <span class="hljs-number">4</span>aed7d5835ce70cd788e8dca25ecc17a3b483d62<br>M: cf2e667527b1a8e1eda1cd81403a97cfb9da6a93 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6381</span><br>   slots:[<span class="hljs-number">10923</span><span class="hljs-number">-16383</span>] (<span class="hljs-number">5461</span> slots) master<br>   <span class="hljs-number">1</span> additional replica(s)<br>S: e323baf44b75e2aeb88b691f6593753a3b9224ca <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6383</span><br>   slots: (<span class="hljs-number">0</span> slots) slave<br>   replicates cf2e667527b1a8e1eda1cd81403a97cfb9da6a93<br>S: d237450657c855a57236c52c6e6ec6e098384dec <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6382</span><br>   slots: (<span class="hljs-number">0</span> slots) slave<br>   replicates a8942f231fc69c02640bb2e61cb0e6f659b4d218<br>M: a8942f231fc69c02640bb2e61cb0e6f659b4d218 <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6380</span><br>   slots:[<span class="hljs-number">5461</span><span class="hljs-number">-10922</span>] (<span class="hljs-number">5462</span> slots) master<br>   <span class="hljs-number">1</span> additional replica(s)<br>[OK] All nodes agree about slots configuration.<br>&gt;&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...<br>&gt;&gt;&gt; Check slots coverage...<br>[OK] All <span class="hljs-number">16384</span> slots covered.<br></code></pre></div></td></tr></table></figure>
<p><strong>集群完整性检查：</strong><br>集群完整性指所有的槽都分配到存活的主节点上，只要16384个槽中有一个没有分配给节点则表示集群不完整。可以使用命令<code>redis-cli --cluster check {ip:port}</code>检测之前创建的两个集群是否成功，check命令只需要给出集群中任意一个节点地址就可以完成整个集群的检查工作<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E9%9B%86%E7%BE%A4%E5%AE%8C%E6%95%B4%E6%80%A7%E6%A3%80%E6%9F%A5.png?raw=true" alt=""></p>
<h1 id="节点通信"><a href="#节点通信" class="headerlink" title="节点通信"></a>节点通信</h1><h2 id="通信流程"><a href="#通信流程" class="headerlink" title="通信流程"></a>通信流程</h2><p>在分布式存储中需要提供维护节点元数据信息的机制，所谓元数据是指：节点负责哪些数据，是否出现故障等状态信息。常见的元数据维护方式分为：集中式和P2P方式。Redis集群采用P2P的Gossip（流言）协议，Gossip协议工作原理就是节点彼此不断通信交换信息，一段时间后所有的节点都会知道集群完整的信息。<br>通信过程：</p>
<ol>
<li>集群中的每个节点都会单独开辟一个TCP通道，用于节点之间彼此通信，通信端口号在基础端口上加10000。</li>
<li>每个节点在固定周期内通过特定规则选择几个节点发送ping消息。</li>
<li>接收到ping消息的节点用pong消息作为响应。</li>
</ol>
<h2 id="Gossip消息"><a href="#Gossip消息" class="headerlink" title="Gossip消息"></a>Gossip消息</h2><p>Gossip协议的主要职责就是信息交换。信息交换的载体就是节点彼此发送的Gossip消息。常用的Gossip消息可分为：ping消息、pong消息、meet消息、fail消息 。</p>
<ul>
<li>meet消息：用于通知新节点加入。消息发送者通知接收者加入到当前集群，meet消息通信正常完成后，接收节点会加入到集群中并进行周期性的ping、pong消息交换。</li>
<li>ping消息：集群内交换最频繁的消息，集群内每个节点每秒向多个其他节点发送ping消息，用于检测节点是否在线和交换彼此状态信息。ping消息发送封装了自身节点和部分其他节点的状态数据。</li>
<li>pong消息：当接收到ping、meet消息时，作为响应消息回复给发送方确认消息正常通信。pong消息内部封装了自身状态数据。节点也可以向集群内广播自身的pong消息来通知整个集群对自身状态进行更新。</li>
<li>fail消息：当节点判定集群内另一个节点下线时，会向集群内广播一个 fail消息，其他节点接收到fail消息之后把对应节点更新为下线状态。</li>
</ul>
<p>所有的消息格式划分为：消息头和消息体。<br>消息头包含发送节点自身状态数据，结构如下：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> &#123;</span> <br>	<span class="hljs-keyword">char</span> sig[<span class="hljs-number">4</span>]; <span class="hljs-comment">/* 信号标示 */</span> <br>	<span class="hljs-keyword">uint32_t</span> totlen; <span class="hljs-comment">/* 消息总长度 */</span> <br>	<span class="hljs-keyword">uint16_t</span> ver; <span class="hljs-comment">/* 协议版本*/</span> <br>	<span class="hljs-keyword">uint16_t</span> type; <span class="hljs-comment">/* 消息类型,用于区分meet,ping,pong等消息 */</span> <br>	<span class="hljs-keyword">uint16_t</span> count; <span class="hljs-comment">/* 消息体包含的节点数量，仅用于meet,ping,ping消息类型*/</span> <br>	<span class="hljs-keyword">uint64_t</span> currentEpoch; <span class="hljs-comment">/* 当前发送节点的配置纪元 */</span> <br>	<span class="hljs-keyword">uint64_t</span> configEpoch; <span class="hljs-comment">/* 主节点/从节点的主节点配置纪元 */</span> <br>	<span class="hljs-keyword">uint64_t</span> offset; <span class="hljs-comment">/* 复制偏移量 */</span> <br>	<span class="hljs-keyword">char</span> sender[CLUSTER_NAMELEN]; <span class="hljs-comment">/* 发送节点的nodeId */</span> <br>	<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span> myslots[CLUSTER_SLOTS/<span class="hljs-number">8</span>]; <span class="hljs-comment">/* 发送节点负责的槽信息 */</span> <br>	<span class="hljs-keyword">char</span> slaveof[CLUSTER_NAMELEN]; <span class="hljs-comment">/* 如果发送节点是从节点，记录对应主节点的nodeId */</span> <br>	<span class="hljs-keyword">uint16_t</span> port; <span class="hljs-comment">/* 端口号 */</span> <br>	<span class="hljs-keyword">uint16_t</span> flags; <span class="hljs-comment">/* 发送节点标识,区分主从角色，是否下线等 */</span> <br>	<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span> state; <span class="hljs-comment">/* 发送节点所处的集群状态 */</span> <br>	<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span> mflags[<span class="hljs-number">3</span>]; <span class="hljs-comment">/* 消息标识 */</span> <br>	<span class="hljs-keyword">union</span> clusterMsgData data <span class="hljs-comment">/* 消息正文 */</span>; <br>&#125; clusterMsg;<br></code></pre></div></td></tr></table></figure>
<p>接收节点根据消息头就可以获取到发送节点的相关数据。<br>集群内所有的消息都采用相同的消息头结构clusterMsg，它包含了节点发送关键信息，如节点id、槽映射、节点标识（主从角色，是否下线）等。<br>消息体在Redis内部采用clusterMsgData结构声明：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-keyword">union</span> clusterMsgData &#123; <br>	<span class="hljs-comment">/* ping,meet,pong消息体*/</span> <br>	<span class="hljs-class"><span class="hljs-keyword">struct</span> &#123;</span> <br>		<span class="hljs-comment">/* gossip消息结构数组 */</span> c<br>		lusterMsgDataGossip gossip[<span class="hljs-number">1</span>];<br>	&#125; ping;<br> 	<span class="hljs-comment">/* FAIL 消息体 */</span> <br>	<span class="hljs-class"><span class="hljs-keyword">struct</span> &#123;</span> <br>		clusterMsgDataFail about; <br>	&#125; fail; <br>	<span class="hljs-comment">// ... </span><br>&#125;;<br></code></pre></div></td></tr></table></figure>
<p>消息体clusterMsgData定义发送消息的数据，其中ping、meet、pong都采用cluster MsgDataGossip数组作为消息体数据，实际消息类型使用消息头的type属性区分。每个消息体包含该节点的多个clusterMsgDataGossip结构数据，用于信息交换。<br>接受节点收到ping/meet消息时，执行解析消息头和消息体流程：</p>
<ul>
<li>解析消息头过程：：消息头包含了发送节点的信息，如果发送节点是新节点且消息是meet类型，则加入到本地节点列表；如果是已知节点，则尝试更新发送节点的状态，如槽映射关系、主从角色等状态。</li>
<li>解析消息体过程：如果消息体的clusterMsgDataGossip数组包含的节点是新节点，则尝试发起与新节点的meet握手流程；如果是已知节点，则根据cluster MsgDataGossip中的flags字段判断该节点是否下线，用于故障转移。</li>
</ul>
<p>消息处理完后回复pong消息，内容同样包含消息头和消息体，发送节点接收到回复的pong消息后，采用类似的流程解析处理消息并更新与接收节点最后通信时间，完成一次消息通信。</p>
<h2 id="节点选择"><a href="#节点选择" class="headerlink" title="节点选择"></a>节点选择</h2><p>Redis集群内节点通信采用固定频率（定时任务每秒执行10次）。<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E9%9B%86%E7%BE%A4%E5%86%85%E8%8A%82%E7%82%B9%E9%80%9A%E4%BF%A1.png?raw=true" alt=""><br>由上图可以看出：消息交换的成本主要体现在单位时间选择发送消息的节点数量和每个消息携带的数据量</p>
<h3 id="选择发送消息的节点数量"><a href="#选择发送消息的节点数量" class="headerlink" title="选择发送消息的节点数量"></a>选择发送消息的节点数量</h3><p>集群内每个节点维护定时任务默认每秒执行10次，每秒会随机选取5个节点找出最久没有通信的节点发送ping消息，用于保证Gossip信息交换的随机性。每100毫秒都会扫描本地节点列表，如果发现节点最近一次接受pong消息的时间大于<code>cluster_node_timeout/2</code>，则立刻发送ping消息，防止该节点信息太长时间未更新。根据以上规则得出每个节点每秒需要发送ping消息的数量=<code>1+10*num（node.pong_received&gt;cluster_node_timeout/2）</code>。</p>
<h3 id="消息数据量"><a href="#消息数据量" class="headerlink" title="消息数据量"></a>消息数据量</h3><p>每个ping消息的数据量体现在消息头和消息体中，其中消息头主要占用空间的字段是myslots[CLUSTER_SLOTS/8]，占用2KB，这块空间占用相对固定。消息体会携带一定数量的其他节点信息用于信息交换。</p>
<figure class="highlight"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c">def get_wanted():<br>	int total_size = size(cluster.nodes) <br>	# 默认包含节点总量的1/10 594 <br>	int wanted = floor(total_size/10); <br>	if wanted &lt; 3: <br>		# 至少携带3个其他节点信息 <br>		wanted = 3; <br>	if wanted &gt; total_size -2 : <br>		# 最多包含total_size - 2个 <br>		wanted = total_size - 2; <br>	return wanted;<br></code></pre></div></td></tr></table></figure>

<h1 id="集群伸缩"><a href="#集群伸缩" class="headerlink" title="集群伸缩"></a>集群伸缩</h1><p>集群伸缩指的是为集群添加节点进行扩容和下线部分节点进行缩容</p>
<blockquote>
<p>集群伸缩 = 槽和数据在节点之间的移动  </p>
</blockquote>
<h2 id="扩容集群"><a href="#扩容集群" class="headerlink" title="扩容集群"></a>扩容集群</h2><h3 id="准备新节点"><a href="#准备新节点" class="headerlink" title="准备新节点"></a>准备新节点</h3><p>配置新的节点文件后启动节点</p>
<h3 id="加入集群"><a href="#加入集群" class="headerlink" title="加入集群"></a>加入集群</h3><p>可以使用两种方式将新节点加入集群：<br>一种是使用<code>cluster meet</code>命令，此方法在线上不建议使用，会 601 造成被加入节点的集群合并到现有集群的情况，从而造成数据丢失和错乱， 后果非常严重。<br>另一种是<code>redis-cli --cluster add-node new_host:new_port existing_host:existing_port --slave --master-id {arg}</code>命令</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">redis-cli --cluster add-node 127.0.0.1:6385 127.0.0.1:6379<br><span class="hljs-meta"><br>&gt;</span><span class="bash">&gt;&gt; Adding node 127.0.0.1:6385 to cluster 127.0.0.1:6379</span><br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379)</span><br>M: 4aed7d5835ce70cd788e8dca25ecc17a3b483d62 127.0.0.1:6379<br>   slots:[0-5460] (5461 slots) master<br>   1 additional replica(s)<br>S: d60caa155a815118ef4de33e9f71667d89cb1fc0 127.0.0.1:6384<br>   slots: (0 slots) slave<br>   replicates 4aed7d5835ce70cd788e8dca25ecc17a3b483d62<br>M: cf2e667527b1a8e1eda1cd81403a97cfb9da6a93 127.0.0.1:6381<br>   slots:[10923-16383] (5461 slots) master<br>   1 additional replica(s)<br>S: e323baf44b75e2aeb88b691f6593753a3b9224ca 127.0.0.1:6383<br>   slots: (0 slots) slave<br>   replicates cf2e667527b1a8e1eda1cd81403a97cfb9da6a93<br>S: d237450657c855a57236c52c6e6ec6e098384dec 127.0.0.1:6382<br>   slots: (0 slots) slave<br>   replicates a8942f231fc69c02640bb2e61cb0e6f659b4d218<br>M: a8942f231fc69c02640bb2e61cb0e6f659b4d218 127.0.0.1:6380<br>   slots:[5461-10922] (5462 slots) master<br>   1 additional replica(s)<br>[OK] All nodes agree about slots configuration.<br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...</span><br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span><br>[OK] All 16384 slots covered.<br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:6385 to make it join the cluster.</span><br>[OK] New node added correctly.<br></code></pre></div></td></tr></table></figure>
<h3 id="迁移槽和数据"><a href="#迁移槽和数据" class="headerlink" title="迁移槽和数据"></a>迁移槽和数据</h3><h4 id="迁移步骤："><a href="#迁移步骤：" class="headerlink" title="迁移步骤："></a>迁移步骤：</h4><ol>
<li>槽迁移计划：需要事先为新节点制定槽分配计划，保证每个节点负责相似数量的槽，从而保证各节点的数据均匀。</li>
<li>迁移数据：</li>
</ol>
<ul>
<li>在目标节点发送<code>cluster setslot {slot} importing {sourceNodeId}</code>命令，让目标节点准备导入槽的数据</li>
<li>源节点发送<code>cluster setslot {slot} migrating {targetNodeId}</code>命令，让源节点准备迁出槽的数据</li>
<li>源节点循环执行<code>cluster getkeysinslot {slot} {count}</code>命令，获取count个属于slot的键</li>
<li>在源节点上执行<code>migrate {targetIp} {targetPort} &quot;&quot; 0 {timeout} keys {keys ...}</code>，把上面获取到的键通过流水线机制批量迁移到目标节点。</li>
<li>重复执行上面两个步骤知道所有键迁移完毕</li>
<li>在集群内的所有主节点发送<code>cluster setslot {slot} node {targetNodeId}</code>命令，通知所有主节点槽分配给目标节点。</li>
</ul>
<h4 id="使用redis-cli-—cluster-1"><a href="#使用redis-cli-—cluster-1" class="headerlink" title="使用redis-cli —cluster"></a>使用redis-cli —cluster</h4><p>实际使用时，每次要操作大量的槽和对应的非常多的键，应该使用redis-cli —cluster提供的槽重分片功能</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs brainfuck"><span class="hljs-comment">redis</span><span class="hljs-literal">-</span><span class="hljs-comment">cli</span> --<span class="hljs-comment">cluster</span> <span class="hljs-comment">reshard</span> <span class="hljs-comment">host:port</span> --<span class="hljs-comment">from</span> &lt;<span class="hljs-comment">arg</span>&gt; --<span class="hljs-comment">to</span> &lt;<span class="hljs-comment">arg</span>&gt; --<span class="hljs-comment">slots</span> &lt;<span class="hljs-comment">arg</span>&gt; --<span class="hljs-comment">yes</span> --<span class="hljs-comment">timeout</span>&lt;<span class="hljs-comment">arg</span>&gt; --<span class="hljs-comment">pipeline</span> &lt;<span class="hljs-comment">arg</span>&gt;<br></code></pre></div></td></tr></table></figure>
<p>参数说明：</p>
<ul>
<li>host：port：必传参数，集群内任意节点地址，用来获取整个集群信息。 </li>
<li>from：制定源节点的id，如果有多个源节点，使用逗号分隔，如果是all源节点变为集群内所有主节点，在迁移过程中提示用户输入。</li>
<li>to：需要迁移的目标节点的id，目标节点只能填写一个，在迁移过程中提示用户输入。 </li>
<li>slots：需要迁移槽的总数量，在迁移过程中提示用户输入。 </li>
<li>yes：当打印出reshard执行计划时，是否需要用户输入yes确认后再执行reshard。</li>
<li>timeout：控制每次migrate操作的超时时间，默认为60000毫秒。 </li>
<li>pipeline：控制每次批量迁移键的数量，默认为10。<blockquote>
<p>执行redis-cli —cluster reshard 127.0.0.1:6379后，首先会要求我们输入要迁移的槽数量，然后会提示我们输入目标节点的ID，之后需要输入多个源节点ID（<strong>用done结束</strong>），最后输入yes同意迁移计划即可执行迁移工作。<br>迁移之后的槽可能没有按顺序排列，由于槽用于hash运算本身顺序没有意义，因此无须强制要求节点负责槽的顺序性。  </p>
</blockquote>
</li>
</ul>
<p>迁移成功后可以通过<code>redis-cli --cluster rebalance {newIp:newPort}</code>节点之间槽的均衡性</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">redis-cli --cluster rebalance 127.0.0.1:6380<br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6380)</span><br>[OK] All nodes agree about slots configuration.<br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...</span><br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span><br>[OK] All 16384 slots covered.<br>*** No rebalancing needed! All nodes are within the 2.00% threshold.<br></code></pre></div></td></tr></table></figure>
<p>可以看出迁移之后所有主节点负责的槽数量差异在2%以内，因此集群 节点数据相对均匀，无需调整。</p>
<h3 id="添加从节点"><a href="#添加从节点" class="headerlink" title="添加从节点"></a>添加从节点</h3><p>需要为新的主节点添加从节点从而保证整个集群的高可用。使用<code>cluster replicate{masterNodeId}</code>命令为主节点添加对应从节点</p>
<blockquote>
<p>集群模式下slaveof添加从节点操作不在支持  </p>
</blockquote>
<h2 id="收缩集群"><a href="#收缩集群" class="headerlink" title="收缩集群"></a>收缩集群</h2><p>收缩集群由两个步骤：</p>
<ol>
<li>首先需要确定下线节点是否有负责的槽，如果有，需要把槽迁移到其他节点，保证节点下线后整个集群槽节点映射的完整性</li>
<li>当下线节点不再负责槽或者本身是从节点时，就可以通知集群内其他节点忘记下线节点，当所有节点忘记该节点后可以正常关闭</li>
</ol>
<h3 id="下线迁移槽"><a href="#下线迁移槽" class="headerlink" title="下线迁移槽"></a>下线迁移槽</h3><p>需要把要下线的节点所负责的槽平均分给其他节点：<br>首先执行<code>cluster nodes</code>查看节点信息</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">127.0.0.1:6385&gt; cluster nodes<br><br>4aed7d5835ce70cd788e8dca25ecc17a3b483d62 127.0.0.1:6379@16379 master - 0 1563112011761 1 connected 1365-5460<br>a8942f231fc69c02640bb2e61cb0e6f659b4d218 127.0.0.1:6380@16380 master - 0 1563112008739 2 connected 6827-10922<br>cf2e667527b1a8e1eda1cd81403a97cfb9da6a93 127.0.0.1:6381@16381 master - 0 1563112009746 3 connected 12288-16383<br>d60caa155a815118ef4de33e9f71667d89cb1fc0 127.0.0.1:6384@16384 slave 4aed7d5835ce70cd788e8dca25ecc17a3b483d62 0 1563112010753 1 connected<br>f128f9e49e5340f9a642a07fda9934867fb41513 127.0.0.1:6385@16385 myself,master - 0 1563112010000 7 connected 0-1364 5461-6826 10923-12287<br>e323baf44b75e2aeb88b691f6593753a3b9224ca 127.0.0.1:6383@16383 slave cf2e667527b1a8e1eda1cd81403a97cfb9da6a93 0 1563112011000 3 connected<br>d237450657c855a57236c52c6e6ec6e098384dec 127.0.0.1:6382@16382 slave a8942f231fc69c02640bb2e61cb0e6f659b4d218 0 1563112010000 2 connected<br></code></pre></div></td></tr></table></figure>
<p>然后执行槽迁移，使用<code>redis-cli --cluster reshard {ip:port}(要下线的节点）</code>让其他每个主节点平均接管槽，同样需要输入槽数量，要接受的节点Id，源节点ID。</p>
<h3 id="忘记节点"><a href="#忘记节点" class="headerlink" title="忘记节点"></a>忘记节点</h3><p>忘记节点就是让其他节点不再与要下线节点进行Gossip消息交换。Redis提供<code>cluster forget {downNodeId}</code>命令实现该功能<br>当节点接收到cluster forget{down NodeId}命令后，会把nodeId指定的节点加入到禁用列表中，在禁用列表内的节点不再发送Gossip消息。禁用列表有效期是60秒，超过60秒节点会再次参与消息交换。也就是说当第一次forget命令发出后，有60秒的时间让集群内的所有节点忘记下线节点。<br>线上一般不建议使用该命令，因为需要跟大量节点交互，容易遗漏，超时<br>可以使用<code>redis-cli --cluster del-node {host:port} {downNodeId}</code>命令<br>当下线主节点具有从节点时需要把该从节点指向到其他主节点，因此对于主从节点都下线的情况，建议先下线从节点再下线主节点，防止不必要的全量复制。</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">redis-cli --cluster del-node 127.0.0.1:6379 f128f9e49e5340f9a642a07fda9934867fb41513<br><span class="hljs-meta"><br>&gt;</span><span class="bash">&gt;&gt; Removing node f128f9e49e5340f9a642a07fda9934867fb41513 from cluster 127.0.0.1:6379</span><br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER FORGET messages to the cluster...</span><br><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; SHUTDOWN the node.</span><br></code></pre></div></td></tr></table></figure>

<h1 id="请求路由"><a href="#请求路由" class="headerlink" title="请求路由"></a>请求路由</h1><p>在集群模式下，Redis接收任何键相关命令时首先计算键对应的槽，再根据槽找出所对应的节点，如果节点是自身，则处理键命令；否则回复MOVED重定向错误，通知客户端请求正确的节点。这个过程称为MOVED重定向。可以通过<code>cluster keyslot {key}</code>返回key对应的槽：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">127.0.0.1:6379&gt; set hello1 world<br>(error) MOVED 11613 127.0.0.1:6381<br>127.0.0.1:6379&gt; cluster keyslot hello1<br>(integer) 11613<br></code></pre></div></td></tr></table></figure>
<p>在使用redis-cli时，可以加入<code>-c</code>参数支持自动重定向</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">redis-cli -c<br><br>127.0.0.1:6379&gt; set hello1 world<br><span class="hljs-meta"><br>-&gt;</span><span class="bash"> Redirected to slot [11613] located at 127.0.0.1:6381</span><br>OK<br><br>127.0.0.1:6381&gt;<br></code></pre></div></td></tr></table></figure>
<p>redis-cli自动帮我们连接到正确的节点执行命令，这个过程是在redis-cli内部维护，实质上是client端接到MOVED信息之后再次发起请求，并不在Redis节点中完成请求转发,节点对于不属于它的键命令只回复重定向响应，并不负责转发。<br>键命令执行步骤分为两部：计算槽和查找槽对应的节点</p>
<ol>
<li>计算槽<br>根据键的有效部分使用CRC16函数计算出散列值，再取对16383的余数，使每个键都可以映射到0~16383槽范围内。<blockquote>
<p>如果键内容包含{和}大括号字符，则计算槽的有效部分是<strong>括号内的内容</strong>；否则采用键的全内容计算槽。  </p>
</blockquote>
</li>
</ol>
<p>键内部使用大括号包含的内容又叫做hash_tag，它提供不同的键可以具备相同slot的功能，常用于Redis IO优化。如在集群模式下使用mget等命令优化批量调用时，键列表必须具有相同的slot，否则会报错。这时可以利用hash_tag让不同的键具有相同的slot达到优化的目的。<br>2. 查找槽对应的节点<br>集群内通过消息交换每个节点都会知道所有节点的槽信息，内部保存在clusterState结构中，结构所示：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterState</span> &#123;</span><br>	clusterNode *myself; <span class="hljs-comment">/* 自身节点,clusterNode代表节点结构体 */</span><br>	clusterNode *slots[CLUSTER_SLOTS]; <span class="hljs-comment">/* 16384个槽和节点映射数组，数组下标代表对应的槽 */</span> <br>	... <br>&#125; clusterState;<br></code></pre></div></td></tr></table></figure>
<p>根据MOVED重定向机制，客户端可以随机连接集群内任一Redis获取键所在节点，这种客户端又叫Dummy（傀儡）客户端，它优点是代码实现简单，对客户端协议影响较小，只需要根据重定向信息再次发送请求即可。但是它的弊端很明显，每次执行键命令前都要到Redis上进行重定向才能找到要执行命令的节点，额外增加了IO开销。</p>
<h2 id="Smart客户端"><a href="#Smart客户端" class="headerlink" title="Smart客户端"></a>Smart客户端</h2><p>大多数开发语言的Redis客户端都采用Smart客户端支持集群协议，Smart客户端通过在内部维护slot→node的映射关系，本地就可实现键到节点的查找，从而保证IO效率的最大化，而MOVED重定向负责协助Smart客户端更新slot→node映射。</p>
<h2 id="ASK重定向"><a href="#ASK重定向" class="headerlink" title="ASK重定向"></a>ASK重定向</h2><p>当集群正在执行槽迁移时，此时一部分数据在源节点，一部分数据在目标节点。若在此时发送键命令，如果该键已经迁移到目标节点，则会回复ASK重定向异常。</p>
<h3 id="具体流程："><a href="#具体流程：" class="headerlink" title="具体流程："></a>具体流程：</h3><ol>
<li>客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直接执行并返回结果给客户端。</li>
<li>如果键对象不存在，则可能存在于目标节点，这时源节点会回复ASK重定向异常。格式如下：<code>（error）ASK {slot} {targetIP}：{targetPort}</code></li>
<li>客户端从ASK重定向异常提取出目标节点信息，发送asking命令到目标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返回不存在信息。</li>
</ol>
<blockquote>
<p>ASK与MOVED虽然都是对客户端的重定向控制，但是有着本质区别。ASK重定向说明集群正在进行slot数据迁移，客户端无法知道什么时候迁移完成，因此只能是临时性的重定向，客户端不会更新slots缓存。但是MOVED重定向说明键对应的槽已经明确指定到新的节点，因此需要更新slots缓存。  </p>
</blockquote>
<h3 id="节点内部支持"><a href="#节点内部支持" class="headerlink" title="节点内部支持"></a>节点内部支持</h3><p>源节点和目标节点在内部的<strong>clusterState</strong>结构中维护着当前正在迁移的槽信息，用于识别槽迁移情况:</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterState</span> &#123;</span><br>	clusterNode *myself; <span class="hljs-comment">/* 自身节点 / <br>	clusterNode *slots[CLUSTER_SLOTS]; /* 槽和节点映射数组 */</span> <br>	clusterNode *migrating_slots_to[CLUSTER_SLOTS];<span class="hljs-comment">/* 正在迁出的槽节点数组 */</span> <br>	clusterNode *importing_slots_from[CLUSTER_SLOTS];<span class="hljs-comment">/* 正在迁入的槽节点数组*/</span> <br>	... <br>&#125; clusterState;<br></code></pre></div></td></tr></table></figure>
<p>节点每次收到键命令时，都会根据clusterState内的迁移属性进行命令处理：</p>
<ul>
<li>如果键所在的槽由当前节点负责，但键不存在则查找<code>migrating_slots_to</code>数组查看槽是否正在迁出，如果是返回ASK重定向。</li>
<li>如果客户端发送asking命令打开了<code>CLIENT_ASKING</code>标识，则该客户端下次发送键命令时查找importing_slots_from数组获取clusterNode，如果指向自身则执行命令。</li>
<li>asking命令是一次性命令，每次执行完后客户端标识都会修改回原状态，因此每次客户端接收到ASK重定向后都需要发送asking命令。</li>
<li>批量操作。ASK重定向对单键命令支持得很完善，但是，使用批量操作，如mget或pipeline。当槽处于迁移状态时，批量操作会受到影响。</li>
</ul>
<blockquote>
<p>集群环境下对于使用批量操作的场景，优先使用Pipeline方式，在客户端实现对ASK重定向的正确处理，这样既可以受益于批量操作的IO优化，又可以兼容slot迁移场景。  </p>
</blockquote>
<h1 id="故障迁移"><a href="#故障迁移" class="headerlink" title="故障迁移"></a>故障迁移</h1><p>当集群内少量节点出现故障时通过自动故障转移保证集群可以正常对外提供服务</p>
<h2 id="故障发现"><a href="#故障发现" class="headerlink" title="故障发现"></a>故障发现</h2><p>Redis集群内节点通过ping/pong消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其他状态如：主从状态、节点故障等。因此故障发现也是通过消息传播机制实现的，主要环节包括：主观下线（pfail）和客观下线（fail）</p>
<ul>
<li>主观下线：指某个节点认为另一个节点不可用，即下线状态，这个状态并不是最终的故障判定，只能代表一个节点的意见，可能存在误判情况。</li>
<li>客观下线：指标记一个节点真正的下线，集群内多个节点都认为该节点不可用，从而达成共识的结果。如果是持有槽的主节点故障，需要为该节点进行故障转移。</li>
</ul>
<h3 id="主观下线"><a href="#主观下线" class="headerlink" title="主观下线"></a>主观下线</h3><p>集群中每个节点都会定期向其他节点发送ping消息，接收节点回复pong消息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接收节点标记为主观下线（pfail）状态。<br>具体流程：</p>
<ol>
<li>节点a发送ping消息给节点b，如果通信正常将接收到pong消息，节点a更新最近一次与节点b的通信时间。</li>
<li>如果节点a与节点b通信出现问题则断开连接，下次会进行重连。如果一直通信失败，则节点a记录的与节点b最后通信时间将无法更新。</li>
<li>节点a内的定时任务检测到与节点b最后通信时间超过<code>cluster-node-timeout</code>时，更新本地对节点b的状态为主观下线（pfail）。</li>
</ol>
<h3 id="客观下线"><a href="#客观下线" class="headerlink" title="客观下线"></a>客观下线</h3><p>当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播。ping/pong消息的消息体会携带集群1/10的其他节点状态数据，当接受节点发现消息体中含有主观下线的节点状态时，会在本地找到故障节点的ClusterNode结构，保存到下线报告链表中。结构如下：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterNode</span> &#123;</span> <span class="hljs-comment">/* 认为是主观下线的clusterNode结构 */</span> <br>	<span class="hljs-built_in">list</span> *fail_reports; <span class="hljs-comment">/* 记录了所有其他节点对该节点的下线报告 */</span> <br>	... <br>&#125;;<br></code></pre></div></td></tr></table></figure>
<p>通过Gossip消息传播，集群内节点不断收集到故障节点的下线报告。当半数以上<strong>持有槽的主节点</strong>都标记某个节点是主观下线时，触发客观下线流程。</p>
<blockquote>
<p>集群模式下只有处理槽的主节点才负责读写请求和集群槽等关键信息维护，而从节点只进行主节点数据和状态信息的复制。所以必须是负责槽的主节点参与故障发现决策  </p>
</blockquote>
<p>客观下线流程：</p>
<ol>
<li>当消息体内含有其他节点的pfail状态会判断发送节点的状态，如果发送节点是主节点则对报告的pfail状态处理，从节点则忽略。</li>
<li>找到pfail对应的节点结构，更新clusterNode内部下线报告链表。</li>
<li>根据更新后的下线报告链表告尝试进行客观下线。</li>
</ol>
<h4 id="下线报告链表"><a href="#下线报告链表" class="headerlink" title="下线报告链表"></a>下线报告链表</h4><p>每个ClusterNode结构中都会存在一个下线链表结构，保存了其他主节点针对当前节点的下线报告：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterNodeFailReport</span> &#123;</span> <br>	<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterNode</span> *<span class="hljs-title">node</span>;</span> <span class="hljs-comment">/* 报告该节点为主观下线的节点 */</span> <br>	<span class="hljs-keyword">mstime_t</span> time; <span class="hljs-comment">/* 最近收到下线报告的时间 */</span> <br>&#125; clusterNodeFailReport;<br></code></pre></div></td></tr></table></figure>
<p>下线报告中保存了报告故障的节点结构和最近收到下线报告的时间，当接收到fail状态时，会维护对应节点的下线上报链表。每个下线报告都存在有效期，每次在尝试触发客观下线时，都会检测下线报告是否过期，对于过期的下线报告将被删除。如果在<code>cluster-node-time*2</code>的时间内该下线报告没有得到更新则过期并删除.</p>
<h4 id="尝试客观下线"><a href="#尝试客观下线" class="headerlink" title="尝试客观下线"></a>尝试客观下线</h4><p>集群中的节点每次接收到其他节点的pfail状态，都会尝试触发客观下线：</p>
<ol>
<li>首先统计有效的下线报告数量，如果小于集群内持有槽的主节点总数的一半则退出。</li>
<li>当下线报告大于槽主节点数量一半时，标记对应故障节点为客观下线状态。</li>
<li>向集群广播一条fail消息，通知所有的节点将故障节点标记为客观下线，fail消息的消息体只包含故障节点的ID。<br>广播fail消息有一下两个作用：</li>
</ol>
<ul>
<li>通知集群内所有的节点标记故障节点为客观下线状态并立即生效</li>
<li>通知故障节点的从节点触发故障转移流程</li>
</ul>
<h2 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h2><p>故障节点变为客观下线后，如果下线节点是<strong>持有槽的主节点</strong>，则需要在它的从节点中选出一个替换它，从而保证集群的高可用。下线主节点的所有从节点承担故障恢复的义务，当从节点通过内部定时任务发现自身复制的主节点进入客观下线时，将会触发故障恢复流程：</p>
<ol>
<li>资格检查</li>
<li>准备选举时间</li>
<li>发起选举</li>
<li>选举投票</li>
<li>替换主节点</li>
</ol>
<h3 id="资格检查"><a href="#资格检查" class="headerlink" title="资格检查"></a>资格检查</h3><p>每个从节点都要检查最后与主节点断线时间，判断是否有资格替换故障的主节点。如果从节点与主节点断线时间超过<code>cluster-node-time*cluster-slave-validity-factor</code>，则当前从节点不具备故障转移资格。参数cluster-slavevalidity-factor用于从节点的有效因子，默认为10。</p>
<h3 id="准备选举时间"><a href="#准备选举时间" class="headerlink" title="准备选举时间"></a>准备选举时间</h3><p>当从节点符合故障转移资格后，更新触发故障选举的时间，只有到达该时间后才能执行后续流程。故障选举时间相关字段如下：</p>
<figure class="highlight c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">clusterState</span> &#123;</span> <br>	... <br>	<span class="hljs-keyword">mstime_t</span> failover_auth_time; <span class="hljs-comment">/* 记录之前或者下次将要执行故障选举时间 */</span> <br>	<span class="hljs-keyword">int</span> failover_auth_rank; <span class="hljs-comment">/* 记录当前从节点排名 */</span> <br>&#125;<br></code></pre></div></td></tr></table></figure>
<p>采用延迟触发机制，主要是通过对多个从节点使用不同的延迟选举时间来支持优先级问题。复制偏移量越大说明从节点延迟越低，那么它应该具有更高的优先级来替换故障主节点。<br><img src="https://github.com/xiao-ming9/xiao-ming9.github.io/blob/master/images/redis%E9%80%89%E4%B8%BE%E5%BB%B6%E8%BF%9F%E5%87%BA%E5%8F%91%E6%9C%BA%E5%88%B6.png?raw=true" alt=""></p>
<h3 id="发起选举"><a href="#发起选举" class="headerlink" title="发起选举"></a>发起选举</h3><p>当从节点定时任务检测到达故障选举时间（failover_auth_time）到达后，发起选举流程如下：</p>
<ol>
<li>更新配置纪元：<br>配置纪元是一个只增不减的整数，每个主节点自身维护一个配置纪元（clusterNode.configEpoch）标示当前主节点的版本，所有主节点的配置纪元都不相等，从节点会复制主节点的配置纪元。整个集群又维护一个全局的配置纪元（clusterState.current Epoch），用于记录集群内所有主节点配置纪元 的最大版本。执行cluster info命令可以查看配置纪元信息。配置纪元会跟随ping/pong消息在集群内传播，当发送方与接收方都是主节点且配置纪元相等时代表出现了冲突，nodeId更大的一方会递增全局配置纪元并赋值给当前节点来区分冲突。</li>
</ol>
<p><strong>配置纪元的主要作用：</strong></p>
<ul>
<li>标示集群内每个主节点的不同版本和当前集群最大的版本。</li>
<li>每次集群发生重要事件时，这里的重要事件指出现新的主节点（新加入的或者由从节点转换而来），从节点竞争选举。都会递增集群全局的配置纪元并赋值给相关主节点，用于记录这一关键事件。</li>
<li>主节点具有更大的配置纪元代表了更新的集群状态，因此当节点间进行ping/pong消息交换时，如出现slots等关键信息不一致时，以配置纪元更大的一方为准，防止过时的消息状态污染集群。</li>
</ul>
<p>从节点每次发起投票时都会自增集群的全局配置纪元，并单独保存在clusterState.failover_auth_epoch变量中用于标识本次从节点发起选举的版本。<br>2. 广播选举消息<br>在集群内广播选举消息（FAILOVER_AUTH_REQUEST），并记录已发送过消息的状态，保证该从节点在一个配置纪元内只能发起一次选举。消息内容如同ping消息只是将type类型变为FAILOVER_AUTH_REQUEST。</p>
<h3 id="选举投票"><a href="#选举投票" class="headerlink" title="选举投票"></a>选举投票</h3><p>只有持有槽的主节点才会处理故障选举消息（<code>FAILOVER_AUTH_REQUEST</code>），因为每个持有槽的节点在一个配置纪元内都有唯一的一张选票，当接到第一个请求投票的从节点消息时回复FAILOVER_AUTH_ACK消息作为投票，之后相同配置纪元内其他从节点的选举消息将忽略。<br>当从节点收集到N/2+1个持有槽的主节点投票时，从节点可以执行替换主节点操作。</p>
<ul>
<li>投票作废：每个配置纪元代表了一次选举周期，如果在开始投票之后的<code>cluster-node-timeout*2</code>时间内从节点没有获取足够数量的投票，则本次选举作废。从节点对配置纪元自增并发起下一轮投票，直到选举成功为止。</li>
</ul>
<h3 id="替换主节点"><a href="#替换主节点" class="headerlink" title="替换主节点"></a>替换主节点</h3><ol>
<li>当前从节点取消复制变为主节点。</li>
<li>执行clusterDelSlot操作撤销故障主节点负责的槽，并执行clusterAddSlot把这些槽委派给自己。</li>
<li>向集群广播自己的pong消息，通知集群内所有的节点当前从节点变为主节点并接管了故障主节点的槽信息。</li>
</ol>
<h1 id="集群运维"><a href="#集群运维" class="headerlink" title="集群运维"></a>集群运维</h1><h2 id="集群完整性"><a href="#集群完整性" class="headerlink" title="集群完整性"></a>集群完整性</h2><p>默认情况下只有当集群中16384个槽都指派了节点时，真个集群才能够使用，否则不可用。此时，从故障发现到自动 完成转移期间整个集群是不可用状态，可以通过修改配置参数<code>cluster-require-full-coverage</code>配置为no，当主节点故障时，只影响它负责槽的相关命令，不影响其他主节点的可用性。</p>
<h2 id="带宽消耗"><a href="#带宽消耗" class="headerlink" title="带宽消耗"></a>带宽消耗</h2><p>集群的带宽消耗主要分为：<strong>读写命令消耗+Gossip消息消耗</strong>，其中节点间消息通信对带宽的消耗体现在以下几个方面：</p>
<ul>
<li>消息发送频率：跟cluster-node-timeout密切相关，当节点发现与其他节点最后通信时间超过cluster-node-timeout/2时会直接发送ping消息。</li>
<li>消息数据量：每个消息主要的数据占用包含：slots槽数组（2KB空 间）和整个集群1/10的状态数据（10个节点状态数据约1KB）。</li>
<li>节点部署的机器规模：机器带宽的上线是固定的，因此相同规模的集群分布的机器越多每台机器划分的节点越均匀，则集群内整体的可用带宽越高。</li>
</ul>
<h2 id="集群倾斜"><a href="#集群倾斜" class="headerlink" title="集群倾斜"></a>集群倾斜</h2><p>指不同节点之间数据量和请求量出现明显差异，包括数据倾斜和请求倾斜</p>
<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><ol>
<li>节点和槽分配严重不均<br>可以使用<code>redis-cli --cluster info {host:ip}</code>查看所有节点负责的槽和键总量以及每个槽平均键数量，当节点对应的槽数量不均匀时，可以使用<code>redis-cli --cluster rebalance {host:ip}</code>进行平衡</li>
<li>不同槽对应键数量差异过大<br>键通过CRC16哈希函数映射到槽上， 正常情况下槽内键数量会相对均匀。但当大量使用hash_tag时，会产生不同的键映射到同一个槽的情况。通过命令：<code>cluster countkeysinslot {slot}</code>可以获取槽对应的键数量，识别出哪些槽映射了过多的键。再通过命令<code>cluster getkeysinslot {slot} {count}</code>循环迭代出槽下所有的键。从而发现过度使用 hash_tag的键</li>
<li>集合对象包含大量元素。</li>
<li>内存相关配置不一致。</li>
</ol>
<h3 id="请求倾斜"><a href="#请求倾斜" class="headerlink" title="请求倾斜"></a>请求倾斜</h3><p>集群内特定节点请求量/流量过大将导致节点之间负载不均。</p>
<h3 id="手动故障转移"><a href="#手动故障转移" class="headerlink" title="手动故障转移"></a>手动故障转移</h3><p>Redis集群提供手动故障转移功能：指定从节点发起转移流程，主从节点角色进行切换，从节点变为新的主节点对外提供服务，旧的主节点变为它的从节点。<br>通过在从节点上执行<code>cluster failover</code>命令发起转移流程，默认情况下转移期间客户端请求会有短暂的阻塞，但不会丢失数据。<br>具体流程：</p>
<ol>
<li>从节点通知主节点停止处理所有客户端请求。</li>
<li>主节点发送对应从节点延迟复制的数据</li>
<li>节点接收处理复制延迟的数据，直到主从复制偏移量一致为止， 保证复制数据不丢失。</li>
<li>从节点立刻发起投票选举（这里不需要延迟触发选举）。选举成功后断开复制变为新的主节点，之后向集群广播主节点pong消息</li>
<li>旧主节点接受到消息后更新自身配置变为从节点，解除所有客户端请求阻塞，这些请求会被重定向到新主节点上执行。</li>
<li>旧主节点变为从节点后，向新的主节点发起全量复制流程。</li>
</ol>
<h3 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h3><p>把单机Redis数据迁移到集群环境：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">redis-trib.rb import host:port --from &lt;arg&gt; --copy --replace<br></code></pre></div></td></tr></table></figure>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a>
                    
                      <a class="hover-with-bg" href="/tags/Redis/">Redis</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2019/07/16/SpringBoot/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Spring boot</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2019/07/11/Redis%E5%93%A8%E5%85%B5/">
                        <span class="hidden-mobile">Redis哨兵</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments">
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'xiao-ming9/xiao-ming9.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        粤ICP备18114217号
      </a>
    </span>
    
      
        <span>
          <a
            href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=粤ICP备18114217号-1"
            rel="nofollow noopener"
            class="beian-police"
            target="_blank"
          >
            
              <span style="visibility: hidden; width: 0">|</span>
              <img src="/img/police_beian.png" alt="police-icon"/>
            
            <span>粤ICP备18114217号-1</span>
          </a>
        </span>
      
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->




  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?54ebb03ad7ad5b762ac8ff7958df6d3f";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'G-M2RT7SDT3L', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  
    <!-- Google gtag.js -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-M2RT7SDT3L"></script>
    <script defer>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-M2RT7SDT3L');
    </script>
  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
