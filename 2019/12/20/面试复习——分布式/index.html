<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="title: 分布式date: 2019-11-15 16:10:17tags: [面试,分布式] 分布式和集群的区别是什么？ 分布式：一个业务分拆多个子业务，部署在不同的服务器上，不同的业务模块部署在不同的服务器上或者同一个业务模块分拆多个子业务，部署在不同的服务器上，解决高并发的问题 集群：同一个业务，部署在多个服务器上，提高系统可用性。">
<meta property="og:type" content="article">
<meta property="og:title" content="面试复习——分布式">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;20&#x2F;%E9%9D%A2%E8%AF%95%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F&#x2F;index.html">
<meta property="og:site_name" content="Silverming">
<meta property="og:description" content="title: 分布式date: 2019-11-15 16:10:17tags: [面试,分布式] 分布式和集群的区别是什么？ 分布式：一个业务分拆多个子业务，部署在不同的服务器上，不同的业务模块部署在不同的服务器上或者同一个业务模块分拆多个子业务，部署在不同的服务器上，解决高并发的问题 集群：同一个业务，部署在多个服务器上，提高系统可用性。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;cap%E5%AE%9A%E7%90%86">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;base%E7%90%86%E8%AE%BA%E4%B8%89%E8%A6%81%E7%B4%A0">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8F%90%E4%BA%A4%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8F%90%E4%BA%A4%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%88%90%E5%8A%9F%E5%9C%BA%E6%99%AF.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%9B%9E%E6%BB%9A%E5%9C%BA%E6%99%AF.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B2.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B3.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B4.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B5.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B6.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;MQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;MQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF2">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Sagas%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E2%80%94%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA.jpeg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Learner%E5%AD%A6%E4%B9%A0%E9%80%89%E5%AE%9Avalue%E6%96%B9%E6%A1%88.webp">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Paxos%E7%AE%97%E6%B3%95%E6%B4%BB%E6%80%A7.webp">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E4%BB%BB%E6%9C%9F.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F.jpeg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-3.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-4.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-5.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-6.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-7.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft-leader%E5%92%8Cfollower%E6%97%A5%E5%BF%97%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E6%83%85%E5%86%B5.jpeg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%20%E6%8F%90%E4%BA%A4%E9%99%90%E5%88%B6.jpeg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-3.jpeg">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;raft%E6%97%A5%E5%BF%97%E5%BF%AB%E7%85%A7.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Sticky%20Session.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Session%20Replication.png">
<meta property="og:image" content="https:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;Session%20Server.png">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E5%BC%80%E6%BA%90%E7%BD%91%E5%85%B3Zuul%E6%9E%B6%E6%9E%84">
<meta property="og:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;%E7%86%94%E6%96%AD%E6%9C%BA%E5%88%B6.jpg">
<meta property="og:updated_time" content="2022-07-12T16:15:46.887Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;qiniu.xiaoming.net.cn&#x2F;cap%E5%AE%9A%E7%90%86">

<link rel="canonical" href="http://yoursite.com/2019/12/20/%E9%9D%A2%E8%AF%95%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>面试复习——分布式 | Silverming</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Silverming</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Stay hungry,stay foolish</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/20/%E9%9D%A2%E8%AF%95%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Silverming">
      <meta itemprop="description" content="Wechat:934933088">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Silverming">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面试复习——分布式
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-20 20:05:10" itemprop="dateCreated datePublished" datetime="2019-12-20T20:05:10+08:00">2019-12-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-07-13 00:15:46" itemprop="dateModified" datetime="2022-07-13T00:15:46+08:00">2022-07-13</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>43k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>39 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>title: 分布式<br>date: 2019-11-15 16:10:17<br>tags: [面试,分布式]</p>
<h1 id="分布式和集群的区别是什么？"><a href="#分布式和集群的区别是什么？" class="headerlink" title="分布式和集群的区别是什么？"></a>分布式和集群的区别是什么？</h1><ul>
<li><strong>分布式</strong>：一个业务分拆多个子业务，部署在不同的服务器上，不同的业务模块部署在不同的服务器上或者同一个业务模块分拆多个子业务，部署在不同的服务器上，解决高并发的问题</li>
<li><strong>集群</strong>：同一个业务，部署在多个服务器上，提高系统可用性。</li>
</ul>
<a id="more"></a>

<h1 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h1><p><img src="http://qiniu.xiaoming.net.cn/cap%E5%AE%9A%E7%90%86" alt="CAP定理"></p>
<p>CAP定义，又称为布鲁尔定理，它指对于一个分布式计算系统来说，不可能同时满足以下三点：</p>
<ul>
<li><strong>一致性（Consistence）</strong>：系统在执行过某项操作后仍然处于一致的状态。在分布式系统中，更新操作执行成功后所有的用户都应该读到最新的值，这样的系统被认为是具有强一致性的。 等同于所有节点访问同一份最新的数据副本</li>
<li><strong>可用性（Availability）</strong>：每一个操作总是能够在一定的时间内返回结果，这里需要注意的是”一定时间内”和”返回结果”。一定时间指的是，在可以容忍的范围内返回结果，结果可以是成功或者失败。对数据更新具备高可用性。每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据</li>
<li><strong>分区容错性（Partition tolerance）</strong>：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。这里的网络分区是指由于某种原因，网络被分成若干个孤立的区域，而区域之间互不相通。还有一些人将分区容错性理解为系统对节点动态加入和离开的能力，因为节点的加入和离开可以认为是集群内部的网络分区。</li>
</ul>
<p>CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，可以不仅仅局限在CAP问题上。</p>
<p>当发生网络分区的时候，如果要继续服务，那么<strong>强一致性和可用性只能2选1</strong>。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）是必须要实现的。</p>
<h1 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h1><p>BASE 是 Basically Available（基本可用）、Soft-state（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。BASE 理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了对系统的要求。</p>
<h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p>
<p>针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：</p>
<ul>
<li>按功能划分数据库</li>
<li>分片（如开源的Mycat、Amoeba等）。</li>
</ul>
<h2 id="BASE-理论三要素"><a href="#BASE-理论三要素" class="headerlink" title="BASE 理论三要素"></a>BASE 理论三要素</h2><p><img src="http://qiniu.xiaoming.net.cn/base%E7%90%86%E8%AE%BA%E4%B8%89%E8%A6%81%E7%B4%A0" alt="BASE理论三要素"></p>
<h3 id="1-基本可用"><a href="#1-基本可用" class="headerlink" title="1. 基本可用"></a>1. 基本可用</h3><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这不等价于系统不可用。例如：</p>
<ul>
<li><strong>响应时间上的损失</strong>：正常情况下，一个在线搜索引擎需要在0.5秒内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加1-2秒</li>
<li><strong>系统功能上的损失</strong>：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。</li>
</ul>
<h3 id="2-软状态"><a href="#2-软状态" class="headerlink" title="2. 软状态"></a>2. 软状态</h3><p>软状态是指允许系统中的数据存在中间状态，并认为该中间状体的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</p>
<h3 id="3-最终一致性"><a href="#3-最终一致性" class="headerlink" title="3. 最终一致性"></a>3. 最终一致性</h3><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一直，而不需要实时保证系统数据的强一致性。</p>
<h1 id="分布式系统设计的两大思路"><a href="#分布式系统设计的两大思路" class="headerlink" title="分布式系统设计的两大思路"></a>分布式系统设计的两大思路</h1><p>分布式系统设计的两大思路：中心化和去中心化</p>
<h2 id="中心化设计"><a href="#中心化设计" class="headerlink" title="中心化设计"></a>中心化设计</h2><ul>
<li><strong>两个角色</strong>：中心化的设计思想很简单，分布式集群中的节点机器按照角色分工，大体上分为两种角色：“领导”和“干活的”</li>
<li><strong>角色职责</strong>：“领导”通常负责分发任务并监督“干活的”，发现谁空闲或者相对太闲，就想方设法给其安排新任务，确保没有一个“干活的”能够偷懒，如果“领导”发现某个“干活的”崩溃了，会直接将其踢除，然后把它的任务分给其他人。</li>
<li><strong>中心化设计的问题</strong>：</li>
</ul>
<ol>
<li>中心化的设计存在的最大问题就是“领导”的安危问题，如果“领导”出了问题，则群龙无首，整个集群就崩溃了，但是难以同时安排两个“领导”以避免单点问题。</li>
<li>中心化设计还存在的另一个潜在的问题：即“领导”的能力问题：可以领导10个人高效工作并不意味着可以领到100个人高效工作，如果系统设计和实现不好，问题就会卡在“领导”身上。</li>
</ol>
<ul>
<li><strong>领导安危问题解决方法</strong>：大多数中心化系统都采用了主备两个“领导”的设计方案，可以是热备或者冷备，也可以是自动切换或者手动切换，而且越来越多的新系统都开始具备自动选取切换“领导”的能力，以提升系统的可用性。</li>
</ul>
<h2 id="去中心化设计"><a href="#去中心化设计" class="headerlink" title="去中心化设计"></a>去中心化设计</h2><ul>
<li><strong>众生地位平等</strong>：在去中心化的设计里，通常没有“领导”和“干活的”这两种角色的区分，大家的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备宕机，都只会影响很小范围的功能。</li>
<li><strong>“去中心化”不是不要中心，而是由节点来自由选择中心</strong>：集群的成员会自发的举行“会议”选举新的“领导”主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd</li>
<li><strong>去中心化设计的问题</strong>：去中心化设计里最难解决的一个问题是<strong>脑裂问题</strong>，这种情况的发生概率很低，但影响很大。脑裂指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生严重的数据冲突和错误。一般的设计思路是，当集群判断发生了脑裂问题时，规模较小的集群就“自杀”或者拒绝服务。</li>
</ul>
<h1 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h1><p>分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。</p>
<h2 id="分布式事务和分布式锁的区别"><a href="#分布式事务和分布式锁的区别" class="headerlink" title="分布式事务和分布式锁的区别"></a>分布式事务和分布式锁的区别</h2><p>分布式锁和分布式事务区别：</p>
<ul>
<li>锁问题的关键在于进程操作的互斥关系，例如多个进程同时修改账户的余额，如果没有互斥关系则会导致该账户的余额不正确。</li>
<li>而事务问题的关键则在于事务涉及的一系列操作需要满足 ACID 特性，例如要满足原子性操作则需要这些操作要么都执行，要么都不执行。</li>
</ul>
<h2 id="分布式事务产生的原因"><a href="#分布式事务产生的原因" class="headerlink" title="分布式事务产生的原因"></a>分布式事务产生的原因</h2><h3 id="数据库分库分表"><a href="#数据库分库分表" class="headerlink" title="数据库分库分表"></a>数据库分库分表</h3><p>当数据库单表一年产生的数据超过1000W时，就需要考虑分库分表，就是说将原来的数据库变成多个数据库。这时候，如果一个操作既访问01库，又访问02库，而且还要保证数据的一致性，那么就需要用到分布式事务。</p>
<h3 id="应用SOA化"><a href="#应用SOA化" class="headerlink" title="应用SOA化"></a>应用SOA化</h3><p>所谓的SOA化，就是业务的服务化。比如原来单机支撑了整个电商网站，现在对整个网站进行拆解，分离出了订单中心、用户中心、库存中心。对于订单中心，有专门的数据库存储订单信息，用户中心也有专门的数据库存储用户信息，库存中心也会有专门的数据库存储库存信息。这时候如果要同时对订单和库存进行操作，那么就会涉及到订单数据库和库存数据库，为了保证数据一致性，就需要用到分布式事务。</p>
<h2 id="常见的分布式事务解决方案"><a href="#常见的分布式事务解决方案" class="headerlink" title="常见的分布式事务解决方案"></a>常见的分布式事务解决方案</h2><p>基于数据库资源层面实现方案，由于存在多个事务，需要存在一个角色管理各个事务的状态。我们将这个角色称为协调者，事务参与者称为参与者。参与者与协调者一般会基于某种特定协议，目前比较有名的为 XA 接口协议。基于协调者与参与者的思想设定，分别提出了 2PC 与 3PC 实现XA 分布式事务。</p>
<h3 id="基于XA协议的两阶段提交（2PC）"><a href="#基于XA协议的两阶段提交（2PC）" class="headerlink" title="基于XA协议的两阶段提交（2PC）"></a>基于XA协议的两阶段提交（2PC）</h3><p>XA 是一个分布式事务协议，XA中大致分为两部分：事务管理器（协调者）和本地资源管理器。其中本地资源管理器往往由数据库实现，比如 Oracle、DB2 这些商业数据库都实现了 XA 接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。主要过程如下：</p>
<h4 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h4><p><img src="http://qiniu.xiaoming.net.cn/%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8F%90%E4%BA%A4%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5" alt="两段式提交第一阶段"></p>
<p>应用程序调用了事务管理器的提交方法，此后第一阶段分为两个步骤：</p>
<ul>
<li>事务管理器通知参与该事务的各个资源管理器，通知他们准备事务</li>
<li>资源管理器接收到消息后开始准备阶段，写好 <code>Undo</code> 和 <code>Redo</code> 事务日志并执行事务，但不提交，然后将是否就绪的消息返回给事务管理器（此时已经将事务的大部分事情做完，以后的内容耗时极小）。</li>
</ul>
<h4 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h4><p><img src="http://qiniu.xiaoming.net.cn/%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8F%90%E4%BA%A4%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5" alt="两段式提交第二阶段"></p>
<p>第二阶段也分为两个步骤：</p>
<ul>
<li>事务管理器在接收各个消息后，开始分析，如果有任意其一失败，则发送回滚命令，否则发送提交命令。</li>
<li>各个资源管理器接收到命令后，执行（耗时很少），并将提交信息返回给事务管理器。</li>
</ul>
<p>事务管理器接收消息后，事务结束，应用程序继续执行。</p>
<p>以下是成功和回滚两种场景示例：</p>
<p><img src="https://qiniu.xiaoming.net.cn/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%88%90%E5%8A%9F%E5%9C%BA%E6%99%AF.jpg" alt="分布式事务提交成功场景"></p>
<p><img src="https://qiniu.xiaoming.net.cn/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%9B%9E%E6%BB%9A%E5%9C%BA%E6%99%AF.jpg" alt="分布式事务回滚场景"></p>
<p><strong>为什么要分两步执行</strong>：一是因为分两步，就有了事务管理器统一管理的机会；二是尽可能晚提交事务，让事务在提交前尽可能地完成所有能完成的工作，这样，最后的提交阶段将是耗时极短，耗时极短意味着操作失败的可能性也就降低。同时，二阶段提交协议为了保证事务的一致性，不管是事务管理器还是各个资源管理器，每执行一步操作，都会记录日志，为出现故障后的恢复准备依据。</p>
<p>XA 也有致命的缺点，那就是性能不理想，特别是在交易下单链路，往往并发量很高，XA无法满足高并发场景。XA 目前在商业数据库支持的比较理想，在 mysql 数据库中支持的不太理想，mysql 的 XA 实现，没有记录 prepare 阶段日志，主备切换回导致主库与备库数据不一致。许多 nosql 也没有支持 XA，这让 XA 的应用场景变得非常狭隘。具有如下问题：</p>
<ol>
<li><strong>同步阻塞</strong>：从上面的描述可以看出，对于任何一次指令必须收到明确的响应，才会继续做下一步，否则处于阻塞状态，占用的资源被一直锁定，不会被释放。</li>
<li><strong>单点故障</strong>：如果事务管理器宕机，资源管理器没有了事务管理器指挥，会一直阻塞，尽管可以通过选举新的事务管理器替代原有协调者，但是如果之前事务管理器在发送一个提交指令后宕机，而提交指令仅仅被一个资源管理器接收，并且参与接收后资源管理器也宕机，新上任的事务管理器无法处理这种情况。</li>
<li><strong>脑裂</strong>：事务管理器发送提交指令，有的资源管理器接收后执行了事务，有的参与者没有接收到事务，就没有执行事务，多个参与者之间是不一致的。</li>
</ol>
<h3 id="基于XA协议的三阶段提交（3PC）"><a href="#基于XA协议的三阶段提交（3PC）" class="headerlink" title="基于XA协议的三阶段提交（3PC）"></a>基于XA协议的三阶段提交（3PC）</h3><p>三阶段提交，在两阶段提交的基础下，改进两阶段。三阶段步骤如下。</p>
<ol>
<li><p><code>CanCommit</code>，协调者询问参与者是否可以进行事务提交。</p>
</li>
<li><p><code>PreCommit</code>，若所有参与者可以进行事务提交，协调者下达 <code>PreCommit</code> 命令，参与者锁定资源，并等待最终命令。</p>
</li>
<li><ul>
<li>所有参与者返回确认信息，协调者向各个事务下发事务执行通知，锁定资源，并将执行情况返回。</li>
<li>部分参与者返回否认信息或<strong>协调者等待超时</strong>。这种情况，协调者认为事务无法正常执行，下发中断指令，各个参与者退出预备状态</li>
</ul>
</li>
<li><p><code>Do Commit</code>，若第二阶段全部回应 <code>ack</code>，则下达 <code>Do Commit</code> ，进行事务最终提交，否则下达中断事务命令，所有参与者进行事务回滚。</p>
</li>
<li><ul>
<li>所有参与者正常执行执行事务，协调者下发最终提交指令，释放锁定资源。</li>
<li>部分参与者执行事务失败，协调者等待超时，协调者下发回滚指令，释放锁定资源。</li>
</ul>
</li>
</ol>
<p><img src="https://qiniu.xiaoming.net.cn/%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.jpg" alt="三阶段提交"></p>
<p>三阶段提交对比两阶段，引入超时机制减少事务阻塞，解决单点故障。在第三阶段，<strong>一旦参与者无法接受到协调者信号时，等待超时之后，参与者默认执行 commit，释放资源</strong>。</p>
<blockquote>
<p>这里之所以这么设计，其实是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了 <code>PreCommit</code> 请求，那么协调者产生 <code>PreCommit</code> 请求的前提条件是他在第二阶段开始之前，收到所有参与者的 <code>CanCommit</code> 响应都是 Yes。（一旦参与者收到了 <code>PreCommit</code>，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到 <code>commit</code> 或者 <code>abort</code> 响应，但是他有理由相信：成功提交的几率很大。</p>
</blockquote>
<p>三阶段仍然不能解决数据一致性问题。若协调者发出回滚命令，但是由于网络问题，参与者在等待时间内都无法接收到，这时参与者默认提交事务，而其他事务进行了回滚，造成事务不一致。</p>
<h3 id="补偿事务（TCC）"><a href="#补偿事务（TCC）" class="headerlink" title="补偿事务（TCC）"></a>补偿事务（TCC）</h3><p>TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段：</p>
<ul>
<li><code>Try</code> 阶段主要是对业务系统做检测及资源预留，完成所有业务检查(一致性），预留必须业务资源(准隔离性)</li>
<li><code>Confirm</code> 阶段主要是对业务系统做确认提交,<code>Try</code> 阶段执行成功并开始执行 <code>Confirm</code> 阶段时，默认 <code>Confirm</code> 阶段是不会出错的。即：只要 <code>Try</code> 成功，<code>Confirm</code> 一定成功。</li>
<li><code>Cancel</code> 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。</li>
</ul>
<p>比如执行如下事务：</p>
<pre><code>A:-100(补偿为 A:+100)
B:+100</code></pre><p>那么如果 <code>B:+100</code> 失败后就需要执行 <code>A:+100</code>。</p>
<p><strong>优点</strong>：跟 2PC 比起来，实现以及流程相对简单了一些，但是数据的一致性比 2PC 要差一些。</p>
<p><strong>缺点</strong>：TCC 属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用 TCC 不太好定义及处理。</p>
<h4 id="引入-TCC-的例子"><a href="#引入-TCC-的例子" class="headerlink" title="引入 TCC 的例子"></a>引入 TCC 的例子</h4><p>下面模拟商城一次支付过程。用户下单使用组合支付，即余额加红包支付。一次正常流程为：</p>
<ol>
<li><p>创建订单</p>
</li>
<li><p>下单</p>
</li>
<li><ul>
<li>调用余额系统，扣减余额</li>
<li>调用红包系统，扣减红包余额</li>
<li>修改订单状态为已支付</li>
<li>完后支付。</li>
</ul>
</li>
</ol>
<p>实际过程如下图：</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B1.jpg" alt="TCC支付流程1"></p>
<p>但是这么一个支付过程调用多个子服务，并不能保证所有服务都能成功，比如在调用红包系统扣减红包系统失败。这个时候就碰到尴尬的场景，由于红包服务失败，导致方法异常退出，这个时候订单状态为初始状态，但是用户余额已经扣减。这对用户体验非常不友好。所以这次支付过程，必须存在机制将这次过程当成一次整体的行为，必须保证这其中服务调用，要么都成功，要么都失败，成为一个整体的事务。</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B2.jpg" alt="TCC支付流程2"></p>
<p>这时可以引入 TCC 事务，将整个下单过程作为一个整体。引入后，由于余额系统扣减是失败，这个时候回滚订单系统与红包系统。整个过程如下图:</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B3.jpg" alt="TCC支付流程3"></p>
<p>由于余额系统的失败，需要撤销这次过程中所有更改，所以向订单系统发送撤销通知，向红包系统发出撤销通知。</p>
<p>因此系统引入 TCC 事务后，需要改造我们的调用过程。</p>
<h4 id="系统如何引入-TCC-事务"><a href="#系统如何引入-TCC-事务" class="headerlink" title="系统如何引入 TCC 事务"></a>系统如何引入 TCC 事务</h4><p>根据 TCC 事务三步，这个时候必须将各个服务改造成 <code>Try</code>、<code>Confirm</code> 、<code>Cancle</code> 三步</p>
<p><strong>TCC TRY</strong>：</p>
<p>根据上面的业务，订单系统增加 <code>try</code> 方法将订单状态修改成 <strong>PAYING</strong>。余额系统增加一个 <code>try</code> 方法，先检查用于余额是否充足，然后先将余额扣减，然后将扣减的余额增加到冻结金额。红包系统同余额系统。从改造过程可以看出，TCC <code>try</code> 方法需检查各业务资源，且这过程需要引入中间状态。根据下图来看整个过程：</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B4.jpg" alt="TCC支付流程4"></p>
<p><strong>TCC Confirm</strong>:</p>
<p>TCC 第一步 TRY 如果所有子服务调用都成功，这个时候就需要确认各服务。各个服务增加 <code>confirm</code> 方法。如余额系统 <code>confirm</code> 方法用来将冻结金额置为0，红包系统如上。订单系统将订单状态修改为 <strong>SUCCESS</strong>。<code>confirm</code> 方法需要注意实现幂等。如订单系统更新前，一定要先判断该笔订单状态处于 <strong>PAYING</strong>，才能更新订单。整个过程如下图：</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B5.jpg" alt="TCC支付流程5"></p>
<p>讲到这里，必须用到 TCC 事务框架推动各服务。TCC 事务管理器感知到 <code>TRY</code> 方法结束后，自动调用各服务提供的 <code>confirm</code> 方法，将各服务状态修改为终态。</p>
<p><strong>TCC Cancle</strong>：</p>
<p>如若 TCC <code>Try</code> 过程中，冻结红包方法失败，这时就需要将之前修改都撤销，修改成其初始状态。<code>cancle</code> 方法也需要实现幂等如 <code>confirm</code> 方法，如下图：</p>
<p><img src="https://qiniu.xiaoming.net.cn/TCC%E6%94%AF%E4%BB%98%E6%B5%81%E7%A8%8B6.jpg" alt="TCC支付流程6"></p>
<p>看到这，可以看出 TCC Try 成功，confirm 必定要成功，try 失败，cancle 必定要成功。因为 confirm 是系统更新为终态的关键。但是实际上，生产系统 confirm 或 cancle 肯定会有几率失败，这个时候就需要 TCC 框架记录调用 confirm 结果。如果 confirm 调用失败，TCC 框架需要记录下来，然后间隔一定时间再次去调用。</p>
<h4 id="TCC-与阶段提交对比"><a href="#TCC-与阶段提交对比" class="headerlink" title="TCC 与阶段提交对比"></a>TCC 与阶段提交对比</h4><p>使用 2PC 或 3PC 实现的分布式框架，业务应用层无需改动，接入较简单。但是相对应能较低，数据资源锁定较长。不太适合互联网等高并发业务场景。</p>
<p>而使用基于 TCC 实现分布式框架，相对 2PC 性能较高，可以保证数据最终一致性。但是对于应用层来说，一个方法必须改造成三个方法，且业务中需引入一些中间状态，相对而言应用改造程度较大。</p>
<h3 id="本地消息表（异步确保）"><a href="#本地消息表（异步确保）" class="headerlink" title="本地消息表（异步确保）"></a>本地消息表（异步确保）</h3><p>本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。</p>
<p><img src="https://qiniu.xiaoming.net.cn/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8.jpg" alt="本地消息表"></p>
<p>基本思路是：</p>
<p>在消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。</p>
<p>在消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会执行重试。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。</p>
<p>生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。</p>
<p>这种方案遵循BASE理论，采用的是最终一致性，是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。</p>
<p><strong>优点</strong>： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET中 有现成的解决方案。</p>
<p><strong>缺点</strong>： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。</p>
<h3 id="MQ事务消息"><a href="#MQ事务消息" class="headerlink" title="MQ事务消息"></a>MQ事务消息</h3><p>所谓的消息事务就是基于消息中间件的两阶段提交，本质上是对消息中间件的一种特殊利用，它是将本地事务和发消息放在一个分布式事务里，保证要么本地操作成功并且对外发消息成功，要么两者都失败，开源的RocketMQ就支持这一特性，具体原理如下：</p>
<p><img src="https://qiniu.xiaoming.net.cn/MQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF" alt="MQ事务消息"></p>
<ol>
<li>A 系统向消息中间件发送一条预备消息</li>
<li>消息中间件保存预备消息并返回成功</li>
<li>A 执行本地事务</li>
<li>A 发送提交消息给消息中间件</li>
</ol>
<p>通过以上4步就完成了一个消息事务，对于以上4个步骤，每个步骤都可能发生错误：</p>
<ul>
<li>步骤一出错，则整个事务失败，不会执行A的本地操作</li>
<li>步骤二出错，则整个事务失败，不会执行A的本地操作</li>
<li>步骤三出错，这时候需要回滚预备消息，怎么回滚？答案是A系统实现一个消息中间件的回调接口，消息中间件会去不断执行回调接口，检查A事务执行是否执行成功，如果失败则回滚预备消息</li>
<li>步骤四出错，这时候A的本地事务是成功的，那么消息中间件要回滚A吗？答案是不需要，其实通过回调接口，消息中间件能够检查到A执行成功了，这时候其实不需要A发提交消息了，消息中间件可以自己对消息进行提交，从而完成整个消息事务。</li>
</ul>
<p>基于消息中间件的两阶段提交往往用在高并发场景下，将一个分布式事务拆成一个消息事务（A系统的本地操作+发消息）+B系统的本地操作，其中B系统的操作由消息驱动，只要消息事务成功，那么A操作就一定成功，消息也一定发出来了，这时候B会收到消息去执行本地操作，如果本地操作失败，消息就会重投，直到B操作成功，这样就变相地实现了A与B的分布式事务。原理如下：</p>
<p><img src="https://qiniu.xiaoming.net.cn/MQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF2" alt="MQ消息事务2"></p>
<p>虽然上面的方案能够完成A和B的操作，但是A和B并不是严格一致的，而是最终一致的，在这里牺牲了一致性，换来了性能的大幅度提升。当然，这种做法也是有风险的，如果B一直执行不成功，那么一致性会被破坏，具体要不要这样做，还是得看业务能够承担多少风险。</p>
<h3 id="Sagas事务模型"><a href="#Sagas事务模型" class="headerlink" title="Sagas事务模型"></a>Sagas事务模型</h3><p>Sagas事务模型又叫做长时间运行的事务（Long-running-transaction），它描述的是另外一种在没有两阶段提交的情况下解决分布式系统中复杂的业务事务问题。</p>
<p>该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由Sagas工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。</p>
<p>比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。</p>
<p><img src="http://qiniu.xiaoming.net.cn/Sagas%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%9E%8B.png" alt="Sagas事务模型"></p>
<p>他们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。</p>
<p>因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者也可以叫流程管理器（Process Manager），工作流引擎和Process Manager虽然不是同一个东西，但是在这里，他们的职责是相同的。</p>
<blockquote>
<p>Sagas事务模型理论是一个相对比较新的理论，目前市面上还没有什么解决方案，只是一个理论上的模型。</p>
</blockquote>
<h1 id="分布式一致性算法"><a href="#分布式一致性算法" class="headerlink" title="分布式一致性算法"></a>分布式一致性算法</h1><p>在分布式系统中，为了消除单点提高系统可用性，通常会使用副本来进行容错，但这会带来另一个问题，即如何保证多个副本之间的一致性？</p>
<p>分布式一致性 (distributed consensus) 是分布式系统中最基本的问题，用来保证一个分布式系统的可靠性以及容错能力。简单来说，<strong>分布式一致性是指多个服务器的保持状态一致</strong>。</p>
<p>在分布式系统中，可能出现各种意外（断电、网络拥塞、CPU/内存耗尽等等），使得服务器宕机或无法访问，最终导致无法和其他服务器保持状态一致。为了应对这种情况，就需要有一种一致性协议来进行容错，使得分布式系统中即使有部分服务器宕机或无法访问，整体依然可以对外提供服务。</p>
<p>所谓的强一致性（线性一致性）并不是指集群中所有节点在任一时刻的状态必须完全一致，而是指一个目标，即让一个分布式系统看起来只有一个数据副本，并且读写操作都是原子的，这样应用层就可以忽略系统底层多个数据副本间的同步问题。也就是说，我们可以将一个强一致性分布式系统当成一个整体，一旦某个客户端成功的执行了写操作，那么所有客户端都一定能读出刚刚写入的值。即使发生网络分区故障，或者少部分节点发生异常，整个集群依然能够像单机一样提供服务。</p>
<p>共识算法（Consensus Algorithm）就是用来做这个事情的，它保证即使在小部分（≤ (N-1)/2）节点故障的情况下，系统仍然能正常对外提供服务。</p>
<p>共识算法通常基于状态复制机（Replicated State Machine）模型，也就是所有节点从同一个 state 出发，经过同样的操作 log，最终达到一致的 state。</p>
<p><img src="https://qiniu.xiaoming.net.cn/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E2%80%94%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA.jpeg" alt="分布式算法—复制状态机"></p>
<p>复制状态机（Replicated State Machines） 是指一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。</p>
<p>复制状态机通常都是基于复制日志实现的，每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。</p>
<p>保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。</p>
<p><strong>实际系统中使用的一致性算法通常含有以下特性：</strong></p>
<ul>
<li><p><strong>安全性保证</strong>（绝对不会返回一个错误的结果）：在非拜占庭错误（消息篡改）情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证正确。</p>
</li>
<li><p><strong>可用性：</strong>集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。他们当有稳定的存储的时候可以从状态中恢复回来并重新加入集群。</p>
</li>
<li><p><strong>不依赖时序来保证一致性：</strong>物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。</p>
</li>
</ul>
<p>通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。</p>
<p>共识算法是构建强一致性分布式系统的基石，Paxos 是共识算法的代表，而 Raft 则是其作者在博士期间研究 Paxos 时提出的一个变种，主要优点是容易理解、易于实现，甚至关键的部分都在论文中给出了伪代码实现。</p>
<h1 id="Paxos算法"><a href="#Paxos算法" class="headerlink" title="Paxos算法"></a>Paxos算法</h1><p>Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。</p>
<p>在常见的分布式系统中，总会发生诸如机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos 算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。</p>
<p>注：这里某个数据的值并不只是狭义上的某个数，它可以是一条日志，也可以是一条命令（command）等。根据应用场景不同，某个数据的值有不同的含义。</p>
<p>在Paxos算法中，有三种角色：</p>
<ul>
<li><strong>Proposer</strong>（提案人）</li>
<li><strong>Acceptor</strong>（接收者）</li>
<li><strong>Learners</strong>（学习者）</li>
</ul>
<p>在具体的实现中，一个进程可能同时充当多种角色，比如一个进程可以既是 Proposer 又是 Acceptor 又是 Learner。</p>
<p>还有一个概念叫<strong>提案</strong>（Proposal）。最终要达成一致的 value 就在提案里。</p>
<p>假设有一组可以提出（propose）<code>value</code>（value 在提案 Proposal 里）的进程集合。一个一致性算法需要保证提出的这么多 <code>value</code> 中，只有一个 <code>value</code> 被选定（chosen）。如果没有 <code>value</code> 被提出，就不应该有 <code>value</code> 被选定。如果一个 <code>value</code> 被选定，那么所有进程都应该能学习（learn）到这个被选定的 <code>value</code>。对于一致性算法，安全性（safaty）要求如下：</p>
<ul>
<li>只有被提出的 <code>value</code> 才能被选定。</li>
<li>只有一个 <code>value</code> 被选定</li>
<li>如果某个进程认为某个 <code>value</code> 被选定了，那么这个 <code>value</code> 必须是真的被选定的那个。</li>
</ul>
<p>一致性算法的目标是保证最终有一个提出的 <code>value</code> 被选定。当一个 <code>value</code> 被选定后，进程最终也能学习到这个 <code>value</code>。</p>
<blockquote>
<p>Paxos的目标：保证最终有一个 <code>value</code> 会被选定，当 <code>value</code> 被选定后，进程最终也能获取到被选定的 <code>value</code>。</p>
</blockquote>
<p>假设不同角色之间可以通过发送消息来进行通信，那么：</p>
<ul>
<li>每个角色以任意的速度执行，可能因出错而停止，也可能会重启。一个 <code>value</code> 被选定后，所有的角色可能失败然后重启，除非那些失败后重启的角色能记录某些信息，否则等他们重启后无法确定被选定的值。</li>
<li>消息在传递过程中可能出现任意时长的延迟，可能会重复，也可能丢失。但是消息不会被损坏，即消息内容不会被篡改（拜占庭将军问题）。</li>
</ul>
<p>Paxos算法分为两个阶段：</p>
<ul>
<li><strong>阶段一</strong>：</li>
</ul>
<ol>
<li>Proposer 选择一个提案编号 N，然后向半数以上的 Acceptor 发送编号为 N 的 Prepare 请求。<blockquote>
<p>Proposer 生成提案之前，应该先去学习已经被选定或者可能被选定的 value，然后以该 value 作为自己提出的提案的 value。如果没有 value 被选定，Proposer 才可以自己决定 value 的值。这样才能达成一致。这个学习的阶段就是通过一个 <strong>Prepare 请求</strong>实现。</p>
</blockquote>
</li>
<li>如果 Acceptor 收到一个编号为 N 的 Prepare 请求，且 N 大于该 Acceptor 已经响应过的所有 Prepare 请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给 Proposer，同时该 Acceptor 承诺不再接受任何编号小于N的提案。</li>
</ol>
<ul>
<li><strong>阶段二</strong>：</li>
</ul>
<ol>
<li>如果 Proposer 收到半数以上 Acceptor 对其发出的编号为 N 的 Prepare 请求的响应，那么它就会发送一个针对 <code>[N,V]</code> 提案的 Accept 请求给半数以上的 Acceptor。如果响应中不包含任何提案，那么 V 就由 Proposer 自己决定<blockquote>
<p>V 就是收到的响应中编号最大的提案的 value。</p>
</blockquote>
</li>
<li>如果 Accepter 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor 没有对编号大于 N 的 Prepare 请求作出响应，它就接收该提案。</li>
</ol>
<p>Learner 学习（获取）被选定的value有以下三种方案：</p>
<p><img src="https://qiniu.xiaoming.net.cn/Learner%E5%AD%A6%E4%B9%A0%E9%80%89%E5%AE%9Avalue%E6%96%B9%E6%A1%88.webp" alt="Learner学习选定value方案"></p>
<p>如何保证 Paxos 算法的活性？</p>
<p>如果有两个 Proposer 依次提出编号递增的提案，最终会陷入死循环，没有 value 被选定（无法保证活性）</p>
<p><img src="https://qiniu.xiaoming.net.cn/Paxos%E7%AE%97%E6%B3%95%E6%B4%BB%E6%80%A7.webp" alt="Paxos算法活性"></p>
<p>结局方法是选取一个主 Proposer，只有主 Proposer 才能提出提案。</p>
<h1 id="RAFT"><a href="#RAFT" class="headerlink" title="RAFT"></a>RAFT</h1><p>Raft 是一种为了管理日志复制的分布式一致性算法。Raft 出现之前，Paxos 一直是分布式一致性算法的标准。Paxos 难以理解，更难以实现。Raft 的设计目标是简化 Paxos，使得算法既容易理解，也容易实现。</p>
<p>Raft 算法同样是一种分布式算法，是对 paxos 的一种简化和改进。相比于 Paxos 难以理解、实现和排错，RAFT 是一个通俗易懂，更容易落的分布式协议。</p>
<p>Paxos 和 Raft 都是分布式一致性算法，这个过程如同投票选举领袖（Leader），参选者（Candidate）需要说服大多数投票者（Follower）投票给他，一旦选举出领袖，就由领袖发号施令。Paxos 和 Raft 的区别在于选举的具体过程不同。</p>
<p><strong>Raft 可以解决分布式 CAP 理论中的 CP</strong>，即一致性（C：Consistency） 和分区容忍性（P：Partition Tolerance），并不能解决可用性（A：Availability） 的问题。</p>
<h2 id="Raft-的基本概念"><a href="#Raft-的基本概念" class="headerlink" title="Raft 的基本概念"></a>Raft 的基本概念</h2><p>Raft 使用 Quorum 机制来实现共识和容错，对 Raft 集群的操作称为提案，每当发起一个提案，必须得到大多数（&gt; N/2）节点的同意才能提交。</p>
<blockquote>
<p>这里的“提案”可以先狭义地理解为对集群的读写操作，“提交”理解为操作成功。</p>
</blockquote>
<p>那么当向 Raft 集群发起一系列读写操作时，集群内部究竟发生了什么呢？</p>
<p>首先，Raft 集群必须存在一个主节点（leader），客户端向集群发起的所有操作都必须经由主节点处理。所以 Raft 核心算法中的第一部分就是<strong>选主</strong>（<strong>Leader election</strong>）——没有主节点集群就无法工作，先票选出一个主节点，再考虑其它事情。</p>
<p>其次，主节点需要承载什么工作呢？它会负责接收客户端发过来的操作请求，将操作包装为<strong>日志</strong>同步给其它节点，在保证<strong>大部分</strong>节点都同步了本次操作后，就可以安全地给客户端回应响应了。这一部分工作在 Raft 核心算法中叫<strong>日志复制</strong>（<strong>Log replication</strong>）。</p>
<p>然后，因为主节点的责任是如此之大，所以节点们在选主的时候一定要谨慎，只有<strong>符合条件</strong>的节点才可以当选主节点。此外主节点在处理操作日志的时候也一定要谨慎，为了保证集群对外展现的一致性，不可以<strong>覆盖或删除</strong>前任主节点已经处理成功的操作日志。所谓的“谨慎处理”，其实就是在选主和提交日志的时候进行一些限制，这一部分在 Raft 核心算法中叫<strong>安全性</strong>（<strong>Safety</strong>）。</p>
<p><strong>Raft 核心算法其实就是由这三个子问题组成的：选主（Leader election）、日志复制（Log replication）、安全性（Safety）。这三部分共同实现了 Raft 核心的共识和容错机制。</strong></p>
<p>除了核心算法外，Raft 也提供了几个工程实践中必须面对问题的解决方案。</p>
<p>第一个是关于日志无限增长的问题。Raft 将操作包装成为了日志，集群每个节点都维护了一个不断增长的日志序列，状态机只有通过重放日志序列来得到。但由于这个日志序列可能会随着时间流逝不断增长，因此必须有一些办法来避免无休止的磁盘占用和过久的日志重放。这一部分叫<strong>日志压缩</strong>（<strong>Log compaction</strong>）。</p>
<p>第二个是关于集群成员变更的问题。一个 Raft 集群不太可能永远是固定几个节点，总有扩缩容的需求，或是节点宕机需要替换的时候。直接更换集群成员可能会导致严重的<strong>脑裂</strong>问题。Raft 给出了一种安全变更集群成员的方式。这一部分叫<strong>集群成员变更</strong>（<strong>Cluster membership change</strong>）。</p>
<p>此外，还会额外讨论<strong>线性一致性</strong>的定义、为什么 <strong>Raft 不能与线性一致划等号</strong>、如何<strong>基于 Raft 实现线性一致</strong>，以及在如何<strong>保证线性一致的前提下进行读性能优化</strong>。</p>
<h2 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h2><p>原生的 Paxos 算法使用了一种点对点（peer-to-peer）的方式，所有节点地位是平等的。在理想情况下，算法的目的是制定<strong>一个决策</strong>，这对于简化的模型比较有意义。但在工业界很少会有系统会使用这种方式，当有一系列的决策需要被制定的时候，先选出一个 leader 节点然后让它去协调所有的决策，这样算法会更加简单快速。</p>
<p>此外，和其它一致性算法相比，Raft 赋予了 leader 节点更强的领导力，称之为 <strong>Strong Leader</strong>。比如说日志条目只能从 leader 节点发送给其它节点而不能反着来，这种方式简化了日志复制的逻辑，使 Raft 变得更加简单易懂。</p>
<h3 id="节点角色"><a href="#节点角色" class="headerlink" title="节点角色"></a>节点角色</h3><p>Raft 集群中每个节点都处于以下三种角色之一：</p>
<ul>
<li><strong>Leader</strong>: 所有请求的处理者，接收客户端发起的操作请求，写入本地日志后同步至集群其它节点。</li>
<li><strong>Follower</strong>: 请求的被动更新者，从 leader 接收更新请求，写入本地文件。如果客户端的操作请求发送给了 follower，会首先由 follower 重定向给 leader。</li>
<li><strong>Candidate</strong>: 如果 follower 在一定时间内没有收到 leader 的心跳，则判断 leader 可能已经故障，此时启动 leader election 过程，本节点切换为 candidate 直到选主结束。</li>
</ul>
<h3 id="任期"><a href="#任期" class="headerlink" title="任期"></a>任期</h3><p>每开始一次新的选举，称为一个<strong>任期</strong>（<strong>term</strong>），每个 term 都有一个严格递增的整数与之关联。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E4%BB%BB%E6%9C%9F.png" alt="raft任期"></p>
<p>每当 candidate 触发 leader election 时都会增加 term，如果一个 candidate 赢得选举，他将在本 term 中担任 leader 的角色。但并不是每个 term 都一定对应一个 leader，有时候某个 term 内会由于选举超时导致选不出 leader，这时 candicate 会递增 term 号并开始新一轮选举。</p>
<p>不同服务器节点观察到的任期转换状态可能不一样：有的服务器节点可能观察到多次的任期转换，而有的服务器节点可能观察不到任何一次任期转换。</p>
<p>任期在 Raft 算法中充当逻辑时钟的作用，使得服务器节点可以查明一些过期的信息（比如过期的 Leader）。每个服务器节点都会存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号。</p>
<ul>
<li>如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。</li>
<li>如果一个 Candidate 或者 Leader 发现自己的任期号过期了，那么他会立即恢复成跟随者状态。</li>
<li>如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。</li>
</ul>
<h3 id="节点通信"><a href="#节点通信" class="headerlink" title="节点通信"></a>节点通信</h3><p>Raft 算法中服务器节点之间的通信使用 <strong>远程过程调用（RPC）</strong>。基本的一致性算法只需要两种 RPC：</p>
<ul>
<li>RequestVote RPC - 请求投票 RPC，由 Candidate 在选举期间发起。</li>
<li>AppendEntries RPC - 附加条目 RPC，由 Leader 发起，用来复制日志和提供一种心跳机制。</li>
</ul>
<h3 id="节点状态转换"><a href="#节点状态转换" class="headerlink" title="节点状态转换"></a>节点状态转换</h3><p>集群每个节点的状态都只能是 leader、follower 或 candidate，那么节点什么时候会处于哪种状态呢？下图展示了一个节点可能发生的状态转换：</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2.png" alt="raft状态转换"></p>
<p>下面详细讨论下每个转换所发生的场景。</p>
<h4 id="Follower-状态转换过程"><a href="#Follower-状态转换过程" class="headerlink" title="Follower 状态转换过程"></a>Follower 状态转换过程</h4><p>Raft 的选主基于一种心跳机制，集群中每个节点刚启动时都是 follower 身份（<strong>Step: starts up</strong>），leader 会周期性的向所有节点发送心跳包来维持自己的权威，那么首个 leader 是如何被选举出来的呢？方法是如果一个 follower 在一段时间内没有收到任何心跳，也就是选举超时，那么它就会主观认为系统中没有可用的 leader，并发起新的选举（<strong>Step: times out, starts election</strong>）。</p>
<p>若 follower 想发起一次选举，follower 需要先增加自己的当前 term，并将身份切换为 candidate。然后它会向集群其它节点发送“请给自己投票”的消息（RequestVote RPC）。</p>
<p>这里有一个问题，即这个“选举超时时间”该如何制定？如果所有节点在同一时刻启动，经过同样的超时时间后同时发起选举，整个集群会变得低效不堪，极端情况下甚至会一直选不出一个主节点（后面分析）。Raft 巧妙的使用了一个随机化的定时器，让每个节点的“超时时间”在一定范围内随机生成（一般为 150ms ~ 300ms），这样就大大的降低了多个节点同时发起选举的可能性。</p>
<h4 id="Candidate-状态转换过程"><a href="#Candidate-状态转换过程" class="headerlink" title="Candidate 状态转换过程"></a>Candidate 状态转换过程</h4><p>Follower 切换为 candidate 并向集群其他节点发送“请给自己投票”的消息后，接下来会有三种可能的结果，也即上面<strong>节点状态图中 candidate 状态向外伸出的三条线</strong>。</p>
<h5 id="选举成功（Step-receives-votes-from-majority-of-servers）"><a href="#选举成功（Step-receives-votes-from-majority-of-servers）" class="headerlink" title="选举成功（Step: receives votes from majority of servers）"></a>选举成功（Step: receives votes from majority of servers）</h5><p>当candicate从整个集群的<strong>大多数</strong>（N/2+1）节点获得了针对同一 term 的选票时，它就赢得了这次选举，立刻将自己的身份转变为 leader 并开始向其它节点发送心跳来维持自己的权威。</p>
<p>每个节点针对每个 term 只能投出一张票，并且按照先到先得的原则。这个规则确保只有一个 candidate 会成为 leader。</p>
<h5 id="选举失败（Step-discovers-current-leader-or-new-term）"><a href="#选举失败（Step-discovers-current-leader-or-new-term）" class="headerlink" title="选举失败（Step: discovers current leader or new term）"></a>选举失败（Step: discovers current leader or new term）</h5><p>Candidate 在等待投票回复的时候，可能会突然收到其它自称是 leader 的节点发送的心跳包（AppendEntries RPC），如果这个心跳包里携带的 term <strong>不小于</strong> candidate 当前的 term，那么 candidate 会承认这个 leader，并将身份切回 follower。</p>
<p>这说明其它节点已经成功赢得了选举，只需立刻跟随即可。但如果心跳包中的 term 比自己小，candidate 会拒绝这次请求并保持选举状态。</p>
<h5 id="选举超时（Step-times-out-new-election）"><a href="#选举超时（Step-times-out-new-election）" class="headerlink" title="选举超时（Step: times out, new election）"></a>选举超时（Step: times out, new election）</h5><p>第三种可能的结果是 candidate 既没有赢也没有输。如果有多个 follower 同时成为 candidate，选票是可能被瓜分的，如果没有任何一个 candidate 能得到大多数节点的支持，那么每一个 candidate 都会超时。</p>
<p>此时 candidate 需要增加自己的 term，然后发起新一轮选举。</p>
<p>如果这里不做一些特殊处理，选票可能会一直被瓜分，导致选不出 leader 来。这里的“特殊处理”指的就是前文所述的<strong>随机化选举超时时间</strong>。</p>
<p>Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，竞选超时时间是一个<strong>随机的时间</strong>，在一个固定的区间（例如 150-300 毫秒）随机选择，这样可以把选举都分散开。</p>
<p>以至于在大多数情况下，只有一个服务器会超时，然后它赢得选举，成为 Leader，并在其他服务器超时之前发送心跳包。</p>
<p>同样的机制也被用在选票瓜分的情况下：每一个 Candidate 在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。</p>
<h4 id="Leader-状态转换过程"><a href="#Leader-状态转换过程" class="headerlink" title="Leader 状态转换过程"></a>Leader 状态转换过程</h4><p>节点状态图中的最后一条线是：<strong>discovers server with higher term</strong>。</p>
<p>想象一个场景：当 leader 节点发生了宕机或网络断连，此时其它 follower 会收不到 leader 心跳，首个触发超时的节点会变为 candidate 并开始拉票（由于随机化各个 follower 超时时间不同），由于该 candidate 的 term 大于原 leader 的 term，因此所有 follower 都会投票给它，这名 candidate 会变为新的 leader。一段时间后原 leader 恢复了，收到了来自新 leader 的心跳包，发现心跳中的 term 大于自己的 term，此时该节点会立刻切换为 follower 并跟随的新 leader。</p>
<p>以上就是 Raft 的选主逻辑，但还有一些细节（譬如是否给该 candidate 投票还有一些其它条件）依赖算法的其它部分基础，会在后续“安全性”一章描述。</p>
<p>当票选出 leader 后，leader 也该承担起相应的责任了，这个责任是什么？就是下面介绍的“日志复制”。</p>
<h2 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h2><p>前面说过，共识算法通常基于<strong>状态复制机</strong>（<strong>Replicated State Machine</strong>）模型，所有节点从<strong>同一个 state</strong> 出发，经过一系列<strong>同样操作 log</strong> 的步骤，最终也必将达到<strong>一致的 state</strong>。也就是说，只要保证集群中所有节点的 log 一致，那么经过一系列应用（apply）后最终得到的状态机也就是一致的。</p>
<p>Raft 负责保证集群中所有节点 <strong>log 的一致性</strong>。</p>
<p>此外还提到过：Raft 赋予了 leader 节点更强的领导力（<strong>Strong Leader</strong>）。那么 Raft 保证 log 一致的方式就很容易理解了，即所有 log 都必须交给 leader 节点处理，并由 leader 节点复制给其它节点。</p>
<p>这个过程，就叫做<strong>日志复制</strong>（<strong>Log replication</strong>）。</p>
<h3 id="日志格式"><a href="#日志格式" class="headerlink" title="日志格式"></a>日志格式</h3><p>日志由含日志索引（log index）的日志条目（log entry）组成。每个日志条目包含它被创建时的 Term 号（下图中方框中的数字），和一个复制状态机需要执行的指令。如果一个日志条目被复制到半数以上的服务器上，就被认为可以提交（Commit）了。</p>
<p>日志模型如下图所示：（x ← 3 代表 x 赋值为 3）</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F.jpeg" alt="raft日志格式"></p>
<p>每条日志除了存储状态机的操作指令外，还会拥有一个<strong>唯一的整数索引值</strong>（<strong>log index</strong>）来表明它在日志集合中的位置。</p>
<p>此外，每条日志还会存储一个 <strong>term</strong> 号（日志条目方块最上方的数字，相同颜色 term 号相同），该 term 表示 leader 收到这条指令时的当前任期，term 相同的 log 是由同一个 leader 在其任期内发送的。</p>
<p>当一条日志被 leader 节点认为可以安全的 apply 到状态机时，称这条日志是 <strong>committed</strong>（上图中的 <strong>committed entries</strong>）。</p>
<p>那么什么样的日志可以被 commit 呢？答案是：<strong>当 leader 得知这条日志被集群过半的节点复制成功时</strong>。因此在上图中可以看到 (term3, index7) 这条日志以及之前的日志都是 committed，尽管有两个节点拥有的日志并不完整。</p>
<p>Raft 保证所有 committed 日志都已经被<strong>持久化</strong>，且“<strong>最终</strong>”一定会被状态机apply。</p>
<blockquote>
<p>注：这里的“最终”用词很微妙，它表明了一个特点：Raft 保证的只是集群内日志的一致性，而真正期望的集群对外的状态机一致性还需要做一些额外工作，这一点在后面线性一致性与读性能优化说。</p>
</blockquote>
<h3 id="日志复制流程"><a href="#日志复制流程" class="headerlink" title="日志复制流程"></a>日志复制流程</h3><p>S1 当选 leader，此时还没有任何日志。此时模拟客户端向 S1 发起一个请求。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-1.png" alt="raft日志复制流程-1"></p>
<p>S1 收到客户端请求后新增了一条日志 (term2, index1)，然后并行地向其它节点发起 AppendEntries RPC。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-2.png" alt="raft日志复制流程-2"></p>
<p>S2、S4 率先收到了请求，各自附加了该日志，并向 S1 回应响应。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-3.png" alt="raft日志复制流程-3"></p>
<p>所有节点都附加了该日志，但由于 leader 尚未收到任何响应，因此暂时还不清楚该日志到底是否被成功复制。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-4.png" alt="raft日志复制流程-4"></p>
<p>当 S1 收到<strong>2个节点</strong>的响应时，该日志条目的边框就已经变为实线，表示该日志已经<strong>安全的复制</strong>，因为在5节点集群中，2个 follower 节点加上 leader 节点自身，副本数已经确保过半，此时 <strong>S1 将响应客户端的请求</strong>。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-5.png" alt="raft日志复制流程-5"></p>
<p>leader 后续会持续发送心跳包给 followers，心跳包中会携带当前<strong>已经安全复制（我们称之为 committed）的日志索引</strong>，此处为 (term2, index1)。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-6.png" alt="raft日志复制流程-6"></p>
<p>所有 follower 都通过心跳包得知 (term2, index1) 的 log 已经成功复制 （committed），因此所有节点中该日志条目的边框均变为实线。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E6%B5%81%E7%A8%8B-7.png" alt="raft日志复制流程-7"></p>
<h3 id="日志一致性保障"><a href="#日志一致性保障" class="headerlink" title="日志一致性保障"></a>日志一致性保障</h3><p>前边使用了 (term2, index1) 这种方式来表示一条日志条目，这里为什么要带上 term，而不仅仅是使用 index？原因是 term 可以用来检查不同节点间日志是否存在不一致的情况。</p>
<p>Raft 日志同步保证如下两点：</p>
<ol>
<li><p>如果不同日志中的两个日志条目有着相同的日志索引和 Term，则<strong>它们所存储的命令是相同的</strong>。</p>
<p> 这个特性基于这条原则：Raft 要求 leader 在一个 term 内针对同一个 index 只能创建一条日志，并且永远不会修改它。</p>
</li>
<li><p>如果不同日志中的两个日志条目有着相同的日志索引和 Term，则<strong>它们之前的所有条目都是完全一样的</strong>。</p>
<p> 这个特性由 AppendEntries RPC 的一个简单的一致性检查所保证。在发送 AppendEntries RPC 时，Leader 会把新日志条目之前的日志条目的日志索引和 Term 号一起发送。如果 Follower 在它的日志中找不到包含相同日志索引和 Term 号的日志条目，它就会拒绝接收新的日志条目。</p>
</li>
</ol>
<p>所以，只要 follower 持续正常地接收来自 leader 的日志，那么就可以通过归纳法验证上述结论。</p>
<h3 id="可能出现的日志不一致场景"><a href="#可能出现的日志不一致场景" class="headerlink" title="可能出现的日志不一致场景"></a>可能出现的日志不一致场景</h3><p>一般情况下，Leader 和 Followers 的日志保持一致，因此日志条目一致性检查通常不会失败。然而，Leader 崩溃可能会导致日志不一致：旧的 Leader 可能没有完全复制完日志中的所有条目。</p>
<p>Leader 和 Follower 可能存在多种日志不一致的可能：</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft-leader%E5%92%8Cfollower%E6%97%A5%E5%BF%97%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E6%83%85%E5%86%B5.jpeg" alt="raft-leader和follower日志不一致的情况"></p>
<p>上图展示了一个 term8 的 leader 刚上任时，集群中日志可能存在的混乱情况。例如 follower 可能缺少一些日志（a ~ b），可能多了一些未提交的日志（c ~ d），也可能既缺少日志又多了一些未提交日志（e ~ f）。</p>
<blockquote>
<p>Follower 不可能比 leader 多出一些已提交（committed）日志，这一点是通过选举上的限制来达成的。</p>
</blockquote>
<p>先来尝试复现上述 a ~ f 场景，最后再讲 Raft 如何解决这种不一致问题。</p>
<h4 id="场景a-b-Follower-日志落后于-leader"><a href="#场景a-b-Follower-日志落后于-leader" class="headerlink" title="场景a~b: Follower 日志落后于 leader"></a>场景a~b: Follower 日志落后于 leader</h4><p>这种场景其实很简单，即 <strong>follower 宕机了一段时间</strong>，follower-a 从收到 (term6, index9) 后开始宕机，follower-b 从收到 (term4, index4) 后开始宕机。</p>
<h4 id="场景c-Follower-日志比-leader-多-term6"><a href="#场景c-Follower-日志比-leader-多-term6" class="headerlink" title="场景c. Follower 日志比 leader 多 term6"></a>场景c. Follower 日志比 leader 多 term6</h4><p>当 term6 的 leader 正在将 (term6, index11) 向 follower 同步时，该 leader 发生了宕机，且此时只有 follower-c 收到了这条日志的 AppendEntries RPC。然后经过一系列的选举，term7 可能是选举超时，也可能是 leader 刚上任就宕机了，最终 term8 的 leader 上任了，于是就看到了场景 c。</p>
<h4 id="场景d-Follower-日志比-leader-多-term7"><a href="#场景d-Follower-日志比-leader-多-term7" class="headerlink" title="场景d. Follower 日志比 leader 多 term7"></a>场景d. Follower 日志比 leader 多 term7</h4><p>当 term6 的 leader 将 (term6, index10) 成功 commit 后，发生了宕机。此时 term7 的 leader 走马上任，连续同步了两条日志给 follower，然而还没来得及 commit 就宕机了，随后集群选出了 term8 的 leader。</p>
<h4 id="场景e-Follower-日志比-leader-少-term5-6，多-term4"><a href="#场景e-Follower-日志比-leader-少-term5-6，多-term4" class="headerlink" title="场景e. Follower 日志比 leader 少 term5 ~ 6，多 term4"></a>场景e. Follower 日志比 leader 少 term5 ~ 6，多 term4</h4><p>当 term4 的 leader 将 (term4, index7) 同步给 follower，且将 (term4, index5) 及之前的日志成功 commit 后，发生了宕机，紧接着 follower-e 也发生了宕机。这样在 term5~7 内发生的日志同步全都被 follower-e 错过了。当 follower-e 恢复后，term8 的 leader 也刚好上任了。</p>
<h4 id="场景f-Follower-日志比-leader-少-term4-6，多-term2-3"><a href="#场景f-Follower-日志比-leader-少-term4-6，多-term2-3" class="headerlink" title="场景f. Follower 日志比 leader 少 term4 ~ 6，多 term2 ~ 3"></a>场景f. Follower 日志比 leader 少 term4 ~ 6，多 term2 ~ 3</h4><p>当 term2 的 leader 同步了一些日志（index4 ~ 6）给 follower 后，尚未来得及 commit 时发生了宕机，但它很快恢复过来了，又被选为了 term3 的 leader，它继续同步了一些日志（index7~11）给 follower，但同样未来得及 commit 就又发生了宕机，紧接着 follower-f 也发生了宕机，当 follower-f 醒来时，集群已经前进到 term8 了。</p>
<h3 id="如何处理日志不一致的场景"><a href="#如何处理日志不一致的场景" class="headerlink" title="如何处理日志不一致的场景"></a>如何处理日志不一致的场景</h3><p>通过上述场景可以看到，真实世界的集群情况很复杂，那么 Raft 是如何应对这么多不一致场景的呢？其实方式很简单暴力，想想 <strong>Strong Leader</strong> 这个词。</p>
<p><strong>Raft 强制要求 follower 必须复制 leader 的日志集合来解决不一致问题。</strong></p>
<p>也就是说，follower 节点上任何与 leader 不一致的日志，都会被 leader 节点上的日志所覆盖。这并不会产生什么问题，因为某些选举上的限制，如果 follower 上的日志与 leader 不一致，那么该日志在 follower 上<strong>一定是未提交的</strong>。未提交的日志并不会应用到状态机，也不会被外部的客户端感知到。</p>
<p>要使得 follower 的日志集合跟自己保持完全一致，leader 必须先找到二者间<strong>最后一次</strong>达成一致的地方。因为一旦这条日志达成一致，在这之前的日志一定也都一致。这个确认操作是在 AppendEntries RPC 的一致性检查步骤完成的。</p>
<p>Leader 针对每个 follower 都维护一个 <strong>next index</strong>，表示下一条需要发送给该follower 的日志索引。当一个 leader 刚刚上任时，它初始化所有 next index 值为自己最后一条日志的 index+1。但凡某个 follower 的日志跟 leader 不一致，那么下次 AppendEntries RPC 的一致性检查就会失败。在被 follower 拒绝这次 Append Entries RPC 后，leader 会减少 next index 的值并进行重试。</p>
<p>最终一定会存在一个 next index 使得 leader 和 follower 在这之前的日志都保持一致。极端情况下 next index 为1，表示 follower 没有任何日志与 leader 一致，leader 必须从第一条日志开始同步。</p>
<p>针对每个 follower，一旦确定了 next index 的值，leader 便开始从该 index 同步日志，follower 会删除掉现存的不一致的日志，保留 leader 最新同步过来的。</p>
<p>整个集群的日志会在这个简单的机制下自动趋于一致。此外要注意，<strong>leader 从来不会覆盖或者删除自己的日志</strong>，而是强制 follower 与它保持一致。</p>
<p>这就要求集群票选出的 leader 一定要具备“日志的正确性”，这也就关联到了前文提到的：选举上的限制。</p>
<h2 id="安全性及正确性"><a href="#安全性及正确性" class="headerlink" title="安全性及正确性"></a>安全性及正确性</h2><p>前面的章节描述了 Raft 算法是如何选主和复制日志的，然而到目前为止描述的<strong>这套机制还不能保证每个节点的状态机会严格按照相同的顺序 apply 日志</strong>。想象以下场景：</p>
<ol>
<li>Leader 将一些日志复制到了大多数节点上，进行 commit 后发生了宕机。</li>
<li>某个 follower 并没有被复制到这些日志，但它参与选举并当选了下一任 leader。</li>
<li>新的 leader 又同步并 commit 了一些日志，这些日志覆盖掉了其它节点上的上一任 committed 日志。</li>
<li>各个节点的状态机可能 apply 了不同的日志序列，出现了不一致的情况。</li>
</ol>
<p>因此需要对“选主+日志复制”这套机制加上一些额外的限制，来保证<strong>状态机的安全性</strong>，也就是 Raft 算法的正确性。</p>
<p>Raft 增加了一些限制来完善 Raft 算法，以保证安全性：保证了任意 Leader 对于给定的 Term，都拥有了之前 Term 的所有被提交的日志条目。</p>
<h3 id="对选举的限制"><a href="#对选举的限制" class="headerlink" title="对选举的限制"></a>对选举的限制</h3><p><strong>拥有最新的已提交的日志条目的 Follower 才有资格成为 Leader。</strong></p>
<p>Raft 使用投票的方式来阻止一个 Candidate 赢得选举，除非这个 Candidate 包含了所有已经提交的日志条目。Candidate 为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果 Candidate 的日志至少和大多数的服务器节点一样新，那么他一定持有了所有已经提交的日志条目。</p>
<p><strong>RequestVote RPC</strong> 实现了这样的限制：<strong>RequestVote RPC 中包含了 Candidate 的日志信息， Follower 会拒绝掉那些日志没有自己新的投票请求</strong>。</p>
<p>再来分析下前文所述的 committed 日志被覆盖的场景，根本问题其实发生在第2步。Candidate 必须有足够的资格才能当选集群 leader，否则它就会给集群带来不可预料的错误。</p>
<p>Candidate 想要赢得选举成为 leader，必须得到集群大多数节点的投票，那么<strong>它的日志就一定至少不落后于大多数节点</strong>。又因为一条日志只有复制到了大多数节点才能被 commit，因此<strong>能赢得选举的 candidate 一定拥有所有 committed 日志</strong>。</p>
<p>因此才会断定地说：Follower 不可能比 leader 多出一些 committed 日志。</p>
<h4 id="如何判断哪个日志条目比较新？"><a href="#如何判断哪个日志条目比较新？" class="headerlink" title="如何判断哪个日志条目比较新？"></a>如何判断哪个日志条目比较新？</h4><p>Raft 通过比较两份日志中最后一条日志条目的日志索引和 Term 来判断哪个日志比较新。</p>
<ol>
<li>先判断 Term，哪个数值大即代表哪个日志比较新。</li>
<li>如果 Term 相同，再比较 日志索引，哪个数值大即代表哪个日志比较新。</li>
</ol>
<h3 id="对提交的限制"><a href="#对提交的限制" class="headerlink" title="对提交的限制"></a>对提交的限制</h3><p><strong>Leader 只允许 commit 包含当前 term 的日志</strong>。</p>
<p>除了对选举增加一点限制外，还需对 commit 行为增加一点限制，来完成 Raft 算法核心部分的最后一块拼图。</p>
<p>回忆下什么是 commit：</p>
<p>当 leader 得知某条日志被集群过半的节点复制成功时，就可以进行 commit，committed 日志一定最终会被状态机 apply。</p>
<p>所谓 commit 其实就是对日志简单进行一个标记，表明其可以被 apply 到状态机，并针对相应的客户端请求进行响应。</p>
<p>然而 leader 并不能在任何时候都随意 commit 旧任期留下的日志，即使它已经被复制到了大多数节点。</p>
<p>Raft 论文给出了一个经典场景：</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%20%E6%8F%90%E4%BA%A4%E9%99%90%E5%88%B6.jpeg" alt="raft 提交限制"></p>
<p>上图从左到右按时间顺序模拟了问题场景。</p>
<p><strong>阶段a</strong>：S1 是 leader，收到请求后将 (term2, index2) 只复制给了 S2，尚未复制给 S3 ~ S5。</p>
<p><strong>阶段b</strong>：S1 宕机，S5 当选 term3 的 leader（S3、S4、S5 三票），收到请求后保存了 (term3, index2)，尚未复制给任何节点。</p>
<p><strong>阶段c</strong>：S5 宕机，S1 恢复，S1 重新当选 term4 的 leader，继续将 (term2, index2) 复制给了 S3，已经满足大多数节点，我们将其 commit。</p>
<p><strong>阶段d</strong>：S1 又宕机，S5 恢复，S5 重新当选 leader（S2、S3、S4 三票），将 (term3, inde2) 复制给了所有节点并 commit。注意，此时发生了致命错误，已经 committed 的 (term2, index2) 被 (term3, index2) 覆盖了。</p>
<p>为了避免这种错误，需要添加一个额外的限制：<strong>Leader 只允许 commit 包含当前 term 的日志</strong>。一旦当前 Term 的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。</p>
<p>针对上述场景，问题发生在阶段c，即使作为 term4 leader 的 S1 将 (term2, index2) 复制给了大多数节点，它也不能直接将其 commit，而是<strong>必须等待 term4 的日志到来并成功复制后，一并进行 commit</strong>。</p>
<p><strong>阶段e</strong>：在添加了这个限制后，要么 (term2, index2) 始终没有被 commit，这样 S5 在阶段d将其覆盖就是安全的；要么 (term2, index2) 同 (term4, index3) 一起被 commit，这样 S5 根本就无法当选 leader，因为大多数节点的日志都比它新，也就不存在前边的问题了。</p>
<p>以上便是对算法增加的两个小限制，它们对确保状态机的安全性起到了至关重要的作用。</p>
<h2 id="集群成员变更与日志压缩"><a href="#集群成员变更与日志压缩" class="headerlink" title="集群成员变更与日志压缩"></a>集群成员变更与日志压缩</h2><p>尽管已经通过的内容了解了 Raft 算法的核心部分，但相较于算法理论来说，在工程实践中仍有一些现实问题需要去面对。Raft 非常贴心的在论文中给出了两个常见问题的解决方案，它们分别是：</p>
<ul>
<li><strong>集群成员变更</strong>：如何安全地改变集群的节点成员。</li>
<li><strong>日志压缩</strong>：如何解决日志集合无限制增长带来的问题。</li>
</ul>
<h3 id="集群成员变更"><a href="#集群成员变更" class="headerlink" title="集群成员变更"></a>集群成员变更</h3><p>前文的理论描述中都假设了集群成员是不变的，然而在实践中有时会需要替换宕机机器或者改变复制级别（即增减节点）。一种最简单暴力达成目的的方式就是：停止集群、改变成员、启动集群。这种方式在执行时会导致集群整体不可用，此外还存在手工操作带来的风险。</p>
<p>为了避免这样的问题，Raft 论文中给出了一种无需停机的、自动化的改变集群成员的方式，其实本质上还是利用了 Raft 的核心算法，将集群成员配置作为一个特殊日志从 leader 节点同步到其它节点去。</p>
<h4 id="直接切换集群成员配置"><a href="#直接切换集群成员配置" class="headerlink" title="直接切换集群成员配置"></a>直接切换集群成员配置</h4><p>先说结论：<strong>所有将集群从旧配置直接完全切换到新配置的方案都是不安全的</strong>。</p>
<p>因此不能想当然的将新配置直接作为日志同步给集群并 apply。因为不可能让集群中的全部节点在“<strong>同一时刻</strong>”<strong>原子地</strong>切换其集群成员配置，所以在切换期间不同的节点看到的集群视图可能存在不同，最终可能导致集群存在多个 leader。</p>
<p>为了理解上述结论，来看一个实际出现问题的场景，下图对其进行了展现。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-1.png" alt="raft集群成员变更-1"></p>
<p>阶段a：集群存在 S1 ~ S3 三个节点，将该成员配置表示为 C-old，绿色表示该节点当前视图（成员配置）为 C-old，其中红边的 S3 为 leader。</p>
<p>阶段b：集群新增了 S4、S5 两个节点，该变更从 leader 写入，将 S1 ~ S5 的五节点新成员配置表示为 C-new，蓝色表示该节点当前视图为 C-new。</p>
<p>阶段c：假设 S3 短暂宕机触发了 S1 与 S5 的超时选主。</p>
<p>阶段d：S1 向 S2、S3 拉票，S5 向其它全部四个节点拉票。由于 S2 的日志并没有比 S1 更新，因此 S2 可能会将选票投给 S1，S1 两票当选（因为 S1 认为集群只有三个节点）。而 S5 肯定会得到 S3、S4 的选票，因为 S1 感知不到 S4，没有向它发送 RequestVote RPC，并且 S1 的日志落后于 S3，S3 也一定不会投给 S1，结果 S5 三票当选。最终集群出现了多个主节点的致命错误，也就是所谓的脑裂。</p>
<p>上图来自论文，用不同的形式展现了和图5-1相同的问题。颜色代表的含义与图5-1是一致的，在 <strong>problem: two disjoint majorities</strong> 所指的时间点，集群可能会出现两个 leader。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-2.png" alt="raft集群成员变更-2"></p>
<p>但是，多主问题并不是在任何新老节点同时选举时都一定可能出现的，有一些文章在举多主的例子时可能存在错误，下面是一个案例：</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-3.jpeg" alt="raft集群成员变更-3"></p>
<p>该假想场景类似前面说的阶段d，模拟过程如下：</p>
<ol>
<li>S1 为集群原 leader，集群新增 S4、S5，该配置被推给了 S3，S2 尚未收到。</li>
<li>此时 S1 发生短暂宕机，S2、S3 分别触发选主。</li>
<li>最终 S2 获得了 S1 和自己的选票，S3 获得了 S4、S5 和自己的选票，集群出现两个 leader。</li>
</ol>
<p>上面的过程看起来好像和前面没有什么大的不同，只是参与选主的节点存在区别，然而事实是<strong>上面的情况是不可能出现的</strong>。</p>
<p>注意：Raft 论文中传递集群变更信息也是通过日志追加实现的，所以也受到选主的限制。</p>
<p>很多人对选主限制中比较的日志是否必须是 committed 产生疑惑：</p>
<p>每个 candidate 必须在 RequestVote RPC 中携带自己本地日志的最新 (term, index)，如果 follower 发现这个 candidate 的日志还没有自己的新，则拒绝投票给该 candidate。</p>
<p>这里明确下，论文里确实间接表明了，<strong>选主时比较的日志是不要求 committed 的，只需比较本地的最新日志就行</strong>！</p>
<p>回到上面的场景，不可能出现的原因在于，S1 作为原 leader 已经第一个保存了新配置的日志，而 S2 尚未被同步这条日志，根据选主限制<strong>，</strong>S1 不可能将选票投给 S2，因此 S2 不可能成为 leader。</p>
<h4 id="两阶段切换集群成员配置"><a href="#两阶段切换集群成员配置" class="headerlink" title="两阶段切换集群成员配置"></a>两阶段切换集群成员配置</h4><p>Raft 使用一种两阶段方法平滑切换集群成员配置来避免遇到前一节描述的问题，具体流程如下：</p>
<h5 id="阶段一"><a href="#阶段一" class="headerlink" title="阶段一"></a>阶段一</h5><ol>
<li>客户端将 C-new 发送给 leader，leader 将 C-old 与 C-new 取<strong>并集</strong>并立即 apply，表示为 <strong>C-old,new</strong>。</li>
<li>Leader 将 C-old,new 包装为日志同步给其它节点。</li>
<li>Follower 收到 C-old,new 后立即 apply，当 <strong>C-old,new 的大多数节点（即 C-old 的大多数节点和 C-new 的大多数节点）</strong>都切换后，leader 将该日志 commit。</li>
</ol>
<h5 id="阶段二"><a href="#阶段二" class="headerlink" title="阶段二"></a>阶段二</h5><ol>
<li>Leader 接着将 C-new 包装为日志同步给其它节点。</li>
<li>Follower 收到 C-new 后立即 apply，如果此时发现自己不在 C-new 列表，则主动退出集群。</li>
<li>Leader 确认 <strong>C-new 的大多数节点</strong>都切换成功后，给客户端发送执行成功的响应。</li>
</ol>
<h4 id="为什么两阶段切换集群成员配置可以保证不会出现多个-leader？"><a href="#为什么两阶段切换集群成员配置可以保证不会出现多个-leader？" class="headerlink" title="为什么两阶段切换集群成员配置可以保证不会出现多个 leader？"></a>为什么两阶段切换集群成员配置可以保证不会出现多个 leader？</h4><p>来按流程逐阶段分析。</p>
<h5 id="阶段1-C-old-new-尚未-commit"><a href="#阶段1-C-old-new-尚未-commit" class="headerlink" title="阶段1. C-old,new 尚未 commit"></a>阶段1. C-old,new 尚未 commit</h5><p>该阶段所有节点的配置要么是 C-old，要么是 C-old,new，但无论是二者哪种，只要原 leader 发生宕机，新 leader 都<strong>必须得到大多数 C-old 集合内节点的投票</strong>。</p>
<p>以上面说到的场景为例，S5 在阶段d根本没有机会成为 leader，因为 C-old 中只有 S3 给它投票了，不满足大多数。</p>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4-1.png" alt="raft集群成员变更-1"></p>
<h5 id="阶段2-C-old-new-已经-commit，C-new-尚未下发"><a href="#阶段2-C-old-new-已经-commit，C-new-尚未下发" class="headerlink" title="阶段2. C-old,new 已经 commit，C-new 尚未下发"></a>阶段2. C-old,new 已经 commit，C-new 尚未下发</h5><p>该阶段 C-old,new 已经 commit，可以确保已经被 C-old,new 的大多数节点（<strong>再次强调：C-old 的大多数节点和 C-new 的大多数节点</strong>）复制。</p>
<p>因此当 leader 宕机时，新选出的 leader 一定是已经拥有 C-old,new 的节点，不可能出现两个 leader。</p>
<h5 id="阶段3-C-new-已经下发但尚未-commit"><a href="#阶段3-C-new-已经下发但尚未-commit" class="headerlink" title="阶段3. C-new 已经下发但尚未 commit"></a>阶段3. C-new 已经下发但尚未 commit</h5><p>该阶段集群中可能有三种节点 C-old、C-old,new、C-new，但由于已经经历了阶段2，因此 C-old 节点不可能再成为 leader。而无论是 C-old,new 还是 C-new 节点发起选举，都需要经过大多数 C-new 节点的同意，因此也不可能出现两个 leader。</p>
<h5 id="阶段4-C-new-已经-commit"><a href="#阶段4-C-new-已经-commit" class="headerlink" title="阶段4. C-new 已经 commit"></a>阶段4. C-new 已经 commit</h5><p>该阶段 C-new 已经被 commit，因此只有 C-new 节点可以得到大多数选票成为 leader。此时集群已经安全地完成了这轮变更，可以继续开启下一轮变更了。</p>
<p>以上便是对该两阶段方法可行性的分步验证，Raft 论文将该方法称之为<strong>共同一致（Joint Consensus）</strong>。</p>
<h3 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h3><p>Raft 核心算法维护了日志的一致性，通过 apply 日志也就得到了一致的状态机，客户端的操作命令会被包装成日志交给 Raft 处理。</p>
<p>然而在实际系统中，客户端操作是连绵不断的，但日志却不能无限增长，首先它会占用很高的存储空间，其次每次系统重启时都需要完整回放一遍所有日志才能得到最新的状态机。</p>
<p>因此 Raft 提供了一种机制去清除日志里积累的陈旧信息，叫做<strong>日志压缩</strong>。</p>
<p><strong>快照（Snapshot）</strong>是一种常用的、简单的日志压缩方式，ZooKeeper、Chubby 等系统都在用。简单来说，就是将某一时刻系统的状态 dump 下来并落地存储，这样该时刻之前的所有日志就都可以丢弃了。</p>
<blockquote>
<p>这里对“压缩”一词不要产生错误理解，并没有办法将状态机快照“解压缩”回日志序列。</p>
</blockquote>
<p><img src="https://qiniu.xiaoming.net.cn/raft%E6%97%A5%E5%BF%97%E5%BF%AB%E7%85%A7.png" alt="raft日志快照"></p>
<p>上图展示了一个节点用快照替换了 (term1, index1) ~ (term3, index5) 的日志。</p>
<p>快照一般包含以下内容：</p>
<ul>
<li><strong>日志的元数据</strong>：最后一条被该快照 apply 的日志 term 及 index</li>
<li><strong>状态机</strong>：前边全部日志 apply 后最终得到的状态机</li>
</ul>
<p>当 leader 需要给某个 follower 同步一些旧日志，但这些日志已经被 leader 做了快照并删除掉了时，leader 就需要把该快照发送给 follower。</p>
<p>同样，当集群中有新节点加入，或者某个节点宕机太久落后了太多日志时，leader 也可以直接发送快照，大量节约日志传输和回放时间。</p>
<p>同步快照使用一个新的 RPC 方法，叫做 <strong>InstallSnapshot RPC</strong>。</p>
<p>注意，<strong>在 Raft 中只能为 committed 日志做 snapshot</strong>，因为只有 committed 日志才是确保最终会应用到状态机的。</p>
<p>另外，<strong>生成快照的频率要适中</strong>，频率过高会消耗大量 I/O 带宽；频率过低，一旦需要执行恢复操作，会丢失大量数据，影响可用性。</p>
<p>推荐当日志达到某个固定的大小时生成快照。生成一次快照可能耗时过长，影响正常日志同步。可以通过使用 copy-on-write 技术避免快照过程影响正常日志同步。</p>
<h2 id="线性一致性与读性能优化"><a href="#线性一致性与读性能优化" class="headerlink" title="线性一致性与读性能优化"></a>线性一致性与读性能优化</h2><h3 id="什么是线性一致性？"><a href="#什么是线性一致性？" class="headerlink" title="什么是线性一致性？"></a>什么是线性一致性？</h3><p>在分布式系统中，为了消除单点提高系统可用性，通常会使用副本来进行容错，但这会带来另一个问题，即如何保证多个副本之间的<strong>一致性</strong>。</p>
<p>什么是一致性？所谓一致性有很多种模型，不同的模型都是用来评判一个并发系统正确与否的不同程度的标准。而现在要讨论的是<strong>强一致性（Strong Consistency）</strong>模型，也就是<strong>线性一致性（Linearizability）</strong>，经常听到的 CAP 理论中的 C 指的就是它。</p>
<p>前面已经简要描述过何为线性一致性：</p>
<p>所谓的强一致性（线性一致性）并不是指集群中所有节点在任一时刻的状态必须完全一致，而是指一个目标，即让一个分布式系统看起来只有一个数据副本，并且读写操作都是原子的，这样应用层就可以忽略系统底层多个数据副本间的同步问题。也就是说，可以将一个强一致性分布式系统当成一个整体，一旦某个客户端成功的执行了写操作，那么所有客户端都一定能读出刚刚写入的值。即使发生网络分区故障，或者少部分节点发生异常，整个集群依然能够像单机一样提供服务。</p>
<p>“<strong>像单机一样提供服务</strong>”从感官上描述了一个线性一致性系统应该具备的特性，那么该如何判断一个系统是否具备线性一致性呢？通俗来说就是不能读到旧（stale）数据，但具体分为两种情况：</p>
<ul>
<li>对于调用时间存在重叠（<strong>并发</strong>）的请求，生效顺序可以任意确定。</li>
<li>对于调用时间存在先后关系（<strong>偏序</strong>）的请求，后一个请求不能违背前一个请求确定的结果。</li>
</ul>
<p>只要根据上述两条规则即可判断一个系统是否具备线性一致性。</p>
<blockquote>
<p>线性一致性并非限定在分布式环境下，在单机单核系统中可以简单理解为“寄存器”的特性。</p>
</blockquote>
<h3 id="Raft-线性一致性读"><a href="#Raft-线性一致性读" class="headerlink" title="Raft 线性一致性读"></a>Raft 线性一致性读</h3><p>在了解了什么是线性一致性之后，我们将其与 Raft 结合来探讨。</p>
<p>首先需要明确一个问题，使用了 Raft 的系统都是线性一致的吗？不是的，Raft 只是提供了一个基础，要实现整个系统的线性一致还需要做一些额外的工作。</p>
<p>假设期望基于 Raft 实现一个线性一致的分布式 kv 系统，从最朴素的方案开始，指出每种方案存在的问题，最终使整个系统满足线性一致性。</p>
<h4 id="写主读从缺陷分析"><a href="#写主读从缺陷分析" class="headerlink" title="写主读从缺陷分析"></a>写主读从缺陷分析</h4><p>写操作并不是我们关注的重点，如果稍微看了一些理论部分就应该知道，所有写操作都要作为提案从 leader 节点发起，当然所有的写命令都应该简单交给 leader 处理。真正关键的点在于<strong>读操作的处理方式，这涉及到整个系统关于一致性方面的取舍</strong>。</p>
<p>在该方案中假设读操作直接简单地向 follower 发起，那么由于 Raft 的 Quorum 机制（大部分节点成功即可），针对某个提案在某一时间段内，集群可能会有以下两种状态：</p>
<ul>
<li>某次写操作的日志尚未被复制到一少部分 follower，但 leader 已经将其 commit。</li>
<li>某次写操作的日志已经被同步到所有 follower，但 leader 将其 commit 后，心跳包尚未通知到一部分 follower。</li>
</ul>
<p>以上每个场景客户端都可能读到<strong>过时的数据</strong>，整个系统显然是不满足线性一致的。</p>
<h4 id="写主读主缺陷分析"><a href="#写主读主缺陷分析" class="headerlink" title="写主读主缺陷分析"></a>写主读主缺陷分析</h4><p>在该方案中我们限定，所有的读操作也必须经由 leader 节点处理，读写都经过 leader 难道还不能满足线性一致？是的！！并且该方案存在不止一个问题！！</p>
<h5 id="问题一：状态机落后于-committed-log-导致脏读"><a href="#问题一：状态机落后于-committed-log-导致脏读" class="headerlink" title="问题一：状态机落后于 committed log 导致脏读"></a>问题一：状态机落后于 committed log 导致脏读</h5><p>回想一下前文讲过的，在解释什么是 commit 时提到了写操作什么时候可以响应客户端：</p>
<p>所谓 commit 其实就是对日志简单进行一个标记，表明其可以被 apply 到状态机，并针对相应的客户端请求进行响应。</p>
<p>也就是说一个提案只要被 leader commit 就可以响应客户端了，Raft 并没有限定提案结果在返回给客户端前必须先应用到状态机。所以从客户端视角当我们的某个写操作执行成功后，下一次读操作可能还是会读到旧值。</p>
<p>这个问题的解决方式很简单，在 leader 收到读命令时只需记录下当前的 commit index，当 apply index 追上该 commit index 时，即可将状态机中的内容响应给客户端。</p>
<h5 id="问题二：网络分区导致脏读"><a href="#问题二：网络分区导致脏读" class="headerlink" title="问题二：网络分区导致脏读"></a>问题二：网络分区导致脏读</h5><p>假设集群发生网络分区，旧 leader 位于少数派分区中，而且此刻旧 leader 刚好还未发现自己已经失去了领导权，当多数派分区选出了新的 leader 并开始进行后续写操作时，连接到旧 leader 的客户端可能就会读到旧值了。</p>
<p>因此，仅仅是直接读 leader 状态机的话，系统仍然不满足线性一致性。</p>
<h4 id="Raft-Log-Read"><a href="#Raft-Log-Read" class="headerlink" title="Raft Log Read"></a>Raft Log Read</h4><p>为了确保 leader 处理读操作时仍拥有领导权，可以将读请求同样作为一个提案走一遍 Raft 流程，当这次读请求对应的日志可以被应用到状态机时，leader 就可以读状态机并返回给用户了。</p>
<p>这种读方案称为 <strong>Raft Log Read</strong>，也可以直观叫做 <strong>Read as Proposal</strong>。</p>
<p>为什么这种方案满足线性一致？</p>
<p>因为该方案根据 commit index 对所有读写请求都一起做了线性化，这样每个读请求都能感知到状态机在执行完前一写请求后的最新状态，将读写日志一条一条的应用到状态机，整个系统当然满足线性一致。但该方案的缺点也非常明显，那就是<strong>性能差</strong>，读操作的开销与写操作几乎完全一致。而且由于所有操作都线性化了，我们无法并发读状态机。</p>
<h3 id="Raft-读性能优化"><a href="#Raft-读性能优化" class="headerlink" title="Raft 读性能优化"></a>Raft 读性能优化</h3><p>接下来将介绍几种优化方案，它们在不违背系统线性一致性的前提下，大幅提升了读性能。</p>
<h4 id="Read-Index"><a href="#Read-Index" class="headerlink" title="Read Index"></a>Read Index</h4><p>与 Raft Log Read 相比，Read Index 省掉了同步 log 的开销，<strong>能够大幅提升读的吞吐，一定程度上降低读的时延</strong>。其大致流程为：</p>
<ol>
<li>Leader 在收到客户端读请求时，记录下当前的 commit index，称之为 read index。</li>
<li>Leader 向 followers 发起一次心跳包，这一步是为了确保领导权，避免网络分区时少数派 leader 仍处理请求。</li>
<li>等待状态机<strong>至少</strong>应用到 read index（即 apply index <strong>大于等于</strong> read index）。</li>
<li>执行读请求，将状态机中的结果返回给客户端。</li>
</ol>
<p>这里第三步的 apply index <strong>大于等于</strong> read index 是一个关键点。因为在该读请求发起时，我们将当时的 commit index 记录了下来，只要使客户端读到的内容在该 commit index 之后，那么结果<strong>一定都满足线性一致</strong>）。</p>
<h4 id="Lease-Read"><a href="#Lease-Read" class="headerlink" title="Lease Read"></a>Lease Read</h4><p>与 Read Index 相比，Lease Read 进一步省去了网络交互开销，因此更能<strong>显著降低读的时延</strong>。</p>
<p>基本思路是 leader 设置一个<strong>比选举超时（Election Timeout）更短的时间作为租期</strong>，在租期内我们可以相信其它节点一定没有发起选举，集群也就一定不会存在脑裂，所以在这个时间段内直接读主即可，而非该时间段内可以继续走 Read Index 流程，Read Index 的心跳包也可以为租期带来更新。</p>
<p>Lease Read 可以认为是 Read Index 的时间戳版本，额外依赖时间戳会为算法带来一些不确定性，如果时钟发生漂移会引发一系列问题，因此需要谨慎的进行配置。</p>
<h4 id="Follower-Read"><a href="#Follower-Read" class="headerlink" title="Follower Read"></a>Follower Read</h4><p>在前边两种优化方案中，无论怎么折腾，核心思想其实只有两点：</p>
<ul>
<li>保证在读取时的最新 commit index 已经被 apply。</li>
<li>保证在读取时 leader 仍拥有领导权。</li>
</ul>
<p>其实无论是 Read Index 还是 Lease Read，最终目的都是为了解决第二个问题。换句话说，读请求最终一定都是由 leader 来承载的。</p>
<p>那么读 follower 真的就不能满足线性一致吗？其实不然，这里给出一个可行的读 follower 方案：<strong>Follower 在收到客户端的读请求时，向 leader 询问当前最新的 commit index，反正所有日志条目最终一定会被同步到自己身上，follower 只需等待该日志被自己 commit 并 apply 到状态机后，返回给客户端本地状态机的结果即可</strong>。这个方案叫做 <strong>Follower Read</strong>。</p>
<p>注意：Follower Read 并不意味着我们在读过程中完全不依赖 leader 了，在保证线性一致性的前提下完全不依赖 leader 理论上是不可能做到的。</p>
<p>以上就是 Raft 算法的核心内容及工程实践最需要考虑的内容。</p>
<blockquote>
<p>Redis 中哨兵就是用了 RAFT 算法</p>
</blockquote>
<h1 id="实现分布式锁的方式"><a href="#实现分布式锁的方式" class="headerlink" title="实现分布式锁的方式"></a>实现分布式锁的方式</h1><p>在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。</p>
<p>阻塞锁通常使用互斥量来实现：</p>
<p>互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态；<br>互斥量为 1 表示未锁定状态。<br>1 和 0 可以用一个整型值表示，也可以用某个数据是否存在表示。</p>
<h2 id="数据库唯一索引"><a href="#数据库唯一索引" class="headerlink" title="数据库唯一索引"></a>数据库唯一索引</h2><p>获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。</p>
<p>存在以下几个问题：</p>
<ul>
<li>锁没有失效时间，解锁失败的话其它进程无法再获得该锁；</li>
<li>只能是非阻塞锁，插入失败直接就报错了，无法重试；</li>
<li>不可重入，已经获得锁的进程也必须重新获取锁。</li>
</ul>
<h2 id="Redis-的-SETNX-指令"><a href="#Redis-的-SETNX-指令" class="headerlink" title="Redis 的 SETNX 指令"></a>Redis 的 SETNX 指令</h2><p>使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。</p>
<p>SETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。</p>
<p>EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。</p>
<h2 id="Redis-的-RedLock-算法"><a href="#Redis-的-RedLock-算法" class="headerlink" title="Redis 的 RedLock 算法"></a>Redis 的 RedLock 算法</h2><p>使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。</p>
<ul>
<li>尝试从 N 个互相独立 Redis 实例获取锁；</li>
<li>计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功；</li>
<li>如果获取锁失败，就到每个实例上释放锁。</li>
</ul>
<h2 id="使用Zookeeper"><a href="#使用Zookeeper" class="headerlink" title="使用Zookeeper"></a>使用Zookeeper</h2><h1 id="负载均衡的方式和实现"><a href="#负载均衡的方式和实现" class="headerlink" title="负载均衡的方式和实现"></a>负载均衡的方式和实现</h1><p>集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。</p>
<p>负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。</p>
<p>负载均衡器可以用来实现高可用以及伸缩性：</p>
<ul>
<li>高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；</li>
<li>伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。</li>
</ul>
<p>负载均衡器运行过程包含两个部分：</p>
<ul>
<li>根据负载均衡算法得到转发的节点；</li>
<li>进行转发。</li>
</ul>
<h2 id="负载均衡算法"><a href="#负载均衡算法" class="headerlink" title="负载均衡算法"></a>负载均衡算法</h2><h3 id="1-轮询"><a href="#1-轮询" class="headerlink" title="1. 轮询"></a>1. 轮询</h3><p>轮询算法把每个请求轮流发送到每个服务器上。</p>
<p>该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载。</p>
<h3 id="2-加权轮询"><a href="#2-加权轮询" class="headerlink" title="2. 加权轮询"></a>2. 加权轮询</h3><p>加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。</p>
<h3 id="3-最少连接"><a href="#3-最少连接" class="headerlink" title="3. 最少连接"></a>3. 最少连接</h3><p>由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。</p>
<p>最少连接算法就是将请求发送给当前最少连接数的服务器上。</p>
<h3 id="4-加权最少连接"><a href="#4-加权最少连接" class="headerlink" title="4. 加权最少连接"></a>4. 加权最少连接</h3><p>在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。</p>
<h3 id="5-随机算法"><a href="#5-随机算法" class="headerlink" title="5. 随机算法"></a>5. 随机算法</h3><p>把请求随机发送到服务器上。</p>
<p>和轮询算法类似，该算法比较适合服务器性能差不多的场景。</p>
<h3 id="6-源地址哈希法"><a href="#6-源地址哈希法" class="headerlink" title="6. 源地址哈希法"></a>6. 源地址哈希法</h3><p>源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号。</p>
<p>可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）</p>
<h2 id="转发的实现"><a href="#转发的实现" class="headerlink" title="转发的实现"></a>转发的实现</h2><h3 id="1-HTTP-重定向"><a href="#1-HTTP-重定向" class="headerlink" title="1. HTTP 重定向"></a>1. HTTP 重定向</h3><p>HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302。客户端收到重定向报文之后，需要重新向服务器发起请求。</p>
<p>缺点：</p>
<ul>
<li>需要两次请求，因此访问延迟比较高；</li>
<li>HTTP 负载均衡器处理能力有限，会限制集群的规模。</li>
</ul>
<p>该负载均衡转发的缺点比较明显，实际场景中很少使用它。</p>
<h3 id="2-DNS-域名解析"><a href="#2-DNS-域名解析" class="headerlink" title="2. DNS 域名解析"></a>2. DNS 域名解析</h3><p>在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址。</p>
<p>优点：</p>
<ul>
<li>DNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。</li>
</ul>
<p>缺点：</p>
<ul>
<li>由于 DNS 具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效。</li>
</ul>
<p>大型网站基本使用了 DNS 做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。</p>
<h3 id="3-反向代理服务器"><a href="#3-反向代理服务器" class="headerlink" title="3. 反向代理服务器"></a>3. 反向代理服务器</h3><p>反向代理服务器位于源服务器前面，用户的请求需要先经过反向代理服务器才能到达源服务器。反向代理可以用来进行缓存、日志记录等，同时也可以用来做为负载均衡服务器。</p>
<p>在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。</p>
<p>优点：</p>
<ul>
<li>与其它功能集成在一起，部署简单。</li>
</ul>
<p>缺点：</p>
<ul>
<li>所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈。</li>
</ul>
<h3 id="4-网络层"><a href="#4-网络层" class="headerlink" title="4. 网络层"></a>4. 网络层</h3><p>在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。</p>
<p>源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。</p>
<p>优点：</p>
<ul>
<li>在内核进程中进行处理，性能比较高。</li>
</ul>
<p>缺点：</p>
<ul>
<li>和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。</li>
</ul>
<h3 id="5-链路层"><a href="#5-链路层" class="headerlink" title="5. 链路层"></a>5. 链路层</h3><p>在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发。</p>
<p>通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。</p>
<p>这是一种三角传输模式，被称为直接路由。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。</p>
<p>这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的负载均衡服务器为 LVS（Linux Virtual Server）。</p>
<h1 id="负载均衡session管理"><a href="#负载均衡session管理" class="headerlink" title="负载均衡session管理"></a>负载均衡session管理</h1><p>一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作。</p>
<h2 id="Sticky-Session"><a href="#Sticky-Session" class="headerlink" title="Sticky Session"></a>Sticky Session</h2><p>需要配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中。</p>
<p>缺点：</p>
<ul>
<li>当服务器宕机时，将丢失该服务器上的所有 Session。</li>
</ul>
<p><img src="https://qiniu.xiaoming.net.cn/Sticky%20Session.png" alt="Sticky Session"></p>
<h2 id="Session-Replication"><a href="#Session-Replication" class="headerlink" title="Session Replication"></a>Session Replication</h2><p>在服务器之间进行 Session 同步操作，每个服务器都有所有用户的 Session 信息，因此用户可以向任何一个服务器进行请求。</p>
<p>缺点：</p>
<ul>
<li>占用过多内存；</li>
<li>同步过程占用网络带宽以及服务器处理器时间。</li>
</ul>
<p><img src="https://qiniu.xiaoming.net.cn/Session%20Replication.png" alt="Session Replication"></p>
<h2 id="Session-Server"><a href="#Session-Server" class="headerlink" title="Session Server"></a>Session Server</h2><p>使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL，也使用 Redis 或者 Memcached 这种内存型数据库。</p>
<p>优点：</p>
<ul>
<li>为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要去实现存取 Session 的代码。</li>
</ul>
<p><img src="https://qiniu.xiaoming.net.cn/Session%20Server.png" alt="Session Server"></p>
<h1 id="微服务架构"><a href="#微服务架构" class="headerlink" title="微服务架构"></a>微服务架构</h1><h2 id="微服务的概念"><a href="#微服务的概念" class="headerlink" title="微服务的概念"></a>微服务的概念</h2><blockquote>
<p>就目前而言，对于微服务业界并没有一个统一的、标准的定义（While there is no precise definition of this architectural style ) 。<br>但通在其常而言，微服务架构是一种架构模式或者说是一种架构风格，它提倡将单一应用程序划分成一组小的服务，每个服务运行独立的自己的进程中，服务之间互相协调、互相配合，为用户提供最终价值。服务之间采用轻量级的通信机制互相沟通（通常是基于 HTTP 的 RESTful API ) 。每个服务都围绕着具体业务进行构建，并且能够被独立地部署到生产环境、类生产环境等。<br>另外，应尽量避免统一的、集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具对其进行构建，可以有一个非常轻量级的集中式管理来协调这些服务。可以使用不同的语言来编写服务，也可以使用不同的数据存储。</p>
</blockquote>
<p>微服务具有以下特点：</p>
<ul>
<li>单一职责：微服务架构中的每个服务，都是具有业务逻辑的，符合高内聚、低耦合原则以及单一职责原则的单元，不同的服务通过“管道”的方式灵活组合，从而构建出庞大的系统。</li>
<li>进程独立：每一组服务都是独立运行的，可能我这个服务运行在tomcat容器，而另一个服务运行在jetty上。可以通过进程方式，不断的横向扩展整个服务。</li>
<li>轻量级通信：过去的协议都是很重的，就像ESB，就像SOAP，轻通信，这意味着相比过去更智能更轻量的服务相互调用，就所谓 smart endpoints and dumb pipes，这些 endpoint 都是解耦的，完成一个业务通信调用串起这些 micro service 就像是 linux 系统中通过管道串起一系列命令业务（通常使用 RESE 作为轻量级通信机制）。  </li>
<li>基于业务的能力：过去的业务，我们通常会考虑各种各样的依赖关系，考虑系统耦合带来的问题。微服务，可以让开发者更专注于业务的逻辑开发。</li>
<li>独立部署：不止业务要独立，部署也要独立。不过这也意味着，传统的开发流程会出现一定程度的改变，开发的适合也要有一定的运维指责</li>
<li>无集中式管理：传统的企业级 SOA 服务往往很大，不易于管理，耦合性高，团队开发成本比较大。微服务，可以让团队各思其政的选择技术实现，不同的 service 可以根据各自的需要选择不同的技术栈来实现其业务逻辑。</li>
</ul>
<h2 id="和单体应用对比"><a href="#和单体应用对比" class="headerlink" title="和单体应用对比"></a>和单体应用对比</h2><h3 id="单体应用的优缺点："><a href="#单体应用的优缺点：" class="headerlink" title="单体应用的优缺点："></a>单体应用的优缺点：</h3><p>优点：</p>
<ul>
<li>易于开发： 开发方式简单，IDE 支持好，方便运行和调试。</li>
<li>易于测试： 所有功能运行在一个进程中，一旦进程启动，便可以进行系统测试。</li>
<li>易于部署： 只需要将打好的一个软件包发布到服务器即可。</li>
<li>易于水平伸缩： 只需要创建一个服务器节点，配置好运行时环境，再将软件包发布到新服务器节点即可运行程序（当然也需要采取分发策略保证请求能有效地分发到新节点）。</li>
</ul>
<p>缺点：</p>
<ul>
<li>维护成本大： 当应用程序的功能越来越多、团队越来越大时，沟通成本、管理成本显著增加。当出现 bug 时，可能引起 bug 的原因组合越来越多，导致分析、定位和修复的成本增加；并且在对全局功能缺乏深度理解的情况下，容易在修复 bug 时引入新的 bug。</li>
<li>持续交付周期长： 构建和部署时间会随着功能的增多而增加，任何细微的修改都会触发部署流水线。</li>
<li>新人培养周期长： 新成员了解背景、熟悉业务和配置环境的时间越来越长。</li>
<li>技术选型成本高： 单块架构倾向于采用统一的技术平台或方案来解决所有问题，如果后续想引入新的技术或框架，成本和风险都很大。</li>
<li>可扩展性差： 随着功能的增加，垂直扩展的成本将会越来越大；而对于水平扩展而言，因为所有代码都运行在同一个进程，没办法做到针对应用程序的部分功能做独立的扩展。</li>
</ul>
<h3 id="微服务的优缺点"><a href="#微服务的优缺点" class="headerlink" title="微服务的优缺点"></a>微服务的优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>每个服务足够内聚，足够小，代码容易理解这样能聚焦一个指定的业务功能或业务需求</li>
<li>开发简单、开发效率提高，一个服务可能就是专一的只干一件事。</li>
<li>微服务能够被小团队单独开发，这个小团队是 2 到 5 人的开发人员组成。</li>
<li>微服务是松藕合的，是有功能意义的服务，无论是在开发阶段或部署阶段都是独立的。</li>
<li>微服务能使用不同的语言开发。</li>
<li>易于和第三方集成，微服务允许容易且灵活的方式集成自动部署，通过持续集成工具，如Jenkins,Hudson,bamboo。</li>
<li>微服务易于被一个开发人员理解，修改和维护，这样小团队能够更关注自己的工作成果。无需通过合作才能体现价值。微服务允许你利用融合最新技术。</li>
<li>微服务只是业务逻辑的代码，不会和 HTML,CSS 或其他界面组件混合。</li>
<li>每个微服务都有自己的存储能力，可以有自己的数据库。也可以有统一数据库。</li>
</ul>
<p>总的来说，微服务的优势，就是在于，面对大的系统，可以有效的减少复杂程度，使服务架构的逻辑更清晰明了。</p>
<p>但是这样也会带来很多问题，就譬如分布式环境下的数据一致性，测试的复杂性，运维的复杂性。</p>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>缺点：</p>
<ul>
<li>微服务提高了系统的复杂度；</li>
<li>开发人员要处理分布式系统的复杂性：性能、可靠性、异步、数据一致性等</li>
<li>服务之间的分布式通信问题；</li>
<li>服务的注册与发现问题；</li>
<li>服务之间的分布式事务问题；</li>
<li>数据隔离带来的报表处理问题；</li>
<li>服务之间的分布式一致性问题；</li>
<li>服务管理的复杂性，服务的编排；</li>
<li>不同服务实例的管理。</li>
</ul>
<h2 id="微服务架构体系"><a href="#微服务架构体系" class="headerlink" title="微服务架构体系"></a>微服务架构体系</h2><p>微服务架构体系主要包括服务注册与发现、服务网关、服务配置中心、服务通信、服务监控以及服务的熔断、隔离、限流、降级。</p>
<h3 id="服务注册与发现——动态扩容"><a href="#服务注册与发现——动态扩容" class="headerlink" title="服务注册与发现——动态扩容"></a>服务注册与发现——动态扩容</h3><p>首先，部署一个服务发现服务，它提供所有已注册服务的地址信息的服务。DNS 也算是一种服务发现服务。然后各个应用服务在启动时自动将自己注册到服务发现服务上。并且应用服务启动后会实时（定期）从服务发现服务同步各个应用服务的地址列表到本地。服务发现服务也会定期检查应用服务的健康状态，去掉不健康的实例地址。这样新增实例时只需要部署新实例，实例下线时直接关停服务即可，服务发现会自动检查服务实例的增减。</p>
<p>服务发现还会跟客户端负载均衡配合使用。由于应用服务已经同步服务地址列表在本地了，所以访问微服务时，可以自己决定负载策略。甚至可以在服务注册时加入一些元数据（服务版本等信息），客户端负载则根据这些元数据进行流量控制，实现A/B测试、蓝绿发布等功能。服务发现有很多组件可以选择，比如说Zookeeper 、Eureka、Consul、Etcd等。</p>
<p>有三种实现方式：</p>
<p>第一种：</p>
<p>开发人员开发了程序以后，会找运维配一个域名，服务的话通过 DNS 就能找到我们对应的服务。</p>
<p>缺点是，由于服务没有负载均衡功能，对负载均衡服务，可能会有相当大的性能问题。</p>
<p>第二种，是目前普遍的做法。每一个服务都通过服务端内置的功能注册到注册中心，服务消费者不断轮询注册中心发现对应的服务，使用内置负载均衡调用服务。</p>
<p>缺点是，对多语言环境不是很好，你需要单独给消费者的客户端开发服务发现和负载均衡功能。当然了，这个方法通常都是用在 Spring Cloud 上的。</p>
<p>第三种，是将客户端和负载均衡放在同一个主机，而不是同一个进程内。</p>
<p>这种方法相对第一种第二种方法来说，改善了他们的缺点，但是会极大增加运维成本。</p>
<h3 id="服务网关——权限控制，服务治理"><a href="#服务网关——权限控制，服务治理" class="headerlink" title="服务网关——权限控制，服务治理"></a>服务网关——权限控制，服务治理</h3><p>拆分成微服务后，出现大量的服务，大量的接口，使得整个调用关系乱糟糟的。经常在开发过程中，写着写着，忽然想不起某个数据应该调用哪个服务。或者写歪了，调用了不该调用的服务，本来一个只读的功能结果修改了数据……</p>
<p>为了应对这些情况，微服务的调用需要一个把关的东西，也就是网关。在调用者和被调用者中间加一层网关，每次调用时进行权限校验。另外，网关也可以作为一个提供服务接口文档的平台。</p>
<p>使用网关有一个问题就是要决定在多大粒度上使用：最粗粒度的方案是整个微服务一个网关，微服务外部通过网关访问微服务，微服务内部则直接调用；最细粒度则是所有调用，不管是微服务内部调用或者来自外部的调用，都必须通过网关。折中的方案是按照业务领域将微服务分成几个区，区内直接调用，区间通过网关调用。</p>
<p>网关的作用：</p>
<ul>
<li>反向路由：很多时候，公司不想让外部人员看到公司的内部，就需要网关来进行反向路由。即将外部请求转换成内部具体服务条用</li>
<li>安全认证：网络中会有很多恶意访问，譬如爬虫，譬如黑客攻击，网关维护安全功能。</li>
<li>限流熔断：当请求很多服务不堪重负，会让我们的服务自动关闭，导致不能用服务。限流熔断可以有效的避免这类问题</li>
<li>日志监控：所有的外面的请求都会经过网关，这样我们就可以使用网关来记录日志信息</li>
<li>灰度发布，蓝绿部署：是指能够平滑过渡的一种发布方式。在其上可以进行 A/B testing，即让一部分用户继续用产品特性 A，一部分用户开始用产品特性 B，如果用户对 B 没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到 B 上面来。</li>
</ul>
<p>开源网关 Zuul 架构：</p>
<p><img src="http://qiniu.xiaoming.net.cn/%E5%BC%80%E6%BA%90%E7%BD%91%E5%85%B3Zuul%E6%9E%B6%E6%9E%84" alt="开源网关 Zuul 架构"></p>
<p>zuul 网关核心其实是一个 servlet，所有请求都会经过 zuul servlet 传到 <code>zuulFilter Runner</code>，然后分发到三种过滤器。</p>
<p>先说说架构图左半部分，分别是使用 Groovy 实现的前置路由过滤器，路由过滤器，后置路由过滤器。</p>
<p>一般请求都会先经过前置路由过滤器处理，一般的自定义 java 封装逻辑也会在这里实现。</p>
<p>路由过滤器，实现的是找到对应的微服务进行调用。</p>
<p>调用完了，响应回来，会经过后置路由过滤器，通过后置路由过滤器我们可以封装日志审计的处理。</p>
<p>可以说zuul网关最大的特色就是它三层过滤器。</p>
<p>架构图右半部分，是 zuul 网关设计的自定义过滤器加载机制。网关内部会有生产者消费者模型，自动的将过滤器脚本发布到zuul网关读取加载运行。</p>
<h3 id="配置中心"><a href="#配置中心" class="headerlink" title="配置中心"></a>配置中心</h3><p>以前，开发人员把配置文件放在开发文件里面，这样会有很多隐患。譬如，配置规范不同，无法追溯配置人员。一旦需要大规模改动配置，改动时间会很长，无法追溯配置人员，从而影响整个产品，后果是我们承担不起的。因此就有了配置中心。</p>
<p>以 Spring Cloud Config 来说，它是用来为分布式系统中的基础设施和微服务应用提供集中化的外部配置支持，它分为服务端与客户端两个部分。其中服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置仓库并为客户端提供获取配置信息、加密/解密信息等访问接口；而客户端则是微服务架构中的各个微服务应用或基础设施，它们通过指定的配置中心来管理应用资源与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息。</p>
<h3 id="服务通讯"><a href="#服务通讯" class="headerlink" title="服务通讯"></a>服务通讯</h3><p>服务间远程调用方式一般有两种：RPC 和 REST</p>
<table>
<thead>
<tr>
<th></th>
<th>RPC</th>
<th>REST</th>
</tr>
</thead>
<tbody><tr>
<td>耦合性</td>
<td>强耦合</td>
<td>松散耦合</td>
</tr>
<tr>
<td>消息协议</td>
<td>TCP</td>
<td>HTTP</td>
</tr>
<tr>
<td>通讯协议</td>
<td>二进制</td>
<td>文本XML，Json</td>
</tr>
<tr>
<td>性能</td>
<td>高</td>
<td>低于RPC</td>
</tr>
<tr>
<td>接口契约IDL</td>
<td>thrift,protobuf,IdL</td>
<td>Swagger</td>
</tr>
<tr>
<td>客户端</td>
<td>强类型客户端，一般自动生成</td>
<td>一般HTTP可访问，生成强类型客户端，多语言支持好</td>
</tr>
<tr>
<td>案例</td>
<td>Dubbo，Dubbox,motan,tars,grpc,thrift</td>
<td>spring boot,tax-rs,dropwizard</td>
</tr>
<tr>
<td>开发者友好</td>
<td>客户端比较方面，二进制消息不能读</td>
<td>可读消息</td>
</tr>
<tr>
<td>对外开放</td>
<td>一般需要转成REST/文本协议</td>
<td>可直接对外开发</td>
</tr>
</tbody></table>
<h3 id="服务监控预警——发现故障的征兆"><a href="#服务监控预警——发现故障的征兆" class="headerlink" title="服务监控预警——发现故障的征兆"></a>服务监控预警——发现故障的征兆</h3><p>在高并发分布式的场景下，故障经常是突然间就雪崩式爆发。所以必须建立完善的监控体系，尽可能发现故障的征兆。微服务架构中组件繁多，各个组件所需要监控的指标不同。比如 Redis 缓存一般监控占用内存值、网络流量，数据库监控连接数、磁盘空间，业务服务监控并发数、响应延迟、错误率等。因此如果做一个大而全的监控系统来监控各个组件是不大现实的，而且扩展性会很差。一般的做法是让各个组件提供报告自己当前状态的接口（ metrics 接口），这个接口输出的数据格式应该是一致的。然后部署一个指标采集器组件，定时从这些接口获取并保持组件状态，同时提供查询服务。最后还需要一个UI，从指标采集器查询各项指标，绘制监控界面或者根据阈值发出告警。</p>
<p>微服务可分5个监控点：日志监控，Metrics监控，健康检查，调用链检查，告警系统。</p>
<h4 id="监控架构"><a href="#监控架构" class="headerlink" title="监控架构"></a>监控架构</h4><p>每一个服务都有一个agent，agent收集到关键信息，会传到一些 MQ 中，为了解耦。同时将日志传入 ELK，将指标传入 InfluxDB 时间序列库。而像 nagios，可以定期向 agent 发起信息检查微服务。</p>
<h4 id="链路追踪"><a href="#链路追踪" class="headerlink" title="链路追踪"></a>链路追踪</h4><p>在微服务架构下，一个用户的请求往往涉及多个内部服务调用。为了方便定位问题，需要能够记录每个用户请求时，微服务内部产生了多少服务调用，及其调用关系。这个叫做链路跟踪。</p>
<p>要实现链路跟踪，每次服务调用会在 HTTP 的 HEADERS 中记录至少记录四项数据：</p>
<ul>
<li>traceId：traceId 标识一个用户请求的调用链路。具有相同 traceId 的调用属于同一条链路。</li>
<li>spanId：标识一次服务调用的ID，即链路跟踪的节点 ID。</li>
<li>parentId：父节点的 spanId。</li>
<li>requestTime &amp; responseTime：请求时间和响应时间。</li>
</ul>
<p>另外，还需要调用日志收集与存储的组件，以及展示链路调用的UI组件。</p>
<h3 id="熔断、隔离、限流和降级"><a href="#熔断、隔离、限流和降级" class="headerlink" title="熔断、隔离、限流和降级"></a>熔断、隔离、限流和降级</h3><p>面对巨大的突发流量下，大型公司一般会采用一系列的熔断（系统自动将服务关闭防止让出现的问题最大化）、隔离（将服务和服务隔离，防止一个服务挂了其他服务不能访问）、限流（单位时间内之允许一定数量用户访问）、降级（当整个微服务架构整体的负载超出了预设的上限阈值或即将到来的流量预计将会超过预设的阈值时，为了保证重要或基本的服务能正常运行，我们可以将一些不重要或不紧急的服务或任务进行服务的延迟使用 或暂停使用）措施。</p>
<h4 id="熔断"><a href="#熔断" class="headerlink" title="熔断"></a>熔断</h4><p>熔断当一个服务因为各种原因停止响应时，调用方通常会等待一段时间，然后超时或者收到错误返回。如果调用链路比较长，可能会导致请求堆积，整条链路占用大量资源一直在等待下游响应。所以当多次访问一个服务失败时，应熔断，标记该服务已停止工作，直接返回错误。直至该服务恢复正常后再重新建立连接。</p>
<p><img src="http://qiniu.xiaoming.net.cn/%E7%86%94%E6%96%AD%E6%9C%BA%E5%88%B6.jpg" alt="熔断机制"></p>
<h4 id="服务降级"><a href="#服务降级" class="headerlink" title="服务降级"></a>服务降级</h4><p>当下游服务停止工作后，如果该服务并非核心业务，则上游服务应该降级，以保证核心业务不中断。比如网上超市下单界面有一个推荐商品凑单的功能，当推荐模块挂了后，下单功能不能一起挂掉，只需要暂时关闭推荐功能即可。</p>
<h4 id="限流"><a href="#限流" class="headerlink" title="限流"></a>限流</h4><p>一个服务挂掉后，上游服务或者用户一般会习惯性地重试访问。这导致一旦服务恢复正常，很可能因为瞬间网络流量过大又立刻挂掉，在棺材里重复着仰卧起坐。因此服务需要能够自我保护——限流。限流策略有很多，最简单的比如当单位时间内请求数过多时，丢弃多余的请求。另外，也可以考虑分区限流。仅拒绝来自产生大量请求的服务的请求。例如商品服务和订单服务都需要访问促销服务，商品服务由于代码问题发起了大量请求，促销服务则只限制来自商品服务的请求，来自订单服务的请求则正常响应。</p>
<h1 id="分布式服务接口的幂等性如何设计（比如不能重复扣款）？"><a href="#分布式服务接口的幂等性如何设计（比如不能重复扣款）？" class="headerlink" title="分布式服务接口的幂等性如何设计（比如不能重复扣款）？"></a>分布式服务接口的幂等性如何设计（比如不能重复扣款）？</h1><p>所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款、不能多插入一条数据、不能将统计值多加了 1。这就是幂等性。</p>
<h2 id="设计方案"><a href="#设计方案" class="headerlink" title="设计方案"></a>设计方案</h2><p><strong>查询操作</strong>：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select 是天然的幂等操作；</p>
<p><strong>删除操作</strong>：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ；</p>
<p><strong>唯一索引，防止新增脏数据</strong>。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；</p>
<p><strong>token机制，防止页面重复提交</strong>：</p>
<ul>
<li>业务要求：页面的数据只能被点击提交一次；</li>
<li>发生原因：由于重复点击或者网络重发，或者 nginx 重发等情况会导致数据被重复提交；</li>
<li>解决办法：集群环境采用 token 加 redis (redis单线程的，处理需要排队)；单JVM环境：采用 token 加 redis 或 token 加 jvm 内存。</li>
<li>处理流程：</li>
</ul>
<ol>
<li>数据提交前要向服务的申请 token，token 放到 redis 或 jvm 内存，token 有效时间；</li>
<li>提交后后台校验 token，同时删除 token，生成新的 token 返回。token 特点：要申请，一次有效性，可以限流。</li>
</ol>
<p>注意：redis 要用删除操作来判断 token，删除成功代表 token 校验通过，如果用 select + delete 来校验 token，存在并发问题，不建议使用；</p>
<p><strong>悲观锁——获取数据的时候加锁获取</strong>：<code>select * from table_xxx where id=&#39;xxx&#39; for update</code>; 注意：id 字段一定是主键或者唯一索引，不然是锁表，悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用； </p>
<p><strong>乐观锁——乐观锁只是在更新数据那一刻锁表</strong>，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过 version 或者其他状态条件：</p>
<ol>
<li>通过版本号实现 <code>update table_xxx set name=#name#,version=version+1 where version=#version#</code></li>
<li>通过条件限制 <code>update table_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# &gt;= 0</code>。要求：quality-#subQuality# &gt;=  0，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高；</li>
</ol>
<p><strong>分布式锁</strong>：还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户 ID + 后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)；</p>
<p><strong>select + insert</strong>: 并发不高的后台系统，或者一些任务 JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法；</p>
<p><strong>对外提供接口的 api 如何保证幂等</strong>。如银联提供的付款接口：需要接入商户提交付款请求时附带：source 来源，seq 序列号 </p>
<p>source + seq 在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源 source，一个是来源方序列号 seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。 </p>
<h1 id="微服务如何进行拆分"><a href="#微服务如何进行拆分" class="headerlink" title="微服务如何进行拆分"></a>微服务如何进行拆分</h1><h2 id="服务粒度"><a href="#服务粒度" class="headerlink" title="服务粒度"></a>服务粒度</h2><ul>
<li>三个火枪手原则，即一个微服务三个人负责开发</li>
<li>从系统规模来讲，3个人负责开发一个系统，系统的复杂度刚好达到每个人都能全面理解整个系统，又能够进行分工的粒度；2个人，系统的复杂度不够，开发人员可能觉得无法体现自己的技术实力；4个及以上，系统复杂度又无法让开发人员对系统的细节都了解很深</li>
<li>从团队管理来说，3个人可以形成一个稳定的备份，即使一个人休假或者调配到其他系统，剩余2个人还可以支撑；2个人压力太大；一个人就是单点啦</li>
</ul>
<h2 id="拆分方法"><a href="#拆分方法" class="headerlink" title="拆分方法"></a>拆分方法</h2><p>基于“三个火枪手”的理论，可以计算出拆分后合适的服务数量</p>
<h3 id="基于业务逻辑拆分"><a href="#基于业务逻辑拆分" class="headerlink" title="基于业务逻辑拆分"></a>基于业务逻辑拆分</h3><p>将系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务。</p>
<p>难点问题在于，对“职责范围”的理解差异很大。例如，一个电商系统，第一种方式是将服务划分为“商品”“交易”“用户”3个服务，第二种方式是划分为“商品”“订单”“支付”“发货”“卖家”“买家”6个服务，哪种方式更合理？</p>
<p>困惑在于从业务的角度来拆分，规模粗和细都没有问题，因为拆分基础都是业务逻辑，要判断拆分粒度，不能从业务逻辑角度，根据“三个火枪手”原则，计算一下大概的服务范围</p>
<p>例如，有10个人，按以上原则，大约需要划分4个服务，那么“登录、注册、用户信息管理”都可以划到“用户服务”职责范围内；如果团队规模是100人支撑服务，服务数量可以达到40个，那么“用户登录”就是一个服务了；如果团队规模达到1000人支撑业务，那“用户连接管理”可能就是一个独立的服务了</p>
<h3 id="基于可扩展拆分"><a href="#基于可扩展拆分" class="headerlink" title="基于可扩展拆分"></a>基于可扩展拆分</h3><p>将系统中的业务模块按照稳定性排序，将已经成熟和改动不大的服务拆分为稳定服务，将经常变化和迭代的服务拆分为变动服务</p>
<p>稳定的服务粒度可以粗一些，即使逻辑上没有强关联的服务，也可以放在同一个子系统中，例如将“日志服务”和“升级服务”放在同一个子系统中；不稳定的服务粒度可以细一些，但不要太细，始终记住要控制服务的总数量</p>
<p>这样的拆分主要是为了提升项目快速迭代的效率，避免在开发的时候，不小心影响了已有的成熟功能导致线上问题。</p>
<h3 id="基于可靠性拆分"><a href="#基于可靠性拆分" class="headerlink" title="基于可靠性拆分"></a>基于可靠性拆分</h3><ul>
<li>将系统中的业务模块按照优先级排序，将可靠性要求高的核心服务和要求低的非核心服务拆分开来，然后重点保证核心服务的高可用。</li>
</ul>
<p>好处：避免非核心服务故障影响核心服务</p>
<p>例如，日志上报一般都属于非核心服务，但是在某些场景下可能有大量的日志上报，如果系统没有拆分，那么日志上报可能导致核心服务故障；拆分后即使日志上报有问题，也不会影响核心服务</p>
<ul>
<li>核心服务高可用方案可以更加简单</li>
</ul>
<p>核心服务的功能逻辑更加简单，存储的数据可能更少，用到的组件也会更少，设计高可用方案部分情况下要比不拆分简单很多</p>
<ul>
<li>能够降低高可用成本</li>
</ul>
<p>将核心服务拆分出来后，核心服务占用的机器、带宽等资源比不拆分要少很多。因此，只针对核心服务做高可用方案，机器、带宽等成本比不拆分要节省较多</p>
<h3 id="基于性能拆分"><a href="#基于性能拆分" class="headerlink" title="基于性能拆分"></a>基于性能拆分</h3><p>将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其他服务</p>
<p>常见的拆分方式和具体的性能瓶颈有关，可以拆分Web服务、数据库、缓存等</p>
<p>例如，电商的抢购，性能压力最大的是入口的排队功能，可以将排队功能独立为一个服务</p>
<p>以上拆分，可以根据实际情况自由排列组合</p>
<h1 id="微服务和-SOA-区别"><a href="#微服务和-SOA-区别" class="headerlink" title="微服务和 SOA 区别"></a>微服务和 SOA 区别</h1><h2 id="应用SOA化-1"><a href="#应用SOA化-1" class="headerlink" title="应用SOA化"></a>应用SOA化</h2><p>所谓的SOA化，就是业务的服务化。SOA（Service Oriented Architecture），即面向服务的架构。比如原来单机支撑了整个电商网站，现在对整个网站进行拆解，分离出了订单中心、用户中心、库存中心。对于订单中心，有专门的数据库存储订单信息，用户中心也有专门的数据库存储用户信息，库存中心也会有专门的数据库存储库存信息。这时候如果要同时对订单和库存进行操作，那么就会涉及到订单数据库和库存数据库，为了保证数据一致性，就需要用到分布式事务。</p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p>微服务是 SOA 发展出来的产物，它是一种比较细化的 SOA 实现方式。</p>
<p>较早实践微服务的公司 Netflix 就曾经称他们构建的架构是“细粒度的SOA”</p>
<p>SOA的出现其实是为了解决历史问题：企业在信息化的过程中会有各种各样互相隔离的系统，需要有一种机制将他们整合起来。同样的，也造成了 SOA 初期的服务是很大的概念，通常指定的一个可以独立运作的系统（这样看，好像服务间天然的松耦合）。这种做法相当于是把子系统服务化。</p>
<p>而微服务轻装上阵，服务的尺寸通常不会太大，关于服务的尺寸，在实际情况中往往是一个服务应该能够代表实际业务场景中的一块不可分割或不易分割的业务实体。将服务的尺寸控制在一个较小的体量可以带来很多的好处：</p>
<ul>
<li>更易于实现低耦合、高内聚</li>
<li>更易于维护</li>
<li>更易于扩展</li>
<li>更易于关注实际业务场景</li>
</ul>
<h1 id="单体应用怎么改造成分布式应用"><a href="#单体应用怎么改造成分布式应用" class="headerlink" title="单体应用怎么改造成分布式应用"></a>单体应用怎么改造成分布式应用</h1><p>单体由于流量越来越大出现服务器性能问题。</p>
<h2 id="改进1-应用服务器和数据库服务器分离"><a href="#改进1-应用服务器和数据库服务器分离" class="headerlink" title="改进1:应用服务器和数据库服务器分离"></a>改进1:应用服务器和数据库服务器分离</h2><p>对架构增加了一台服务器，应用和数据库分别部署到不同的服务器上，对于开发和测试没有任何影响，只需要应用服务器新增一个远程调用数据库服务器的连接，有效的缓解了应用服务器负载的压力。</p>
<p>出现以下问题：</p>
<ul>
<li>随着请求流量得进一步增大出现应用服务器性能问题。</li>
</ul>
<h2 id="改进2-应用服务器集群"><a href="#改进2-应用服务器集群" class="headerlink" title="改进2:应用服务器集群"></a>改进2:应用服务器集群</h2><p>流量请求得到缓解。</p>
<p>应用服务器集群后出现以下问题：</p>
<ol>
<li>需要使用 session+cookie 维护用户</li>
<li>如何做请求转发（cdn，前端做负载均衡器）</li>
</ol>
<h2 id="改进3-负载均衡器"><a href="#改进3-负载均衡器" class="headerlink" title="改进3:负载均衡器"></a>改进3:负载均衡器</h2><ol>
<li>负载均衡器优化了访问请求在服务器组之间的分配，消除了服务器之间的负载不平衡，从而提高了系统的反应速度与总体性能；</li>
<li>负载均衡器可以对服务器的运行状况进行监控，及时发现运行异常的服务器，并将访问请求转移到其它可以正常工作的服务器上，从而提高服务器组的可靠性采用了负均衡器器以后，可以根据业务量的发展情况灵活增加服务器，系统的扩展能力得到提高，同时简化了管理。</li>
</ol>
<p>负载均衡器之后出现以下问题：<br>随着流量的新增，数据库服务器有性能压力，数据库遇到瓶颈。</p>
<h2 id="改进4-数据库服务器集群"><a href="#改进4-数据库服务器集群" class="headerlink" title="改进4:数据库服务器集群"></a>改进4:数据库服务器集群</h2><p>数据库服务器集群后出现以下问题：</p>
<ol>
<li>数据库读写分离</li>
<li>数据库数据同步</li>
<li>数据库路由</li>
</ol>
<h2 id="改进5-缓存服务器"><a href="#改进5-缓存服务器" class="headerlink" title="改进5:缓存服务器"></a>改进5:缓存服务器</h2><ul>
<li>用户量是没有上限的</li>
<li>缓存、 限流、 降级</li>
</ul>
<h2 id="改进6-数据库水平-垂直拆分"><a href="#改进6-数据库水平-垂直拆分" class="headerlink" title="改进6:数据库水平/垂直拆分"></a>改进6:数据库水平/垂直拆分</h2><p>目前将数据库进行垂直拆分，还未进行数据库水平拆分（比如将订单表分库分表就属于水平拆分）</p>
<h2 id="改进7-应用服务器垂直拆分"><a href="#改进7-应用服务器垂直拆分" class="headerlink" title="改进7: 应用服务器垂直拆分"></a>改进7: 应用服务器垂直拆分</h2><p>根据不同域名请求访问不同服务器，如果涉及到用户需要查询商品或订单，直接在用户服务器里写DAO层查询商品或订单数据库表。</p>
<p>产生问题：应用服务器交互调用问题。</p>
<h2 id="改进8-微服务拆分"><a href="#改进8-微服务拆分" class="headerlink" title="改进8:微服务拆分"></a>改进8:微服务拆分</h2><p><strong>参考内容</strong></p>
<blockquote>
<p>主要参考以来两篇博客以及相关博客推荐，因找的博客比较多，没注意记录，最后好多忘了在哪2333，如果有侵权，请及时联系我，非常抱歉。 </p>
<p><a href="https://github.com/Snailclimb/JavaGuide" target="_blank" rel="noopener">https://github.com/Snailclimb/JavaGuide</a> </p>
<p><a href="https://github.com/CyC2018/CS-Notes" target="_blank" rel="noopener">https://github.com/CyC2018/CS-Notes</a> </p>
<p><a href="https://juejin.im/post/5c0ba2bef265da614d08fefe" target="_blank" rel="noopener">微服务核心结构梳理</a> </p>
<p><a href="https://www.zhihu.com/question/65502802" target="_blank" rel="noopener">什么是微服务架构</a> </p>
<p><a href="https://mp.weixin.qq.com/s/bCiNxRKzmPxPx4uU4gprEQ" target="_blank" rel="noopener">高并发下接口幂等性解决方案</a> </p>
<p><a href="https://juejin.im/post/5cbbe051f265da03973aabcb" target="_blank" rel="noopener">微服务架构最佳实践</a> </p>
<p><a href="https://juejin.im/post/592f87feb123db0064e5ef7c#heading-4" target="_blank" rel="noopener">简单聊聊SOA和微服务</a> </p>
<p><a href="https://blog.csdn.net/yp1125/article/details/79125477" target="_blank" rel="noopener">如何把应用从单机扩展到分布式</a> </p>
<p><a href="https://mp.weixin.qq.com/s/QCzfo8FqV7A1H_hsfPCyEw" target="_blank" rel="noopener">面试官问了我分布式事务，我感觉他有想给我40k的冲动</a></p>
<p><a href="https://mp.weixin.qq.com/s/CVaLb6I3TvGn81o41N0HhA" target="_blank" rel="noopener">超详细解析 | 一致性协议算法-2PC、3PC、Paxos、Raft、ZAB、NWR</a></p>
<p><a href="https://mp.weixin.qq.com/s/GhI7RYBdsrqlkU9o9CLEAg" target="_blank" rel="noopener">深入剖析共识性算法 Raft</a></p>
<p><a href="https://mp.weixin.qq.com/s/IA4aGJ0o-xsQtU9H0JliYw" target="_blank" rel="noopener">深度解析 Raft 分布式一致性协议（长文）</a></p>
</blockquote>

    </div>

    
    
    
        <div class="reward-container">
  <div>打工不易，想买杯奶茶</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://qiniu.xiaoming.net.cn/%E5%BE%AE%E4%BF%A1%E6%94%B6%E6%AC%BE%E7%A0%81.png" alt="Silverming 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://qiniu.xiaoming.net.cn/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.png" alt="Silverming 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/12/07/SpringBoot%E4%BD%BF%E7%94%A8Swagger2%E5%AE%9E%E7%8E%B0%E5%9C%A8%E7%BA%BF%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3/" rel="prev" title="Spring Boot 使用 Swagger2-UI 实现在线接口文档">
      <i class="fa fa-chevron-left"></i> Spring Boot 使用 Swagger2-UI 实现在线接口文档
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/01/18/%E9%9D%A2%E8%AF%95%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E7%BC%93%E5%AD%98/" rel="next" title="缓存">
      缓存 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式和集群的区别是什么？"><span class="nav-number">1.</span> <span class="nav-text">分布式和集群的区别是什么？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CAP定理"><span class="nav-number">2.</span> <span class="nav-text">CAP定理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BASE理论"><span class="nav-number">3.</span> <span class="nav-text">BASE理论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#核心思想"><span class="nav-number">3.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BASE-理论三要素"><span class="nav-number">3.2.</span> <span class="nav-text">BASE 理论三要素</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基本可用"><span class="nav-number">3.2.1.</span> <span class="nav-text">1. 基本可用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-软状态"><span class="nav-number">3.2.2.</span> <span class="nav-text">2. 软状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-最终一致性"><span class="nav-number">3.2.3.</span> <span class="nav-text">3. 最终一致性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式系统设计的两大思路"><span class="nav-number">4.</span> <span class="nav-text">分布式系统设计的两大思路</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#中心化设计"><span class="nav-number">4.1.</span> <span class="nav-text">中心化设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#去中心化设计"><span class="nav-number">4.2.</span> <span class="nav-text">去中心化设计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式事务"><span class="nav-number">5.</span> <span class="nav-text">分布式事务</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#分布式事务和分布式锁的区别"><span class="nav-number">5.1.</span> <span class="nav-text">分布式事务和分布式锁的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分布式事务产生的原因"><span class="nav-number">5.2.</span> <span class="nav-text">分布式事务产生的原因</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据库分库分表"><span class="nav-number">5.2.1.</span> <span class="nav-text">数据库分库分表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#应用SOA化"><span class="nav-number">5.2.2.</span> <span class="nav-text">应用SOA化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常见的分布式事务解决方案"><span class="nav-number">5.3.</span> <span class="nav-text">常见的分布式事务解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于XA协议的两阶段提交（2PC）"><span class="nav-number">5.3.1.</span> <span class="nav-text">基于XA协议的两阶段提交（2PC）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#第一阶段"><span class="nav-number">5.3.1.1.</span> <span class="nav-text">第一阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第二阶段"><span class="nav-number">5.3.1.2.</span> <span class="nav-text">第二阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于XA协议的三阶段提交（3PC）"><span class="nav-number">5.3.2.</span> <span class="nav-text">基于XA协议的三阶段提交（3PC）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#补偿事务（TCC）"><span class="nav-number">5.3.3.</span> <span class="nav-text">补偿事务（TCC）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#引入-TCC-的例子"><span class="nav-number">5.3.3.1.</span> <span class="nav-text">引入 TCC 的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#系统如何引入-TCC-事务"><span class="nav-number">5.3.3.2.</span> <span class="nav-text">系统如何引入 TCC 事务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TCC-与阶段提交对比"><span class="nav-number">5.3.3.3.</span> <span class="nav-text">TCC 与阶段提交对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本地消息表（异步确保）"><span class="nav-number">5.3.4.</span> <span class="nav-text">本地消息表（异步确保）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MQ事务消息"><span class="nav-number">5.3.5.</span> <span class="nav-text">MQ事务消息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sagas事务模型"><span class="nav-number">5.3.6.</span> <span class="nav-text">Sagas事务模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式一致性算法"><span class="nav-number">6.</span> <span class="nav-text">分布式一致性算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paxos算法"><span class="nav-number">7.</span> <span class="nav-text">Paxos算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RAFT"><span class="nav-number">8.</span> <span class="nav-text">RAFT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Raft-的基本概念"><span class="nav-number">8.1.</span> <span class="nav-text">Raft 的基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选主"><span class="nav-number">8.2.</span> <span class="nav-text">选主</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#节点角色"><span class="nav-number">8.2.1.</span> <span class="nav-text">节点角色</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#任期"><span class="nav-number">8.2.2.</span> <span class="nav-text">任期</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#节点通信"><span class="nav-number">8.2.3.</span> <span class="nav-text">节点通信</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#节点状态转换"><span class="nav-number">8.2.4.</span> <span class="nav-text">节点状态转换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Follower-状态转换过程"><span class="nav-number">8.2.4.1.</span> <span class="nav-text">Follower 状态转换过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Candidate-状态转换过程"><span class="nav-number">8.2.4.2.</span> <span class="nav-text">Candidate 状态转换过程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#选举成功（Step-receives-votes-from-majority-of-servers）"><span class="nav-number">8.2.4.2.1.</span> <span class="nav-text">选举成功（Step: receives votes from majority of servers）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#选举失败（Step-discovers-current-leader-or-new-term）"><span class="nav-number">8.2.4.2.2.</span> <span class="nav-text">选举失败（Step: discovers current leader or new term）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#选举超时（Step-times-out-new-election）"><span class="nav-number">8.2.4.2.3.</span> <span class="nav-text">选举超时（Step: times out, new election）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Leader-状态转换过程"><span class="nav-number">8.2.4.3.</span> <span class="nav-text">Leader 状态转换过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#日志复制"><span class="nav-number">8.3.</span> <span class="nav-text">日志复制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#日志格式"><span class="nav-number">8.3.1.</span> <span class="nav-text">日志格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#日志复制流程"><span class="nav-number">8.3.2.</span> <span class="nav-text">日志复制流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#日志一致性保障"><span class="nav-number">8.3.3.</span> <span class="nav-text">日志一致性保障</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#可能出现的日志不一致场景"><span class="nav-number">8.3.4.</span> <span class="nav-text">可能出现的日志不一致场景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#场景a-b-Follower-日志落后于-leader"><span class="nav-number">8.3.4.1.</span> <span class="nav-text">场景a~b: Follower 日志落后于 leader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#场景c-Follower-日志比-leader-多-term6"><span class="nav-number">8.3.4.2.</span> <span class="nav-text">场景c. Follower 日志比 leader 多 term6</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#场景d-Follower-日志比-leader-多-term7"><span class="nav-number">8.3.4.3.</span> <span class="nav-text">场景d. Follower 日志比 leader 多 term7</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#场景e-Follower-日志比-leader-少-term5-6，多-term4"><span class="nav-number">8.3.4.4.</span> <span class="nav-text">场景e. Follower 日志比 leader 少 term5 ~ 6，多 term4</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#场景f-Follower-日志比-leader-少-term4-6，多-term2-3"><span class="nav-number">8.3.4.5.</span> <span class="nav-text">场景f. Follower 日志比 leader 少 term4 ~ 6，多 term2 ~ 3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何处理日志不一致的场景"><span class="nav-number">8.3.5.</span> <span class="nav-text">如何处理日志不一致的场景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安全性及正确性"><span class="nav-number">8.4.</span> <span class="nav-text">安全性及正确性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对选举的限制"><span class="nav-number">8.4.1.</span> <span class="nav-text">对选举的限制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何判断哪个日志条目比较新？"><span class="nav-number">8.4.1.1.</span> <span class="nav-text">如何判断哪个日志条目比较新？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对提交的限制"><span class="nav-number">8.4.2.</span> <span class="nav-text">对提交的限制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集群成员变更与日志压缩"><span class="nav-number">8.5.</span> <span class="nav-text">集群成员变更与日志压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#集群成员变更"><span class="nav-number">8.5.1.</span> <span class="nav-text">集群成员变更</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#直接切换集群成员配置"><span class="nav-number">8.5.1.1.</span> <span class="nav-text">直接切换集群成员配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#两阶段切换集群成员配置"><span class="nav-number">8.5.1.2.</span> <span class="nav-text">两阶段切换集群成员配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段一"><span class="nav-number">8.5.1.2.1.</span> <span class="nav-text">阶段一</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段二"><span class="nav-number">8.5.1.2.2.</span> <span class="nav-text">阶段二</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么两阶段切换集群成员配置可以保证不会出现多个-leader？"><span class="nav-number">8.5.1.3.</span> <span class="nav-text">为什么两阶段切换集群成员配置可以保证不会出现多个 leader？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段1-C-old-new-尚未-commit"><span class="nav-number">8.5.1.3.1.</span> <span class="nav-text">阶段1. C-old,new 尚未 commit</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段2-C-old-new-已经-commit，C-new-尚未下发"><span class="nav-number">8.5.1.3.2.</span> <span class="nav-text">阶段2. C-old,new 已经 commit，C-new 尚未下发</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段3-C-new-已经下发但尚未-commit"><span class="nav-number">8.5.1.3.3.</span> <span class="nav-text">阶段3. C-new 已经下发但尚未 commit</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#阶段4-C-new-已经-commit"><span class="nav-number">8.5.1.3.4.</span> <span class="nav-text">阶段4. C-new 已经 commit</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#日志压缩"><span class="nav-number">8.5.2.</span> <span class="nav-text">日志压缩</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性一致性与读性能优化"><span class="nav-number">8.6.</span> <span class="nav-text">线性一致性与读性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是线性一致性？"><span class="nav-number">8.6.1.</span> <span class="nav-text">什么是线性一致性？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-线性一致性读"><span class="nav-number">8.6.2.</span> <span class="nav-text">Raft 线性一致性读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#写主读从缺陷分析"><span class="nav-number">8.6.2.1.</span> <span class="nav-text">写主读从缺陷分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#写主读主缺陷分析"><span class="nav-number">8.6.2.2.</span> <span class="nav-text">写主读主缺陷分析</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#问题一：状态机落后于-committed-log-导致脏读"><span class="nav-number">8.6.2.2.1.</span> <span class="nav-text">问题一：状态机落后于 committed log 导致脏读</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#问题二：网络分区导致脏读"><span class="nav-number">8.6.2.2.2.</span> <span class="nav-text">问题二：网络分区导致脏读</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Raft-Log-Read"><span class="nav-number">8.6.2.3.</span> <span class="nav-text">Raft Log Read</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-读性能优化"><span class="nav-number">8.6.3.</span> <span class="nav-text">Raft 读性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Read-Index"><span class="nav-number">8.6.3.1.</span> <span class="nav-text">Read Index</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lease-Read"><span class="nav-number">8.6.3.2.</span> <span class="nav-text">Lease Read</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Follower-Read"><span class="nav-number">8.6.3.3.</span> <span class="nav-text">Follower Read</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实现分布式锁的方式"><span class="nav-number">9.</span> <span class="nav-text">实现分布式锁的方式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据库唯一索引"><span class="nav-number">9.1.</span> <span class="nav-text">数据库唯一索引</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Redis-的-SETNX-指令"><span class="nav-number">9.2.</span> <span class="nav-text">Redis 的 SETNX 指令</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Redis-的-RedLock-算法"><span class="nav-number">9.3.</span> <span class="nav-text">Redis 的 RedLock 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用Zookeeper"><span class="nav-number">9.4.</span> <span class="nav-text">使用Zookeeper</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#负载均衡的方式和实现"><span class="nav-number">10.</span> <span class="nav-text">负载均衡的方式和实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#负载均衡算法"><span class="nav-number">10.1.</span> <span class="nav-text">负载均衡算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-轮询"><span class="nav-number">10.1.1.</span> <span class="nav-text">1. 轮询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-加权轮询"><span class="nav-number">10.1.2.</span> <span class="nav-text">2. 加权轮询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-最少连接"><span class="nav-number">10.1.3.</span> <span class="nav-text">3. 最少连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-加权最少连接"><span class="nav-number">10.1.4.</span> <span class="nav-text">4. 加权最少连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-随机算法"><span class="nav-number">10.1.5.</span> <span class="nav-text">5. 随机算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-源地址哈希法"><span class="nav-number">10.1.6.</span> <span class="nav-text">6. 源地址哈希法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#转发的实现"><span class="nav-number">10.2.</span> <span class="nav-text">转发的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-HTTP-重定向"><span class="nav-number">10.2.1.</span> <span class="nav-text">1. HTTP 重定向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-DNS-域名解析"><span class="nav-number">10.2.2.</span> <span class="nav-text">2. DNS 域名解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-反向代理服务器"><span class="nav-number">10.2.3.</span> <span class="nav-text">3. 反向代理服务器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-网络层"><span class="nav-number">10.2.4.</span> <span class="nav-text">4. 网络层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-链路层"><span class="nav-number">10.2.5.</span> <span class="nav-text">5. 链路层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#负载均衡session管理"><span class="nav-number">11.</span> <span class="nav-text">负载均衡session管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sticky-Session"><span class="nav-number">11.1.</span> <span class="nav-text">Sticky Session</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-Replication"><span class="nav-number">11.2.</span> <span class="nav-text">Session Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-Server"><span class="nav-number">11.3.</span> <span class="nav-text">Session Server</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#微服务架构"><span class="nav-number">12.</span> <span class="nav-text">微服务架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#微服务的概念"><span class="nav-number">12.1.</span> <span class="nav-text">微服务的概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#和单体应用对比"><span class="nav-number">12.2.</span> <span class="nav-text">和单体应用对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单体应用的优缺点："><span class="nav-number">12.2.1.</span> <span class="nav-text">单体应用的优缺点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#微服务的优缺点"><span class="nav-number">12.2.2.</span> <span class="nav-text">微服务的优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#优点"><span class="nav-number">12.2.2.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺点"><span class="nav-number">12.2.2.2.</span> <span class="nav-text">缺点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#微服务架构体系"><span class="nav-number">12.3.</span> <span class="nav-text">微服务架构体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#服务注册与发现——动态扩容"><span class="nav-number">12.3.1.</span> <span class="nav-text">服务注册与发现——动态扩容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#服务网关——权限控制，服务治理"><span class="nav-number">12.3.2.</span> <span class="nav-text">服务网关——权限控制，服务治理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置中心"><span class="nav-number">12.3.3.</span> <span class="nav-text">配置中心</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#服务通讯"><span class="nav-number">12.3.4.</span> <span class="nav-text">服务通讯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#服务监控预警——发现故障的征兆"><span class="nav-number">12.3.5.</span> <span class="nav-text">服务监控预警——发现故障的征兆</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#监控架构"><span class="nav-number">12.3.5.1.</span> <span class="nav-text">监控架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#链路追踪"><span class="nav-number">12.3.5.2.</span> <span class="nav-text">链路追踪</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#熔断、隔离、限流和降级"><span class="nav-number">12.3.6.</span> <span class="nav-text">熔断、隔离、限流和降级</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#熔断"><span class="nav-number">12.3.6.1.</span> <span class="nav-text">熔断</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#服务降级"><span class="nav-number">12.3.6.2.</span> <span class="nav-text">服务降级</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#限流"><span class="nav-number">12.3.6.3.</span> <span class="nav-text">限流</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式服务接口的幂等性如何设计（比如不能重复扣款）？"><span class="nav-number">13.</span> <span class="nav-text">分布式服务接口的幂等性如何设计（比如不能重复扣款）？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#设计方案"><span class="nav-number">13.1.</span> <span class="nav-text">设计方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#微服务如何进行拆分"><span class="nav-number">14.</span> <span class="nav-text">微服务如何进行拆分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#服务粒度"><span class="nav-number">14.1.</span> <span class="nav-text">服务粒度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拆分方法"><span class="nav-number">14.2.</span> <span class="nav-text">拆分方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于业务逻辑拆分"><span class="nav-number">14.2.1.</span> <span class="nav-text">基于业务逻辑拆分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于可扩展拆分"><span class="nav-number">14.2.2.</span> <span class="nav-text">基于可扩展拆分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于可靠性拆分"><span class="nav-number">14.2.3.</span> <span class="nav-text">基于可靠性拆分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于性能拆分"><span class="nav-number">14.2.4.</span> <span class="nav-text">基于性能拆分</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#微服务和-SOA-区别"><span class="nav-number">15.</span> <span class="nav-text">微服务和 SOA 区别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#应用SOA化-1"><span class="nav-number">15.1.</span> <span class="nav-text">应用SOA化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#区别"><span class="nav-number">15.2.</span> <span class="nav-text">区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#单体应用怎么改造成分布式应用"><span class="nav-number">16.</span> <span class="nav-text">单体应用怎么改造成分布式应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#改进1-应用服务器和数据库服务器分离"><span class="nav-number">16.1.</span> <span class="nav-text">改进1:应用服务器和数据库服务器分离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进2-应用服务器集群"><span class="nav-number">16.2.</span> <span class="nav-text">改进2:应用服务器集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进3-负载均衡器"><span class="nav-number">16.3.</span> <span class="nav-text">改进3:负载均衡器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进4-数据库服务器集群"><span class="nav-number">16.4.</span> <span class="nav-text">改进4:数据库服务器集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进5-缓存服务器"><span class="nav-number">16.5.</span> <span class="nav-text">改进5:缓存服务器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进6-数据库水平-垂直拆分"><span class="nav-number">16.6.</span> <span class="nav-text">改进6:数据库水平/垂直拆分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进7-应用服务器垂直拆分"><span class="nav-number">16.7.</span> <span class="nav-text">改进7: 应用服务器垂直拆分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进8-微服务拆分"><span class="nav-number">16.8.</span> <span class="nav-text">改进8:微服务拆分</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Silverming</p>
  <div class="site-description" itemprop="description">Wechat:934933088</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">139</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Silverming</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">2.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">34:34</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.5.0
  </div>
  <div>
     <a href="http://www.beianbeian.com/beianxinxi/56c155c0ed5f44020af3c1659377b89d.html" target="_blank" rel="noopener">粤ICP备18114217号</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
